{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14b3810-34cf-428d-a1cd-098fd6e7d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b4ce30-256b-4561-8543-a5e95c6c1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Hiperparámetros globales ------------------- #\n",
    "learning_rate = 5e-4\n",
    "minibatch_size = 512\n",
    "discount_factor = 0.99\n",
    "MAX_MEMORY = 20_000\n",
    "\n",
    "# Para el Early Stopping\n",
    "PATIENCE = 100  # Número de episodios sin mejora para detener el entrenamiento\n",
    "EARLYSTOP_THRESHOLD = 0.01  # Opcional: umbral de mejora mínima\n",
    "\n",
    "EXPERIMENT_FOLDER = f\"exp_{str(uuid.uuid4())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0720eb6b-568f-418c-9f5a-7a9b673cc676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'exp_db46caaf-4638-4c35-ab15-02c25f7ffa97' created successfully.\n",
      "Directory checkpoints created successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(EXPERIMENT_FOLDER)\n",
    "    print(f\"Directory '{EXPERIMENT_FOLDER}' created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Directory '{EXPERIMENT_FOLDER}' already exists.\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(f'./{EXPERIMENT_FOLDER}/checkpoints')\n",
    "    print(f\"Directory checkpoints created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Directory checkpoints' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af7b355c-18b4-457c-a0ca-643348b68888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, action_size, seed=42):\n",
    "        super(Network, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(10 * 10 * 128, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.bn1(self.conv1(state)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90a6464d-e8ff-4d16-a237-70416d11bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"\n",
    "    Preprocesa el frame del juego para el modelo.\n",
    "    Cambiar la resolución a 128x128 y convertir a tensor.\n",
    "    \"\"\"\n",
    "    frame = Image.fromarray(frame)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    return preprocess(frame).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e68702cd-c967-4e8c-8b88-1ec9183be7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, action_size):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Redes Q local y objetivo\n",
    "        self.local_qnetwork = Network(action_size).to(self.device)\n",
    "        self.target_qnetwork = Network(action_size).to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr=learning_rate)\n",
    "        self.memory = deque(maxlen=MAX_MEMORY)\n",
    "        \n",
    "        # Para registro de pérdidas (loss) en cada batch de aprendizaje\n",
    "        self.losses = []\n",
    "        \n",
    "        # Para registro de TD-errors (guardaremos la diferencia q_expected - q_targets)\n",
    "        self.td_errors = []\n",
    "        \n",
    "        # Para ver la distribución de Q-values de forma periódica\n",
    "        self.q_values_samples = []\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Almacena la experiencia y, si hay suficientes muestras, entrena la red.\n",
    "        \"\"\"\n",
    "        state = preprocess_frame(state)\n",
    "        next_state = preprocess_frame(next_state)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Cada vez que tenemos minibatch_size o más en memoria, entrenamos\n",
    "        if len(self.memory) > minibatch_size:\n",
    "            experiences = random.sample(self.memory, k=minibatch_size)\n",
    "            self.learn(experiences, discount_factor)\n",
    "\n",
    "    def act(self, state, epsilon=0.0):\n",
    "        \"\"\"\n",
    "        Selecciona acción con política epsilon-greedy.\n",
    "        \"\"\"\n",
    "        state = preprocess_frame(state).to(self.device)\n",
    "        self.local_qnetwork.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.local_qnetwork(state)\n",
    "        self.local_qnetwork.train()\n",
    "\n",
    "        # Epsilon-greedy\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"\n",
    "        Actualiza los parámetros de la red Q usando un batch de experiencias.\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "\n",
    "        states = torch.cat(states).float().to(self.device) \n",
    "        actions = torch.from_numpy(np.array(actions)).long().unsqueeze(1).to(self.device)\n",
    "        rewards = torch.from_numpy(np.array(rewards)).float().unsqueeze(1).to(self.device)\n",
    "        next_states = torch.cat(next_states).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().unsqueeze(1).to(self.device)\n",
    "\n",
    "        # Q valores objetivo\n",
    "        next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + (gamma * next_q_targets * (1 - dones))\n",
    "\n",
    "        # Q valores esperados (de la red local)\n",
    "        q_expected = self.local_qnetwork(states).gather(1, actions)\n",
    "\n",
    "        # Cálculo de la pérdida (MSE)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "\n",
    "        # Backprop\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Guardamos la pérdida para graficar\n",
    "        self.losses.append(loss.item())\n",
    "\n",
    "        # Guardamos la TD-error (podemos tomar la diferencia)\n",
    "        td_error = (q_expected - q_targets).detach().cpu().numpy()  # shape: [batch_size, 1]\n",
    "        self.td_errors.extend(td_error.flatten())  # lo aplanamos y lo guardamos\n",
    "\n",
    "        # Actualizamos la red objetivo con la local cada cierto tiempo\n",
    "        self.soft_update(tau=1e-3)\n",
    "\n",
    "    def soft_update(self, tau=1e-3):\n",
    "        \"\"\"\n",
    "        Actualiza los pesos de la red objetivo con la red local\n",
    "        usando factor tau (soft update).\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(self.target_qnetwork.parameters(),\n",
    "                                             self.local_qnetwork.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "    def sample_q_values(self, sample_states):\n",
    "        \"\"\"\n",
    "        Dado un lote de estados, extrae los Q-values con la red local.\n",
    "        Se usa para visualizar la distribución de Q-values cada cierto número de episodios.\n",
    "        \"\"\"\n",
    "        self.local_qnetwork.eval()\n",
    "        with torch.no_grad():\n",
    "            q_vals = self.local_qnetwork(sample_states.to(self.device))\n",
    "        self.local_qnetwork.train()\n",
    "        return q_vals.cpu().numpy()\n",
    "\n",
    "    def load(self, file_name):\n",
    "        \"\"\"\n",
    "        Carga pesos de un archivo .pth.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(file_name, map_location=self.device)\n",
    "        self.local_qnetwork.load_state_dict(checkpoint)\n",
    "        self.target_qnetwork.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57cc7993-9fbb-4ee8-b0c5-986e82edf456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (210, 160, 3)\n",
      "Number of actions:  9\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# Configuración del entorno MsPacman\n",
    "env_name = 'ALE/MsPacman-v5'\n",
    "env = gym.make(env_name, full_action_space=False)\n",
    "state_shape = env.observation_space.shape\n",
    "number_actions = env.action_space.n\n",
    "\n",
    "print('State shape: ', state_shape)\n",
    "print('Number of actions: ', number_actions)\n",
    "\n",
    "agent = Agent(number_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1ac45c2-1951-4b01-a8fb-4ebf0f4101a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_episodes = 10\n",
    "maximum_number_timesteps_per_episode = 10000\n",
    "\n",
    "epsilon_starting_value = 1.0\n",
    "epsilon_ending_value = 0.01\n",
    "epsilon_decay_value = 0.995\n",
    "epsilon = epsilon_starting_value\n",
    "\n",
    "scores_on_100_episodes = deque(maxlen=100)\n",
    "\n",
    "# Listas para registro de métricas\n",
    "all_scores = []      # Score por episodio\n",
    "avg_scores = []      # Promedio móvil de score (100 últimos episodios)\n",
    "eps_history = []     # Epsilon por episodio\n",
    "all_steps = []       # Cantidad de pasos por episodio\n",
    "action_distribution_history = []  # Frecuencia (proporción) de acciones por episodio\n",
    "\n",
    "# Para el heatmap global de visitas\n",
    "track_visits = True\n",
    "accumulated_heatmap = np.zeros((128, 128), dtype=np.int64) if track_visits else None\n",
    "\n",
    "# Para early stopping\n",
    "best_avg_score = -np.inf\n",
    "episodes_no_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec6027-8dd5-4206-9260-de7f7a225a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1, number_episodes + 1):\n",
    "    state, _ = env.reset()\n",
    "    score = 0\n",
    "    \n",
    "    steps_in_episode = 0\n",
    "    action_counts = np.zeros(number_actions, dtype=np.int64)\n",
    "\n",
    "    # Mapa de visitas en este episodio\n",
    "    episode_heatmap = np.zeros((128, 128), dtype=np.int64) if track_visits else None\n",
    "\n",
    "    for t in range(maximum_number_timesteps_per_episode):\n",
    "        # Elegir acción\n",
    "        action = agent.act(state, epsilon)\n",
    "        action_counts[action] += 1\n",
    "        \n",
    "        # Pasar un timestep\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        steps_in_episode += 1\n",
    "        \n",
    "        # Actualizar mapa de visitas del episodio\n",
    "        if track_visits:\n",
    "            state_tensor = preprocess_frame(state).squeeze(0).numpy()  # shape (3,128,128)\n",
    "            channel_0 = state_tensor[0]\n",
    "            episode_heatmap[channel_0 > 0] += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Agregar heatmap del episodio al heatmap acumulado\n",
    "    if track_visits:\n",
    "        accumulated_heatmap += episode_heatmap\n",
    "\n",
    "    # Registro de métricas de cada episodio\n",
    "    scores_on_100_episodes.append(score)\n",
    "    all_scores.append(score)\n",
    "    current_avg_score = np.mean(scores_on_100_episodes)\n",
    "    avg_scores.append(current_avg_score)\n",
    "    eps_history.append(epsilon)\n",
    "    all_steps.append(steps_in_episode)\n",
    "\n",
    "    # Distribución de acciones (en proporción)\n",
    "    action_dist = action_counts / steps_in_episode\n",
    "    action_distribution_history.append(action_dist)\n",
    "\n",
    "    # Decaimiento de epsilon\n",
    "    epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
    "\n",
    "    print(f\"\\rEpisode {episode}\\tScore: {score}\\tAvg Score(últ. 100): {current_avg_score:.2f}\", end=\"\")\n",
    "\n",
    "    # Cada 100 episodios, mostramos info, guardamos checkpoint y muestreamos Q-values\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"\\rEpisode {episode}\\tAverage Score (últ. 100): {current_avg_score:.2f}\")\n",
    "        torch.save(agent.local_qnetwork.state_dict(), f'./{EXPERIMENT_FOLDER}/checkpoints/checkpoint_ep{episode}.pth')\n",
    "        \n",
    "        # Muestreamos Q-values para visualización\n",
    "        # Tomamos 10 estados aleatorios del entorno (manualmente) o de la memoria\n",
    "        sample_size = 10\n",
    "        if len(agent.memory) > sample_size:\n",
    "            sample_experiences = random.sample(agent.memory, sample_size)\n",
    "            sample_states = [exp[0] for exp in sample_experiences]  # exp[0] = state\n",
    "            # Concatenamos en un solo tensor\n",
    "            sample_states_tensor = torch.cat(sample_states).float()\n",
    "            q_vals_sample = agent.sample_q_values(sample_states_tensor)\n",
    "            # Guardamos en la lista para graficar la distribución de Q-values\n",
    "            agent.q_values_samples.append(q_vals_sample)\n",
    "\n",
    "    # Early Stopping manual\n",
    "    if current_avg_score > (best_avg_score + EARLYSTOP_THRESHOLD):\n",
    "        best_avg_score = current_avg_score\n",
    "        episodes_no_improvement = 0\n",
    "    else:\n",
    "        episodes_no_improvement += 1\n",
    "\n",
    "    if episodes_no_improvement >= PATIENCE:\n",
    "        print(f\"\\nNo hubo mejora en {PATIENCE} episodios consecutivos. Deteniendo entrenamiento.\")\n",
    "        break\n",
    "\n",
    "    # Si consideras un umbral para \"resolver\" el entorno\n",
    "    if current_avg_score >= 500.0:\n",
    "        print(f\"\\n¡Entorno solucionado en {episode} episodios! Avg Score: {current_avg_score:.2f}\")\n",
    "        torch.save(agent.local_qnetwork.state_dict(), f'./{EXPERIMENT_FOLDER}/checkpoints/checkpoint_solved_1.pth')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "149fe84f-b814-49c7-ac5c-522a1b27ca8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenamiento finalizado. ¡Las gráficas se han guardado en archivos .png!\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Termina el entrenamiento, hacemos gráficas ------------------- #\n",
    "# 1) Gráfica de Score por episodio y promedio móvil de los últimos 100 episodios\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(all_scores, label='Score por episodio')\n",
    "plt.plot(avg_scores, label='Promedio móvil (100)')\n",
    "plt.title('Evolución de Score en MsPacman')\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'./{EXPERIMENT_FOLDER}/scores_plot.png')\n",
    "plt.close()\n",
    "\n",
    "# 2) Gráfica de pérdidas (loss) acumuladas en cada batch de aprendizaje\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(agent.losses, label='Loss')\n",
    "plt.title('Pérdida (Loss) durante el entrenamiento')\n",
    "plt.xlabel('Actualizaciones de la red')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'./{EXPERIMENT_FOLDER}/loss_plot.png')\n",
    "plt.close()\n",
    "\n",
    "# 3) Gráfica de epsilon\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(eps_history, label='Epsilon')\n",
    "plt.title('Evolución de Epsilon')\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'./{EXPERIMENT_FOLDER}/epsilon_plot.png')\n",
    "plt.close()\n",
    "\n",
    "# 4) Gráfica de TD-error\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(agent.td_errors, label='TD Error (q_expected - q_targets)')\n",
    "plt.title('Evolución de TD-error')\n",
    "plt.xlabel('Actualizaciones de la red')\n",
    "plt.ylabel('TD-error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'./{EXPERIMENT_FOLDER}/td_error_plot.png')\n",
    "plt.close()\n",
    "\n",
    "# 5) Gráfica de pasos por episodio\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(all_steps, label='Steps por episodio')\n",
    "plt.title('Cantidad de pasos por episodio')\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Steps')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'./{EXPERIMENT_FOLDER}/steps_plot.png')\n",
    "plt.close()\n",
    "\n",
    "# 6) Distribución de acciones\n",
    "#    Podemos graficar cómo varía la proporción de cada acción en el tiempo.\n",
    "#    Para cada acción, sacaremos una lista con su proporción en cada episodio.\n",
    "action_distribution_history = np.array(action_distribution_history)  # shape (num_episodes, num_actions)\n",
    "plt.figure(figsize=(12, 5))\n",
    "for action_idx in range(number_actions):\n",
    "    plt.plot(action_distribution_history[:, action_idx], label=f'Accion {action_idx}')\n",
    "plt.title('Distribución de acciones (proporción) por episodio')\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Proporción de uso de la acción')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'./{EXPERIMENT_FOLDER}/action_distribution_plot.png')\n",
    "plt.close()\n",
    "\n",
    "# 7) Distribución de Q-values\n",
    "#    Podemos mostrar un histograma promedio de las muestras recolectadas cada 100 episodios\n",
    "#    (guardadas en agent.q_values_samples).\n",
    "all_q_values = np.concatenate(agent.q_values_samples, axis=0) if len(agent.q_values_samples) > 0 else []\n",
    "if len(all_q_values) > 0:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.hist(all_q_values.flatten(), bins=50)\n",
    "    plt.title('Distribución de Q-values muestreados')\n",
    "    plt.xlabel('Valor Q')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'./{EXPERIMENT_FOLDER}/q_values_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "# 8) Heatmap global de visitas\n",
    "if track_visits:\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(accumulated_heatmap, cmap='hot', interpolation='nearest')\n",
    "    plt.title('Heatmap de visitas (acumulado)')\n",
    "    plt.colorbar()\n",
    "    plt.savefig(f'./{EXPERIMENT_FOLDER}/heatmap_visits.png')\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\nEntrenamiento finalizado. ¡Las gráficas se han guardado en archivos .png!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc3f8dc9-d857-446f-b4bd-07b5f33f2b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiperparámetros guardados en hyperparameters.txt\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    \"env_name\": env_name,\n",
    "    \"state_shape\": state_shape,\n",
    "    \"number_actions\": number_actions,\n",
    "    \"number_episodes\": number_episodes,\n",
    "    \"maximum_number_timesteps_per_episode\": maximum_number_timesteps_per_episode,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"minibatch_size\": minibatch_size,\n",
    "    \"discount_factor\": discount_factor,\n",
    "    \"max_memory\": MAX_MEMORY,\n",
    "    \"patience\": PATIENCE,\n",
    "    \"earlystop_threshold\": EARLYSTOP_THRESHOLD,   \n",
    "    \"epsilon_starting_value\": epsilon_starting_value,\n",
    "    \"epsilon_ending_value\": epsilon_ending_value,\n",
    "    \"epsilon_decay_value\": epsilon_decay_value,   \n",
    "    }\n",
    "\n",
    "# Nombre del archivo donde se guardarán los hiperparámetros\n",
    "file_name = \"hyperparameters.txt\"\n",
    "\n",
    "# Escribir los hiperparámetros en un archivo de texto\n",
    "with open(f'./{EXPERIMENT_FOLDER}/{file_name}', \"w\") as file:\n",
    "    file.write(\"# Hiperparámetros\\n\")\n",
    "    for key, value in hyperparameters.items():\n",
    "        file.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "print(f\"Hiperparámetros guardados en {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687faad-4f58-4bf2-a63a-204e607a8bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
