### State of the Art

@misc{zhang_mm-llms_2024,
	title = {{MM}-{LLMs}: {Recent} {Advances} in {MultiModal} {Large} {Language} {Models}},
	shorttitle = {{MM}-{LLMs}},
	url = {http://arxiv.org/abs/2401.13601},
	doi = {10.48550/arXiv.2401.13601},
	abstract = {In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Initially, we outline general design formulations for model architecture and training pipeline. Subsequently, we introduce a taxonomy encompassing 126 MM-LLMs, each characterized by its specific formulations. Furthermore, we review the performance of selected MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Finally, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.},
	urldate = {2025-10-10},
	publisher = {arXiv},
	author = {Zhang, Duzhen and Yu, Yahan and Dong, Jiahua and Li, Chenxing and Su, Dan and Chu, Chenhui and Yu, Dong},
	month = may,
	year = {2024},
	note = {arXiv:2401.13601 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted by ACL2024 (findings)},
	file = {Full Text PDF:files/547/Zhang et al. - 2024 - MM-LLMs Recent Advances in MultiModal Large Language Models.pdf:application/pdf;Snapshot:files/546/2401.html:text/html},
}

@article{yin_survey_2024,
	title = {A survey on multimodal large language models},
	volume = {11},
	issn = {2095-5138},
	url = {https://doi.org/10.1093/nsr/nwae403},
	doi = {10.1093/nsr/nwae403},
	abstract = {Recently, the multimodal large language model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful large language models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of the MLLM, such as writing stories based on images and optical character recognition–free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even outperform GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First, we present the basic formulation of the MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages and scenarios. We continue with multimodal hallucination and extended techniques, including multimodal in-context learning, multimodal chain of thought and LLM-aided visual reasoning. To conclude the paper, we discuss existing challenges and point out promising research directions.},
	number = {12},
	urldate = {2025-10-10},
	journal = {National Science Review},
	author = {Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
	month = dec,
	year = {2024},
	keywords = {review},
	pages = {nwae403},
	file = {Full Text PDF:files/549/Yin et al. - 2024 - A survey on multimodal large language models.pdf:application/pdf;Snapshot:files/550/nwae403.html:text/html},
}

@inproceedings{cheng_ragtrace_2025,
	address = {New York, NY, USA},
	series = {{UIST} '25},
	title = {{RAGTrace}: {Understanding} and {Refining} {Retrieval}-{Generation} {Dynamics} in {Retrieval}-{Augmented} {Generation}},
	isbn = {979-8-4007-2037-6},
	shorttitle = {{RAGTrace}},
	url = {https://dl.acm.org/doi/10.1145/3746059.3747741},
	doi = {10.1145/3746059.3747741},
	abstract = {Retrieval-Augmented Generation (RAG) systems have emerged as a promising solution to enhance large language models (LLMs) by integrating external knowledge retrieval with generative capabilities. While significant advancements have been made in improving retrieval accuracy and response quality, a critical challenge remains that the internal knowledge integration and retrieval-generation interactions in RAG workflows are largely opaque. This paper introduces RAGTrace, an interactive evaluation system designed to analyze retrieval and generation dynamics in RAG-based workflows. Informed by a comprehensive literature review and expert interviews, the system supports a multi-level analysis approach, ranging from high-level performance evaluation to fine-grained examination of retrieval relevance, generation fidelity, and cross-component interactions. Unlike conventional evaluation practices that focus on isolated retrieval or generation quality assessments, RAGTrace enables an integrated exploration of retrieval-generation relationships, allowing users to trace knowledge sources and identify potential failure cases. The system’s workflow allows users to build, evaluate, and iterate on retrieval processes tailored to their specific domains of interest. The effectiveness of the system is demonstrated through case studies and expert evaluations on real-world RAG applications.},
	urldate = {2025-10-10},
	booktitle = {Proceedings of the 38th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Cheng, Sizhe and Li, Jiaping and Wang, Huanchen and Ma, Yuxin},
	month = sep,
	year = {2025},
	pages = {1--20},
	file = {Full Text PDF:files/572/Cheng et al. - 2025 - RAGTrace Understanding and Refining Retrieval-Generation Dynamics in Retrieval-Augmented Generation.pdf:application/pdf},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2025-11-07},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {Full Text PDF:files/634/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;Snapshot:files/633/2005.html:text/html},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2025-11-07},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency},
	file = {Full Text PDF:files/641/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf;Snapshot:files/640/2106.html:text/html},
}

@misc{lewis_retrieval-augmented_2021,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	url = {http://arxiv.org/abs/2005.11401},
	doi = {10.48550/arXiv.2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	urldate = {2025-11-07},
	publisher = {arXiv},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	month = apr,
	year = {2021},
	note = {arXiv:2005.11401 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Accepted at NeurIPS 2020},
	file = {Full Text PDF:files/645/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf:application/pdf;Snapshot:files/644/2005.html:text/html},
}

@misc{xi_rise_2023,
	title = {The {Rise} and {Potential} of {Large} {Language} {Model} {Based} {Agents}: {A} {Survey}},
	shorttitle = {The {Rise} and {Potential} of {Large} {Language} {Model} {Based} {Agents}},
	url = {http://arxiv.org/abs/2309.07864},
	doi = {10.48550/arXiv.2309.07864},
	abstract = {For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.},
	urldate = {2025-11-07},
	publisher = {arXiv},
	author = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and Zheng, Rui and Fan, Xiaoran and Wang, Xiao and Xiong, Limao and Zhou, Yuhao and Wang, Weiran and Jiang, Changhao and Zou, Yicheng and Liu, Xiangyang and Yin, Zhangyue and Dou, Shihan and Weng, Rongxiang and Cheng, Wensen and Zhang, Qi and Qin, Wenjuan and Zheng, Yongyan and Qiu, Xipeng and Huang, Xuanjing and Gui, Tao},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07864 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 86 pages, 12 figures},
	file = {Full Text PDF:files/649/Xi et al. - 2023 - The Rise and Potential of Large Language Model Based Agents A Survey.pdf:application/pdf;Snapshot:files/648/2309.html:text/html},
}

@misc{dettmers_llmint8_2022,
	title = {{LLM}.int8(): 8-bit {Matrix} {Multiplication} for {Transformers} at {Scale}},
	shorttitle = {{LLM}.int8()},
	url = {http://arxiv.org/abs/2208.07339},
	doi = {10.48550/arXiv.2208.07339},
	abstract = {Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.},
	urldate = {2025-11-07},
	publisher = {arXiv},
	author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
	month = nov,
	year = {2022},
	note = {arXiv:2208.07339 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: Published at NeurIPS 2022. Camera-ready version},
	file = {Full Text PDF:files/653/Dettmers et al. - 2022 - LLM.int8() 8-bit Matrix Multiplication for Transformers at Scale.pdf:application/pdf;Snapshot:files/652/2208.html:text/html},
}

@misc{yuan_hybrid_2024,
	title = {A {Hybrid} {RAG} {System} with {Comprehensive} {Enhancement} on {Complex} {Reasoning}},
	url = {http://arxiv.org/abs/2408.05141},
	doi = {10.48550/arXiv.2408.05141},
	abstract = {Retrieval-augmented generation (RAG) is a framework enabling large language models (LLMs) to enhance their accuracy and reduce hallucinations by integrating external knowledge bases. In this paper, we introduce a hybrid RAG system enhanced through a comprehensive suite of optimizations that significantly improve retrieval quality, augment reasoning capabilities, and refine numerical computation ability. We refined the text chunks and tables in web pages, added attribute predictors to reduce hallucinations, conducted LLM Knowledge Extractor and Knowledge Graph Extractor, and finally built a reasoning strategy with all the references. We evaluated our system on the CRAG dataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and online evaluations demonstrate that our system significantly enhances complex reasoning capabilities. In local evaluations, we have significantly improved accuracy and reduced error rates compared to the baseline model, achieving a notable increase in scores. In the meanwhile, we have attained outstanding results in online assessments, demonstrating the performance and generalization capabilities of the proposed system. The source code for our system is released in {\textbackslash}url\{https://gitlab.aicrowd.com/shizueyy/crag-new\}.},
	urldate = {2025-11-14},
	publisher = {arXiv},
	author = {Yuan, Ye and Liu, Chengwu and Yuan, Jingyang and Sun, Gongbo and Li, Siqi and Zhang, Ming},
	month = aug,
	year = {2024},
	note = {arXiv:2408.05141 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	annote = {Comment: Technical report for 3rd prize in Task 1 of Meta CRAG KDD Cup 2024},
	file = {Full Text PDF:files/703/Yuan et al. - 2024 - A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning.pdf:application/pdf;Snapshot:files/704/2408.html:text/html},
}

@misc{gao_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.10997},
	doi = {10.48550/arXiv.2312.10997},
	abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
	urldate = {2025-12-19},
	publisher = {arXiv},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	month = mar,
	year = {2024},
	note = {arXiv:2312.10997 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Ongoing Work},
	file = {Full Text PDF:files/730/Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language Models A Survey.pdf:application/pdf;Snapshot:files/731/2312.html:text/html},
}

@inproceedings{kandpal_large_2023,
	title = {Large {Language} {Models} {Struggle} to {Learn} {Long}-{Tail} {Knowledge}},
	url = {https://proceedings.mlr.press/v202/kandpal23a.html},
	abstract = {The Internet contains a wealth of knowledge—from the birthdays of historical figures to tutorials on how to code—all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model’s ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, we estimate that today’s models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.},
	language = {en},
	urldate = {2026-01-02},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {15696--15707},
	file = {Full Text PDF:files/835/Kandpal et al. - 2023 - Large Language Models Struggle to Learn Long-Tail Knowledge.pdf:application/pdf},
}


## Control de lectura


@article{wang_retrieval_2025,
	title = {A retrieval augmented generation based optimization approach for medical knowledge understanding and reasoning in large language models},
	volume = {28},
	issn = {25900056},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2590005625001316},
	doi = {10.1016/j.array.2025.100504},
	abstract = {Based on the existing Retrieval Augmented Generation (RAG) technology, this study proposes innovative solution to better address the hallucination issues of current large language models. By optimizing data processing, prompt engineering, and multi-retriever fusion, it resolves issues such as semantic capture bias, inaccurate context retrieval, information redundancy, hallucination generation, and length limitations. Data processing focuses on text cleaning, disambiguation, and removing redundant information to enhance consistency. Prompt engineering aids the model in better understanding the task. The adaptive weight fusion of sparse and dense retrievers improves context retrieval accuracy. Experiments conducted on the CCKS-TCMBench dataset for medical knowledge understanding and semantic reasoning show that the optimized model significantly outperforms the baseline across all evaluation metrics.},
	language = {en},
	urldate = {2025-10-10},
	journal = {Array},
	author = {Wang, Yingshuai and Wan, Yanli and Lei, Xingyun and Chen, Qingkun and Hu, Hongpu},
	month = dec,
	year = {2025},
	pages = {100504},
	file = {PDF:files/538/Wang et al. - 2025 - A retrieval augmented generation based optimization approach for medical knowledge understanding and.pdf:application/pdf},
}

@article{knollmeyer_hybrid_2025,
	title = {Hybrid {Retrieval} for {Retrieval} {Augmented} {Generation} in the {German} {Language} {Production} {Domain}},
	volume = {16},
	issn = {17982340},
	url = {https://www.jait.us/show-255-1704-1.html},
	doi = {10.12720/jait.16.6.819-829},
	abstract = {Retrieval Augmented Generation (RAG) is an emerging method for leveraging Artificial Intelligence in the field of knowledge management, particularly within specialized domains. In this study, we focus on evaluating the effect of hybrid retrieval techniques on German technical documents, which are widely used in the production and engineering departments of German companies. RAG employs a Large Language Model (LLM) that accesses an extensive information store to answer user queries by retrieving the most relevant text passages, known as chunks, from a database. The efficiency of this retrieval process is crucial for the overall performance of the RAG system. Classical RAG employs dense vector embedding and nearest neighbor search for information retrieval. State of the art of shelf embedding models tend to struggle with non-English and highly domain-specific texts. Given the language, complexity, and specificity of technical production planning documents, we propose a hybrid retrieval approach combining full-text and common RAG vector search. We constructed 2,000 question-and-answer pairs for each language, German and English, in a representative corpus of identical texts. Our proposed hybrid approach consistently enhances the retrieval performance for German documents by 20\% over a purely vector-based search, entirely erasing the deficiencies of embedding models for German texts, thus demonstrating its significant potential to improve knowledge management in technical and industrial contexts.},
	language = {en},
	number = {6},
	urldate = {2025-10-10},
	journal = {Journal of Advances in Information Technology},
	author = {Knollmeyer, Simon and Pfaff, Sebastian and Akmal, Muhammad Uzair and Koval, Leonid and Asif, Saara and Mathias, Selvine G. and Großmann, Daniel},
	year = {2025},
	pages = {819--829},
	file = {PDF:files/562/Knollmeyer et al. - 2025 - Hybrid Retrieval for Retrieval Augmented Generation in the German Language Production Domain.pdf:application/pdf},
}

@article{ibtasham_reqrag_nodate,
	title = {{ReqRAG}: {Enhancing} {Software} {Release} {Management} through {Retrieval}-{Augmented} {LLMs}: {An} {Industrial} {Study}},
	abstract = {Context and Motivation] Engineers often need to refer back to release notes, manuals, and system architecture documents to understand, modify, or upgrade functionalities in alignment with new software releases. This is crucial to ensure that new stakeholder requirements align with the existing system, maintaining compatibility and preventing integration issues. [Problem] In practice, the manual process of retrieving the relevant information from technical documentation is time-intensive and frequently results in inefficient software release management. [Principal ideas/results] In this paper, we propose a question-answering chatbot, ReqRAG, leveraging Retrieval Augmented Generation (RAG) with Large Language Models (LLMs) to deliver accurate and up-to-date information from technical documents in response to given queries. We employ various context retrieval techniques paired with state-of-the-art LLMs to evaluate the ReqRAG approach in industrial settings. Furthermore, we conduct human evaluations of the results in collaboration with experts from Alstom to gain practical insights. Our results indicate that, on average, 70\% of the generated responses are adequate, useful, and relevant to the practitioners. [Contribution] Fewer studies have comprehensively evaluated RAG-based approaches in industrial settings. Therefore, this work provides technical considerations for domain-specific chatbots, guiding researchers and practitioners facing similar challenges.},
	language = {en},
	author = {Ibtasham, Saleh and Bashir, Sarmad and Abbas, Muhammad and Haider, Zulqarnain and Saadatmand, Mehrdad and Cicchetti, Antonio},
	file = {PDF:files/576/Ibtasham et al. - ReqRAG Enhancing Software Release Management through Retrieval-Augmented LLMs An Industrial Study.pdf:application/pdf},
}

@article{muludi_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} {Approach}: {Document} {Question} {Answering} using {Large} {Language} {Model}},
	volume = {15},
	issn = {21565570, 2158107X},
	shorttitle = {Retrieval-{Augmented} {Generation} {Approach}},
	url = {http://thesai.org/Publications/ViewPaper?Volume=15&Issue=3&Code=IJACSA&SerialNo=79},
	doi = {10.14569/IJACSA.2024.0150379},
	abstract = {This study introduces the Retrieval Augmented Generation (RAG) method to improve Question-Answering (QA) systems by addressing document processing in Natural Language Processing problems. It represents the latest breakthrough in applying RAG to document question and answer applications, overcoming previous QA system obstacles. RAG combines search techniques in vector store and text generation mechanism developed by Large Language Models, offering a time-efficient alternative to manual reading limitations. The research evaluates RAG's that use Generative Pre-trained Transformer 3.5 or GPT3.5-turbo from the ChatGPT model and its impact on document data processing, comparing it with other applications. This research also provides datasets to test the capabilities of the QA document system. The proposed dataset and Stanford Question Answering Dataset (SQuAD) are used for performance testing. The study contributes theoretically by advancing methodologies and knowledge representation, supporting benchmarking in research communities. Results highlight RAG's superiority: achieving a precision of 0.74 in Recall-Oriented Understudy for Gisting Evaluation (ROUGE) testing, outperforming others at 0.5; obtaining an F1 score of 0.88 in BERTScore, surpassing other QA apps at 0.81; attaining a precision of 0.28 in Bilingual Evaluation Understudy (BLEU) testing, surpassing others with a precision of 0.09; and scoring 0.33 in Jaccard Similarity, outshining others at 0.04. These findings underscore RAG's efficiency and competitiveness, promising a positive impact on various industrial sectors through advanced Artificial Intelligence (AI) technology.},
	language = {en},
	number = {3},
	urldate = {2025-10-11},
	journal = {International Journal of Advanced Computer Science and Applications},
	author = {Muludi, Kurnia and Fitria, Kaira Milani and Triloka, Joko and -, Sutedi},
	year = {2024},
	file = {PDF:files/586/Muludi et al. - 2024 - Retrieval-Augmented Generation Approach Document Question Answering using Large Language Model.pdf:application/pdf},
}

@article{santra_curious_2025,
	title = {The “{Curious} {Case} of {Contexts}” in {Retrieval}-{Augmented} {Generation} {With} a {Combination} of {Labeled} and {Unlabeled} {Data}},
	volume = {15},
	issn = {1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.70021},
	doi = {10.1002/widm.70021},
	abstract = {With the growing reliance on LLMs for a wide range of NLP tasks, optimizing the use of labeled and unlabeled data for effective context generation has become critical. This work explores the interplay between two prominent methodologies in few-shot learning: in-context learning (ICL), which utilizes labeled task-specific data, and retrieval-augmented generation (RAG), which leverages unlabeled external knowledge to augment generative models. Since each has its individual limitations, we propose a novel hybrid approach to obtain “the best of both worlds” by dynamically integrating both labeled and unlabeled data towards improving the downstream performance of LLMs. Our methodology, which we call LU-RAG (labeled and unlabeled RAG), recomputes the scores of top-k labeled instances and top-m unlabeled passages to refine context selection. Our experimental results demonstrate that LU-RAG consistently outperforms both standalone ICL and RAG across multiple benchmarks, showing significant gains in downstream performance. Furthermore, we show that LU-RAG performs better with a semantic neighborhood as compared to a lexical one, highlighting its ability to generalize effectively.},
	language = {en},
	number = {2},
	urldate = {2025-10-23},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Santra, Payel and Ghosh, Madhusudan and Ganguly, Debasis and Basuchowdhuri, Partha and Naskar, Sudip Kumar},
	year = {2025},
	note = {\_eprint: https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.70021},
	keywords = {claim verification, in-context learning, prompt learning, retrieval augmented generation, text classification},
	pages = {e70021},
	annote = {e70021 DMKD-00758.R1},
	file = {Full Text PDF:files/616/Santra et al. - 2025 - The “Curious Case of Contexts” in Retrieval-Augmented Generation With a Combination of Labeled and U.pdf:application/pdf;Snapshot:files/617/widm.html:text/html},
}

@article{shu_utilizing_2024,
	title = {Utilizing {Large} {Language} {Models} for {Hyper} {Knowledge} {Graph} {Construction} in {Mine} {Hoist} {Fault} {Analysis}},
	volume = {16},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-8994},
	url = {https://www.mdpi.com/2073-8994/16/12/1600},
	doi = {10.3390/sym16121600},
	abstract = {The rapid development of artificial intelligence technology is driving the intelligentization process across various fields, particularly in knowledge graph construction, where significant achievements have been made. However, research on hyper-relational knowledge graphs in the industrial domain remains relatively weak. Traditional construction methods suffer from low automation, high cost, and poor reproducibility and portability. To address these challenges, this paper proposes an optimized construction process for a hyper-relational knowledge graph for mine hoist faults based on large language models. This process leverages the strengths of large language models and the logical connections of fault knowledge, employing GPT’s powerful reasoning abilities. A combined strategy of template-based and template-free prompts is designed to generate fault entities and relationships. To address potential data incompleteness caused by prompt engineering, link prediction is used to optimize the initial data generated by GPT o1-preview. We integrated the graph’s topological structure with domain-specific logical rules and applied the Variational EM algorithm for alternating optimization while also incorporating text embeddings to comprehensively enhance data optimization. Experimental results show that compared to the unoptimized MHSD, the optimized MHSD achieved a 0.008 improvement in MRR. Additionally, compared to the latest KICGPT, the optimized MHSD showed a 0.002 improvement in MRR. Finally, the optimized data were successfully imported into Neo4j for visualization.},
	language = {en},
	number = {12},
	urldate = {2025-11-03},
	journal = {Symmetry},
	author = {Shu, Xiaoling and Dang, Xiaochao and Dong, Xiaohui and Li, Fenfang},
	month = dec,
	year = {2024},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {hyper-relational knowledge graphs, industrial intelligence, link prediction, prompt engineering design},
	pages = {1600},
	file = {Full Text PDF:files/623/Shu et al. - 2024 - Utilizing Large Language Models for Hyper Knowledge Graph Construction in Mine Hoist Fault Analysis.pdf:application/pdf},
}

@article{nam_lora-tuned_2025,
	title = {{LoRA}-{Tuned} {Multimodal} {RAG} {System} for {Technical} {Manual} {QA}: {A} {Case} {Study} on {Hyundai} {Staria}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	shorttitle = {{LoRA}-{Tuned} {Multimodal} {RAG} {System} for {Technical} {Manual} {QA}},
	url = {https://www.mdpi.com/2076-3417/15/15/8387},
	doi = {10.3390/app15158387},
	abstract = {This study develops a domain-adaptive multimodal RAG (Retrieval-Augmented Generation) system to improve the accuracy and efficiency of technical question answering based on large-scale structured manuals. Using Hyundai Staria maintenance documents as a case study, we extracted text and images from PDF manuals and constructed QA, RAG, and Multi-Turn datasets to reflect realistic troubleshooting scenarios. To overcome limitations of baseline RAG models, we proposed an enhanced architecture that incorporates sentence-level similarity annotations and parameter-efficient fine-tuning via LoRA (Low-Rank Adaptation) using the bLLossom-8B language model and BAAI-bge-m3 embedding model. Experimental results show that the proposed system achieved improvements of 3.0\%p in BERTScore, 3.0\%p in cosine similarity, and 18.0\%p in ROUGE-L compared to existing RAG systems, with notable gains in image-guided response accuracy. A qualitative evaluation by 20 domain experts yielded an average satisfaction score of 4.4 out of 5. This study presents a practical and extensible AI framework for multimodal document understanding, with broad applicability across automotive, industrial, and defense-related technical documentation.},
	language = {en},
	number = {15},
	urldate = {2025-11-07},
	journal = {Applied Sciences},
	author = {Nam, Yerin and Choi, Hansun and Choi, Jonggeun and Kwon, Hyukjin},
	month = jan,
	year = {2025},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {AI for structured manuals, domain adaptation, LoRA-based fine-tuning, multimodal RAG, question-answering system, technical documentation},
	pages = {8387},
	file = {Full Text PDF:files/625/Nam et al. - 2025 - LoRA-Tuned Multimodal RAG System for Technical Manual QA A Case Study on Hyundai Staria.pdf:application/pdf},
}

@article{knollmeyer_document_2025,
	title = {Document {GraphRAG}: {Knowledge} {Graph} {Enhanced} {Retrieval} {Augmented} {Generation} for {Document} {Question} {Answering} {Within} the {Manufacturing} {Domain}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	shorttitle = {Document {GraphRAG}},
	url = {https://www.mdpi.com/2079-9292/14/11/2102},
	doi = {10.3390/electronics14112102},
	abstract = {Retrieval-Augmented Generation (RAG) systems have shown significant potential for domain-specific Question Answering (QA) tasks, although persistent challenges in retrieval precision and context selection continue to hinder their effectiveness. This study introduces Document Graph RAG (GraphRAG), a novel framework that bolsters retrieval robustness and enhances answer generation by incorporating Knowledge Graphs (KGs) built upon a document’s intrinsic structure into the RAG pipeline. Through the application of the Design Science Research methodology, we systematically design, implement, and evaluate GraphRAG, leveraging graph-based document structuring and a keyword-based semantic linking mechanism to improve retrieval quality. The evaluation, conducted on well-established datasets including SQuAD, HotpotQA, and a newly developed manufacturing dataset, demonstrates consistent performance gains over a naive RAG baseline across both retrieval and generation metrics. The results indicate that GraphRAG improves Context Relevance metrics, with task-dependent optimizations for chunk size, keyword density, and top-k retrieval further enhancing performance. Notably, multi-hop questions benefit most from GraphRAG’s structured retrieval strategy, highlighting its advantages in complex reasoning tasks.},
	language = {en},
	number = {11},
	urldate = {2025-12-19},
	journal = {Electronics},
	author = {Knollmeyer, Simon and Caymazer, Oğuz and Grossmann, Daniel},
	month = jan,
	year = {2025},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {design science research, knowledge graph, large language model, manufacturing, production domain, Retrieval-Augmented Generation},
	pages = {2102},
	file = {Full Text PDF:files/727/Knollmeyer et al. - 2025 - Document GraphRAG Knowledge Graph Enhanced Retrieval Augmented Generation for Document Question Ans.pdf:application/pdf},
}

@inproceedings{liu_hm-rag_2025,
	address = {Dublin Ireland},
	title = {{HM}-{RAG}: {Hierarchical} {Multi}-{Agent} {Multimodal} {Retrieval} {Augmented} {Generation}},
	isbn = {979-8-4007-2035-2},
	shorttitle = {{HM}-{RAG}},
	url = {https://dl.acm.org/doi/10.1145/3746027.3754761},
	doi = {10.1145/3746027.3754761},
	language = {en},
	urldate = {2025-12-30},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Liu, Pei and Liu, Xin and Yao, Ruoyu and Liu, Junming and Meng, Siyuan and Wang, Ding and Ma, Jun},
	month = oct,
	year = {2025},
	pages = {2781--2790},
	file = {PDF:files/798/Liu et al. - 2025 - HM-RAG Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation.pdf:application/pdf;Snapshot:files/797/3746027.html:text/html},
}
