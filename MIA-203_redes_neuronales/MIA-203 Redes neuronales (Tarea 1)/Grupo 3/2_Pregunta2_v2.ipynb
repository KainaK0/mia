{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKLO9d1lS0zi"
      },
      "source": [
        "# **Verificacion con Micrograd**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1GlVsGUjS0zo"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class Value:\n",
        "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
        "\n",
        "    def __init__(self, data, _children=(), _op=''):\n",
        "        self.data = data\n",
        "        self.grad = 0\n",
        "        # internal variables used for autograd graph construction\n",
        "        self._backward = lambda: None\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += out.grad\n",
        "            other.grad += out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "        out = Value(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (other * self.data**(other-1)) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def relu(self):\n",
        "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (out.data > 0) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    ###---Added OmarP\n",
        "\n",
        "    def tanh(self):\n",
        "        x = self.data\n",
        "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
        "        out = Value(t, (self,), 'tanh')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (1 - t**2) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    ####\n",
        "\n",
        "    def backward(self):\n",
        "\n",
        "        # topological order all of the children in the graph\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "\n",
        "        # go one variable at a time and apply the chain rule to get its gradient\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            v._backward()\n",
        "\n",
        "    def __neg__(self): # -self\n",
        "        return self * -1\n",
        "\n",
        "    def __radd__(self, other): # other + self\n",
        "        return self + other\n",
        "\n",
        "    def __sub__(self, other): # self - other\n",
        "        return self + (-other)\n",
        "\n",
        "    def __rsub__(self, other): # other - self\n",
        "        return other + (-self)\n",
        "\n",
        "    def __rmul__(self, other): # other * self\n",
        "        return self * other\n",
        "\n",
        "    def __truediv__(self, other): # self / other\n",
        "        return self * other**-1\n",
        "\n",
        "    def __rtruediv__(self, other): # other / self\n",
        "        return other * self**-1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
        "\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, num_inputs, weights=None, bias=None, activation='relu'):\n",
        "        # Inicializar Pesos y valores de vias\n",
        "        self.weights = [Value(w) for w in weights]\n",
        "        self.bias = Value(bias)\n",
        "        self.activation = activation\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        act = sum((w * x for w, x in zip(self.weights, inputs)), self.bias)\n",
        "        # Aplicar la funcion de activacion correspondiente\n",
        "        if self.activation == 'relu':\n",
        "            return act.relu()\n",
        "        elif self.activation == 'tanh':\n",
        "            return act.tanh()\n",
        "        else:\n",
        "            return act\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.weights + [self.bias]\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, num_inputs, num_neurons, weights_list=None, biases=None, activation='relu'):\n",
        "        self.neurons = []\n",
        "        for i in range(num_neurons):\n",
        "            weights = weights_list[i]\n",
        "            bias = biases[i]\n",
        "            self.neurons.append(Neuron(num_inputs, weights=weights, bias=bias, activation=activation))\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return [neuron(inputs) for neuron in self.neurons]\n",
        "\n",
        "    def parameters(self):\n",
        "        return [param for neuron in self.neurons for param in neuron.parameters()]\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, num_inputs, hidden_layer_sizes, weights=None, biases=None):\n",
        "        sizes = [num_inputs] + hidden_layer_sizes + [1]\n",
        "        self.layers = []\n",
        "\n",
        "        # Crear capas con su respectiva funcion de activacion\n",
        "        for i in range(len(sizes) - 1):\n",
        "            # Usar tanh para la ultima capa, relu para capas ocultas\n",
        "            activation = 'tanh' if i == len(sizes) - 2 else 'relu'\n",
        "            layer_weights = weights[i]\n",
        "            layer_biases = biases[i]\n",
        "            self.layers.append(Layer(sizes[i], sizes[i + 1],\n",
        "                                   weights_list=layer_weights,\n",
        "                                   biases=layer_biases,\n",
        "                                   activation=activation))\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        for layer in self.layers:\n",
        "            inputs = layer(inputs)\n",
        "        return inputs[0]\n",
        "\n",
        "    def parameters(self):\n",
        "        return [param for layer in self.layers for param in layer.parameters()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvzX-2SaS0zr",
        "outputId": "d86c2594-c21c-41bd-9ad5-a56a9e53efad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valores de primer Forward antes de aplicar tanh:\n",
            "0.25597836133221197\n",
            "-0.007699847825942281\n",
            "0.23266012946007794\n",
            "0.011799452353168314\n",
            "Epoch [1/10], Loss: 1.0086\n",
            "Epoch [2/10], Loss: 0.9768\n",
            "Epoch [3/10], Loss: 0.9490\n",
            "Epoch [4/10], Loss: 0.9243\n",
            "Epoch [5/10], Loss: 0.9020\n",
            "Epoch [6/10], Loss: 0.8815\n",
            "Epoch [7/10], Loss: 0.8624\n",
            "Epoch [8/10], Loss: 0.8448\n",
            "Epoch [9/10], Loss: 0.8301\n",
            "Epoch [10/10], Loss: 0.8157\n",
            "\n",
            "Prediciones finales (despues de tanh):\n",
            "Prediccion: 0.4902\n",
            "Prediccion: -0.0761\n",
            "Prediccion: 0.2319\n",
            "Prediccion: 0.2394\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Definicion de pesos y bias iniciales\n",
        "    initial_weights = [\n",
        "        # Primera capa (3 inputs -> 4 neurons)\n",
        "        [\n",
        "            [0.1, 0.5, -0.2],  # weights para primera neurona\n",
        "            [-0.3, 0.4, 0.1],  # weights para segunda neurona\n",
        "            [0.2, -0.1, -0.5],  # weights para tercera neurona\n",
        "            [0.05, 0.3, 0.2]   # weights para  cuarta neurona\n",
        "        ],\n",
        "        # Segunda Capa (4 inputs -> 4 neuronas)\n",
        "        [\n",
        "            [0.1, 0.4, -0.1, 0.2],\n",
        "            [-0.2, 0.3, 0.2, 0.4],\n",
        "            [0.3, -0.1, -0.4, -0.1],\n",
        "            [0.45, 0.2, 0.35, 0.3]\n",
        "        ],\n",
        "        # Capa de salida (4 inputs -> 1 neurona)\n",
        "        [\n",
        "            [0.5, -0.4, 0.4, -0.2]\n",
        "        ]\n",
        "    ]\n",
        "\n",
        "    initial_biases = [\n",
        "        [0.05, -0.07, 0.02, -0.1],  # biases para primera capa\n",
        "        [0.05, 0.1, 0.3, -0.1],  # biases para segunda capa\n",
        "        [0.1]                   # bias para ultima capa\n",
        "    ]\n",
        "\n",
        "    model = MLP(3, [4, 4], weights=initial_weights, biases=initial_biases)\n",
        "\n",
        "    # Data de prueba\n",
        "    X_s = [\n",
        "        [2.5, 3.5, -0.5],\n",
        "        [4.0, -1.0, 0.5],\n",
        "        [0.5, 1.5, 1.0],\n",
        "        [3.0, 2.0, -1.5]\n",
        "    ]\n",
        "    y_s = [1.0, -1.0, -1.0, 1.0]\n",
        "\n",
        "    # Parametros de entrenamiento\n",
        "    learning_rate = 0.01\n",
        "    epochs = 10\n",
        "\n",
        "    ##--- Valores de primer Forward\n",
        "    y_pred_1 = [model([Value(x) for x in x_s]) for x_s in X_s]\n",
        "    print(\"Valores de primer Forward antes de aplicar tanh:\")\n",
        "    for y_value in y_pred_1:\n",
        "        print(y_value.data)\n",
        "\n",
        "\n",
        "\n",
        "    # Entrenamiento\n",
        "    for epoch in range(epochs):\n",
        "        # Forward\n",
        "        y_pred = [model([Value(x) for x in x_s]) for x_s in X_s]\n",
        "\n",
        "        # Perdida\n",
        "        loss = sum((y_pred_i - Value(y_s_i))**2 for y_pred_i, y_s_i in zip(y_pred, y_s)) / len(y_s)\n",
        "\n",
        "        # Backward\n",
        "        for param in model.parameters():\n",
        "            param.grad = 0.0\n",
        "        loss.backward()\n",
        "\n",
        "        # Actualizar paramentros\n",
        "        for param in model.parameters():\n",
        "            param.data -= learning_rate * param.grad\n",
        "\n",
        "        if (epoch + 1) % 1 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.data:.4f}')\n",
        "\n",
        "    # Predicciones finales\n",
        "    print(\"\\nPrediciones finales (despues de tanh):\")\n",
        "    for x_s in X_s:\n",
        "        pred = model([Value(x) for x in x_s])\n",
        "        print(f\"Prediccion: {pred.data:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xiv-aIebS0zt"
      },
      "source": [
        "# **Verificacion con Pytorch:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npj-Eu-QS0zu",
        "outputId": "a7600fcf-419a-4219-dfc8-1718ae70a524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicciones iniciales:\n",
            "tensor([[-0.3817],\n",
            "        [-0.6963],\n",
            "        [-0.4377],\n",
            "        [-0.3915]])\n",
            "Epoch [100/1000], Loss: 0.3154\n",
            "Epoch [200/1000], Loss: 0.1306\n",
            "Epoch [300/1000], Loss: 0.0702\n",
            "Epoch [400/1000], Loss: 0.0452\n",
            "Epoch [500/1000], Loss: 0.0325\n",
            "Epoch [600/1000], Loss: 0.0249\n",
            "Epoch [700/1000], Loss: 0.0201\n",
            "Epoch [800/1000], Loss: 0.0167\n",
            "Epoch [900/1000], Loss: 0.0143\n",
            "Epoch [1000/1000], Loss: 0.0124\n",
            "\n",
            "Predicciones finales despues de entrenamiento:\n",
            "tensor([[ 0.8525],\n",
            "        [-0.9793],\n",
            "        [-0.9249],\n",
            "        [ 0.8525]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#Data de prueba\n",
        "X_s = torch.tensor([[2.5, 3.5, -0.5],\n",
        "                    [4.0, -1.0, 0.5],\n",
        "                    [0.5, 1.5, 1.0],\n",
        "                    [3.0, 2.0, -1.5]], dtype=torch.float32)\n",
        "\n",
        "y_s = torch.tensor([[1.0],\n",
        "                    [-1.0],\n",
        "                    [-1.0],\n",
        "                    [1.0]], dtype=torch.float32)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(3, 4)\n",
        "        self.layer2 = nn.Linear(4, 4)\n",
        "        self.layer3 = nn.Linear(4, 1)\n",
        "\n",
        "        self.activation = nn.ReLU()   # Activation function (ReLU for capas ocultas)\n",
        "        self.output_activation = nn.Tanh()  # Tanh activation para la salida\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.layer1(x))\n",
        "        x = self.activation(self.layer2(x))\n",
        "        x = self.output_activation(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "#\n",
        "model = MLP()\n",
        "\n",
        "# Funcion Loss y optimizador\n",
        "criterion = nn.MSELoss()  # Mean Squared Error\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent con learning rate 0.01\n",
        "\n",
        "# Forward propagation (initial predictions)\n",
        "print(\"Predicciones iniciales:\")\n",
        "with torch.no_grad():\n",
        "    predictions = model(X_s)\n",
        "    print(predictions)\n",
        "\n",
        "#\n",
        "epochs = 1000\n",
        "\n",
        "# Training\n",
        "for epoch in range(epochs):\n",
        "    # Forward\n",
        "    y_pred = model(X_s)\n",
        "\n",
        "    # Computar loss\n",
        "    loss = criterion(y_pred, y_s)\n",
        "\n",
        "    # Backward propagation\n",
        "    optimizer.zero_grad()  #\n",
        "    loss.backward()  # Backpropagate la perdida\n",
        "\n",
        "    # actualizacion de pesos\n",
        "    optimizer.step()  #\n",
        "\n",
        "    #\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Final predictions after training\n",
        "print(\"\\nPredicciones finales despues de entrenamiento:\")\n",
        "with torch.no_grad():\n",
        "    final_predictions = model(X_s)\n",
        "    print(final_predictions)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}