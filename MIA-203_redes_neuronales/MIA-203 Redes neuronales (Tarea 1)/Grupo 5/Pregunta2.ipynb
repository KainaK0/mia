{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install micrograd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRp3xx6wsaLY",
        "outputId": "f3275bd9-7436-4931-c811-8769a1c89083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting micrograd\n",
            "  Downloading micrograd-0.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading micrograd-0.1.0-py3-none-any.whl (4.9 kB)\n",
            "Installing collected packages: micrograd\n",
            "Successfully installed micrograd-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from micrograd.engine import Value\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "Zc68KIbjtuDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función de activación Sigmoid y su derivada\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "# Función de activación ReLU y su derivada\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Inicializar datos\n",
        "X_s = np.array([\n",
        "    [2.5, 3.5, -0.5],\n",
        "    [4.0, -1.0, 0.5],\n",
        "    [0.5, 1.5, 1.0],\n",
        "    [3.0, 2.0, -1.5]\n",
        "])\n",
        "\n",
        "y_s = np.array([[1.0], [0.0], [0.0], [1.0]])\n",
        "\n",
        "# Definir arquitectura de la red\n",
        "input_size = 3\n",
        "hidden_size = 4\n",
        "output_size = 1\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Inicializar pesos y sesgos\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(hidden_size, input_size) * 0.1\n",
        "b1 = np.random.randn(hidden_size, 1) * 0.1\n",
        "W2 = np.random.randn(hidden_size, hidden_size) * 0.1\n",
        "b2 = np.random.randn(hidden_size, 1) * 0.1\n",
        "W3 = np.random.randn(output_size, hidden_size) * 0.1\n",
        "b3 = np.random.randn(output_size, 1) * 0.1\n",
        "\n",
        "# Propagación hacia adelante\n",
        "def forward_propagation(X):\n",
        "    Z1 = np.dot(W1, X.T) + b1  # Primera capa oculta\n",
        "    A1 = relu(Z1)\n",
        "\n",
        "    Z2 = np.dot(W2, A1) + b2  # Segunda capa oculta\n",
        "    A2 = relu(Z2)\n",
        "\n",
        "    Z3 = np.dot(W3, A2) + b3  # Capa de salida\n",
        "    A3 = sigmoid(Z3)\n",
        "\n",
        "    return Z1, A1, Z2, A2, Z3, A3\n",
        "\n",
        "# Propagación hacia atrás\n",
        "def backward_propagation(X, y, Z1, A1, Z2, A2, Z3, A3):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    dZ3 = A3 - y.T\n",
        "    dW3 = (1 / m) * np.dot(dZ3, A2.T)\n",
        "    db3 = (1 / m) * np.sum(dZ3, axis=1, keepdims=True)\n",
        "\n",
        "    dA2 = np.dot(W3.T, dZ3)\n",
        "    dZ2 = dA2 * relu_derivative(Z2)\n",
        "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "    dA1 = np.dot(W2.T, dZ2)\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = (1 / m) * np.dot(dZ1, X)\n",
        "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "    return dW1, db1, dW2, db2, dW3, db3\n",
        "\n",
        "# Actualizar parámetros\n",
        "def update_parameters(dW1, db1, dW2, db2, dW3, db3):\n",
        "    global W1, b1, W2, b2, W3, b3\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W3 -= learning_rate * dW3\n",
        "    b3 -= learning_rate * db3\n",
        "\n",
        "# Bucle de entrenamiento\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    # Propagación hacia adelante\n",
        "    Z1, A1, Z2, A2, Z3, A3 = forward_propagation(X_s)\n",
        "\n",
        "    # Calcular la pérdida (error cuadrático medio)\n",
        "    loss = np.mean((A3 - y_s.T) ** 2)\n",
        "\n",
        "    # Propagación hacia atrás\n",
        "    dW1, db1, dW2, db2, dW3, db3 = backward_propagation(X_s, y_s, Z1, A1, Z2, A2, Z3, A3)\n",
        "\n",
        "    # Actualizar pesos y sesgos\n",
        "    update_parameters(dW1, db1, dW2, db2, dW3, db3)\n",
        "\n",
        "    # Imprimir la pérdida cada 100 épocas\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Salida final\n",
        "_, _, _, _, _, A3 = forward_propagation(X_s)\n",
        "print(\"\\Salida:\")\n",
        "print(A3.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9TnPp6Xs8bm",
        "outputId": "889b637b-1dfb-4a57-c658-a19fcf6215d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.24971877880042945\n",
            "Epoch 100, Loss: 0.24933660645515116\n",
            "Epoch 200, Loss: 0.24892306028616218\n",
            "Epoch 300, Loss: 0.248469448364932\n",
            "Epoch 400, Loss: 0.24787026357066994\n",
            "Epoch 500, Loss: 0.24703075238069847\n",
            "Epoch 600, Loss: 0.2458039996735075\n",
            "Epoch 700, Loss: 0.243949489101394\n",
            "Epoch 800, Loss: 0.2410741738384928\n",
            "Epoch 900, Loss: 0.2365438186789336\n",
            "\\Salida:\n",
            "[[0.50992698]\n",
            " [0.46374528]\n",
            " [0.47275238]\n",
            " [0.50986337]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificación usando Micrograd\n",
        "a = Value(-4.0)\n",
        "b = Value(2.0)\n",
        "c = a + b\n",
        "d = a * b + b * a + b ** 3\n",
        "c += c + 1 + c + (-a)\n",
        "d += d * 2 + (b - a).relu()\n",
        "d += 3 * d + (b - a).relu()\n",
        "e = c - d\n",
        "f = e ** 2\n",
        "g = f / 2.0\n",
        "g += 10.0 / f\n",
        "print(f\"{g.data:.4f}\")  # imprime el resultado de esta pasada hacia adelante\n",
        "g.backward()\n",
        "print(f\"{a.grad:.4f}\")  # imprime el valor numérico de dg/da\n",
        "print(f\"{b.grad:.4f}\")  # imprime el valor numérico de dg/db\n",
        "\n",
        "# Verificación usando PyTorch\n",
        "X_torch = torch.tensor(X_s, dtype=torch.float32)\n",
        "y_torch = torch.tensor(y_s, dtype=torch.float32)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, output_size),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Bucle de entrenamiento para PyTorch\n",
        "for epoch in range(epochs):\n",
        "    # Paso hacia adelante\n",
        "    outputs = model(X_torch)\n",
        "    loss = criterion(outputs, y_torch)\n",
        "\n",
        "    # Paso hacia atrás y optimización\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Imprimir la pérdida cada 100 épocas\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"[PyTorch] Época {epoch}, Pérdida: {loss.item()}\")\n",
        "\n",
        "# Salida final usando PyTorch\n",
        "with torch.no_grad():\n",
        "    final_output = model(X_torch)\n",
        "    print(\"\\nSalida final usando PyTorch:\")\n",
        "    print(final_output.numpy())\n",
        "\n",
        "# Función de activación Sigmoid y su derivada\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "# Función de activación ReLU y su derivada\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Inicializar datos\n",
        "X_s = np.array([\n",
        "    [2.5, 3.5, -0.5],\n",
        "    [4.0, -1.0, 0.5],\n",
        "    [0.5, 1.5, 1.0],\n",
        "    [3.0, 2.0, -1.5]\n",
        "])\n",
        "\n",
        "y_s = np.array([[1.0], [0.0], [0.0], [1.0]])\n",
        "\n",
        "# Definir arquitectura de la red\n",
        "input_size = 3\n",
        "hidden_size = 4\n",
        "output_size = 1\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Inicializar pesos y sesgos\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(hidden_size, input_size) * 0.1\n",
        "b1 = np.random.randn(hidden_size, 1) * 0.1\n",
        "W2 = np.random.randn(hidden_size, hidden_size) * 0.1\n",
        "b2 = np.random.randn(hidden_size, 1) * 0.1\n",
        "W3 = np.random.randn(output_size, hidden_size) * 0.1\n",
        "b3 = np.random.randn(output_size, 1) * 0.1\n",
        "\n",
        "# Propagación hacia adelante\n",
        "def forward_propagation(X):\n",
        "    Z1 = np.dot(W1, X.T) + b1  # Primera capa oculta\n",
        "    A1 = relu(Z1)\n",
        "\n",
        "    Z2 = np.dot(W2, A1) + b2  # Segunda capa oculta\n",
        "    A2 = relu(Z2)\n",
        "\n",
        "    Z3 = np.dot(W3, A2) + b3  # Capa de salida\n",
        "    A3 = sigmoid(Z3)\n",
        "\n",
        "    return Z1, A1, Z2, A2, Z3, A3\n",
        "\n",
        "# Propagación hacia atrás\n",
        "def backward_propagation(X, y, Z1, A1, Z2, A2, Z3, A3):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    dZ3 = A3 - y.T\n",
        "    dW3 = (1 / m) * np.dot(dZ3, A2.T)\n",
        "    db3 = (1 / m) * np.sum(dZ3, axis=1, keepdims=True)\n",
        "\n",
        "    dA2 = np.dot(W3.T, dZ3)\n",
        "    dZ2 = dA2 * relu_derivative(Z2)\n",
        "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "    dA1 = np.dot(W2.T, dZ2)\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = (1 / m) * np.dot(dZ1, X)\n",
        "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "    return dW1, db1, dW2, db2, dW3, db3\n",
        "\n",
        "# Actualizar parámetros\n",
        "def update_parameters(dW1, db1, dW2, db2, dW3, db3):\n",
        "    global W1, b1, W2, b2, W3, b3\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W3 -= learning_rate * dW3\n",
        "    b3 -= learning_rate * db3\n",
        "\n",
        "# Bucle de entrenamiento\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    # Propagación hacia adelante\n",
        "    Z1, A1, Z2, A2, Z3, A3 = forward_propagation(X_s)\n",
        "\n",
        "    # Calcular la pérdida (error cuadrático medio)\n",
        "    loss = np.mean((A3 - y_s.T) ** 2)\n",
        "\n",
        "    # Propagación hacia atrás\n",
        "    dW1, db1, dW2, db2, dW3, db3 = backward_propagation(X_s, y_s, Z1, A1, Z2, A2, Z3, A3)\n",
        "\n",
        "    # Actualizar pesos y sesgos\n",
        "    update_parameters(dW1, db1, dW2, db2, dW3, db3)\n",
        "\n",
        "    # Imprimir la pérdida cada 100 épocas\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Salida final\n",
        "_, _, _, _, _, A3 = forward_propagation(X_s)\n",
        "print(\"\\nSalida final:\")\n",
        "print(A3.T)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyhbW5yEtQAB",
        "outputId": "d0d6ed14-e0c6-414d-dbeb-8ffbb6259b52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2112.5024\n",
            "-2664.9970\n",
            "-3249.9964\n",
            "[PyTorch] Época 0, Pérdida: 0.22794649004936218\n",
            "[PyTorch] Época 100, Pérdida: 0.21062523126602173\n",
            "[PyTorch] Época 200, Pérdida: 0.19227643311023712\n",
            "[PyTorch] Época 300, Pérdida: 0.1730826199054718\n",
            "[PyTorch] Época 400, Pérdida: 0.15566685795783997\n",
            "[PyTorch] Época 500, Pérdida: 0.14137528836727142\n",
            "[PyTorch] Época 600, Pérdida: 0.12852926552295685\n",
            "[PyTorch] Época 700, Pérdida: 0.11658534407615662\n",
            "[PyTorch] Época 800, Pérdida: 0.10531941056251526\n",
            "[PyTorch] Época 900, Pérdida: 0.09473837912082672\n",
            "\n",
            "Salida final usando PyTorch:\n",
            "[[0.64221144]\n",
            " [0.06450532]\n",
            " [0.2831343 ]\n",
            " [0.6428142 ]]\n",
            "Epoch 0, Loss: 0.24971877880042945\n",
            "Epoch 100, Loss: 0.24933660645515116\n",
            "Epoch 200, Loss: 0.24892306028616218\n",
            "Epoch 300, Loss: 0.248469448364932\n",
            "Epoch 400, Loss: 0.24787026357066994\n",
            "Epoch 500, Loss: 0.24703075238069847\n",
            "Epoch 600, Loss: 0.2458039996735075\n",
            "Epoch 700, Loss: 0.243949489101394\n",
            "Epoch 800, Loss: 0.2410741738384928\n",
            "Epoch 900, Loss: 0.2365438186789336\n",
            "\n",
            "Salida final:\n",
            "[[0.50992698]\n",
            " [0.46374528]\n",
            " [0.47275238]\n",
            " [0.50986337]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TfHhP2Uc_WaF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}