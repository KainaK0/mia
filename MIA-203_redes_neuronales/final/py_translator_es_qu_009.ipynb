{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model:int, vocabulary_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocabulary_size\n",
    "        self.embedding = nn.Embedding(vocabulary_size, d_model)\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, sequence_len:int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = sequence_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(sequence_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, sequence_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self,x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, parameters_shape: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(parameters_shape)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(parameters_shape)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "    \n",
    "        def forward(self, x, sublayer):\n",
    "            x = self.norm(x)\n",
    "            x = self.dropout(sublayer(x)) + x\n",
    "            return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N_layers: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N_layers):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N_layers):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mj-callomamani-b\u001b[0m (\u001b[33msalcantaratnaist\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\kainak0\\_netrc\n"
     ]
    }
   ],
   "source": [
    "# from model import build_transformer\n",
    "# from dataset import BilingualDataset, causal_mask\n",
    "# from config import get_config, get_weights_file_path\n",
    "\n",
    "# import torchtext.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset,DatasetDict\n",
    "import datasets\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import math\n",
    "import wandb\n",
    "wandb.login(key=\"5364c808c25399d37be50f8e9227b609dad82d1b\")\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['es', 'qu'],\n",
       "    num_rows: 102747\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ds = load_dataset(\"somosnlp-hackathon-2022/spanish-to-quechua\", )\n",
    "# ds_raw = ds['train']\n",
    "# ds_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"allenai/nllb\", \"quy_Latn-spa_Latn\")\n",
    "ds['train']['translation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw = {}\n",
    "ds_raw['qu'] = [x['quy_Latn'] for x in ds['train']['translation']]\n",
    "ds_raw['es'] = [x['spa_Latn'] for x in ds['train']['translation']]\n",
    "ds_raw = datasets.Dataset.from_dict(ds_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102747"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_raw['es'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(step = 1, d_model = 512, warmup_steps = 8000):\n",
    "    \"\"\"\n",
    "    The LR schedule. This version below is twice the definition in the paper, as used in the official T2T repository.\n",
    "\n",
    "    :param step: training step number\n",
    "    :param d_model: size of vectors throughout the transformer model\n",
    "    :param warmup_steps: number of warmup steps where learning rate is increased linearly; twice the value in the paper, as in the official T2T repo\n",
    "    :return: updated learning rate\n",
    "    \"\"\"\n",
    "    lr =  2.0 * math.pow(d_model, -0.5) * min(math.pow(step, -0.5), step * math.pow(warmup_steps, -1.5))\n",
    "\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 48,\n",
    "        \"num_epochs\": 10,\n",
    "        \"lr\": get_lr(),\n",
    "        \"seq_len\": 200,\n",
    "        \"d_model\": 512,\n",
    "        'N_layers': 2,\n",
    "        'heads': 8,\n",
    "        'dropout': 0.1,\n",
    "        'ffn_hidden': 2048,\n",
    "        'warmup_steps': 8000,\n",
    "        'min_sentence_len': 40,\n",
    "        'max_sentence_len': 200,\n",
    "        'vocab_size_scr':'',\n",
    "        'vocab_size_tgt':'',\n",
    "        'tokenizer':BPE,\n",
    "        'tokenizer_trainer':BpeTrainer,\n",
    "        \"datasource\": 'nllb',\n",
    "        \"version\": '009',\n",
    "        \"lang_src\": \"es\",\n",
    "        \"lang_tgt\": \"qu\",\n",
    "        \"model_folder\": \"weights_es_qu{0}\",\n",
    "        \"model_basename\": \"tmodel_es_qu{0}\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_es_qu_{0}{1}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel_es_qu{0}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_lr(optimizer, new_lr):\n",
    "    \"\"\"\n",
    "    Scale learning rate by a specified factor.\n",
    "\n",
    "    :param optimizer: optimizer whose learning rate must be changed\n",
    "    :param new_lr: new learning rate\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2352647110032733e-07"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest weights file in the weights folder\n",
    "def latest_weights_file_path(config):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder'].format(config['version'])}\"\n",
    "    model_filename = f\"{config['model_basename'].format(config['version'])}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder'].format(config['version'])}\"\n",
    "    model_filename = f\"{config['model_basename'].format(config['version'])}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def hist_len_sentence(ds_raw, lang, min_sentence_len, max_sentence_len):\n",
    "    len_sentence = [len(x) for x in ds_raw[lang]]\n",
    "    len_sentence_ranged = [x for x in len_sentence if min_sentence_len < x < max_sentence_len]\n",
    "\n",
    "    # Generate sample data\n",
    "    data = np.array(len_sentence_ranged)\n",
    "\n",
    "    # Create the figure and axis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.hist(data, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('Normal Distribution Histogram', fontsize=14)\n",
    "    plt.xlabel('Value', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(np.arange(0, max_sentence_len, 20))\n",
    "    # Add some padding to the layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97th percentile length quechua: 194.0\n",
      "97th percentile length spanish: 199.0\n"
     ]
    }
   ],
   "source": [
    "PERCENTILE = 97\n",
    "print( f\"{PERCENTILE}th percentile length quechua: {np.percentile([len(x) for x in ds_raw['qu']], PERCENTILE)}\" )\n",
    "print( f\"{PERCENTILE}th percentile length spanish: {np.percentile([len(x) for x in ds_raw['es']], PERCENTILE)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_len_sentence(ds_raw, lang = config['lang_tgt'], \n",
    "                  min_sentence_len = config['min_sentence_len'], \n",
    "                  max_sentence_len = config['max_sentence_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 102747/102747 [00:00<00:00, 412685.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def filter_by_length(dataset, min_len=20, max_len=200):\n",
    "    def length_filter(example):\n",
    "        # Check length for all string/list features\n",
    "        for key, value in example.items():\n",
    "            if isinstance(value, (str, list)):\n",
    "                if len(value) < min_len or len(value) > max_len:\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    filtered_dataset = dataset.filter(length_filter)\n",
    "    return filtered_dataset\n",
    "\n",
    "# Usage example: \n",
    "filtered_ds_raw = filter_by_length(ds_raw, min_len = config['min_sentence_len'], max_len = config['max_sentence_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item[lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(ds, lang:str,version:str):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang,version))\n",
    "    # if not Path.exists(tokenizer_path):\n",
    "    if True:\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(config['tokenizer'](unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = config['tokenizer_trainer'](special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair[self.src_lang]\n",
    "        tgt_text = src_target_pair[self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "    \n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, beam_size, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_initial_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "\n",
    "    # Create a candidate list\n",
    "    candidates = [(decoder_initial_input, 1)]\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # If a candidate has reached the maximum length, it means we have run the decoding for at least max_len iterations, so stop the search\n",
    "        if any([cand.size(1) == max_len for cand, _ in candidates]):\n",
    "            break\n",
    "\n",
    "        # Create a new list of candidates\n",
    "        new_candidates = []\n",
    "\n",
    "        for candidate, score in candidates:\n",
    "\n",
    "            # Do not expand candidates that have reached the eos token\n",
    "            if candidate[0][-1].item() == eos_idx:\n",
    "                continue\n",
    "\n",
    "            # Build the candidate's mask\n",
    "            candidate_mask = causal_mask(candidate.size(1)).type_as(source_mask).to(device)\n",
    "            # calculate output\n",
    "            out = model.decode(encoder_output, source_mask, candidate, candidate_mask)\n",
    "            # get next token probabilities\n",
    "            prob = model.project(out[:, -1])\n",
    "            # get the top k candidates\n",
    "            topk_prob, topk_idx = torch.topk(prob, beam_size, dim=1)\n",
    "            for i in range(beam_size):\n",
    "                # for each of the top k candidates, get the token and its probability\n",
    "                token = topk_idx[0][i].unsqueeze(0).unsqueeze(0)\n",
    "                token_prob = topk_prob[0][i].item()\n",
    "                # create a new candidate by appending the token to the current candidate\n",
    "                new_candidate = torch.cat([candidate, token], dim=1)\n",
    "                # We sum the log probabilities because the probabilities are in log space\n",
    "                new_candidates.append((new_candidate, score + token_prob))\n",
    "\n",
    "        # Sort the new candidates by their score\n",
    "        candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
    "        # Keep only the top k candidates\n",
    "        candidates = candidates[:beam_size]\n",
    "\n",
    "        # If all the candidates have reached the eos token, stop\n",
    "        if all([cand[0][-1].item() == eos_idx for cand, _ in candidates]):\n",
    "            break\n",
    "\n",
    "    # Return the best candidate\n",
    "    return candidates[0][0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted_greedy = []\n",
    "    predicted_beam = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "            model_out_beam = beam_search_decode(model, 3, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text_beam = tokenizer_tgt.decode(model_out_beam.detach().cpu().numpy())\n",
    "            model_out_text_greedy = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted_greedy.append(model_out_text_greedy)\n",
    "            predicted_beam.append(model_out_text_beam)\n",
    "            \n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED GREEDY: ':>12}{model_out_text_greedy}\")\n",
    "            print_msg(f\"{f'PREDICTED BEAM:   ':>12}{model_out_text_beam}\")\n",
    "\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "    \n",
    "    \n",
    "    # Evaluate the character error rate\n",
    "    # Compute the char error rate \n",
    "    metric_greedy = torchmetrics.CharErrorRate()\n",
    "    cer = metric_greedy(predicted_greedy, expected)\n",
    "    wandb.log({'validation_greedy/cer': cer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the word error rate\n",
    "    metric_greedy = torchmetrics.WordErrorRate()\n",
    "    wer = metric_greedy(predicted_greedy, expected)\n",
    "    wandb.log({'validation_greedy/wer': wer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the BLEU metric\n",
    "    metric_greedy = torchmetrics.BLEUScore()\n",
    "    bleu = metric_greedy(predicted_greedy, expected)\n",
    "    wandb.log({'validation_greedy/BLEU': bleu, 'global_step': global_step})\n",
    "\n",
    "    # Evaluate the character error rate\n",
    "    # Compute the char error rate \n",
    "    metric_beam = torchmetrics.CharErrorRate()\n",
    "    cer = metric_beam(predicted_beam, expected)\n",
    "    wandb.log({'validation_beam/cer': cer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the word error rate\n",
    "    metric = torchmetrics.WordErrorRate()\n",
    "    wer = metric_beam(predicted_beam, expected)\n",
    "    wandb.log({'validation_beam/wer': wer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the BLEU metric\n",
    "    metric = torchmetrics.BLEUScore()\n",
    "    bleu = metric_beam(predicted_beam, expected)\n",
    "    wandb.log({'validation_beam/BLEU': bleu, 'global_step': global_step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'],\n",
    "        N_layers=config['N_layers'], h = config['heads'], dropout = config['dropout'], d_ff = config['ffn_hidden'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config,train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt ):\n",
    "    # Define the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    if (device == 'cuda'):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "     \n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config['datasource']}_{config['model_folder'].format(config['version'])}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9, betas = (0.9, 0.98))\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "\n",
    "    if model_filename:\n",
    "    # if False:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    # define our custom x axis metric\n",
    "    wandb.define_metric(\"global_step\")\n",
    "    # define which metrics will be plotted against it\n",
    "    wandb.define_metric(\"validation_greedy/*\", step_metric=\"global_step\")\n",
    "    wandb.define_metric(\"validation_beam/*\", step_metric=\"global_step\")\n",
    "    wandb.define_metric(\"train/*\", step_metric=\"global_step\")\n",
    "    # wandb.define_metric(\"ln_rate/*\", step_metric=\"global_step\")\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "\n",
    "\n",
    "            # Log the loss\n",
    "            wandb.log({'train/loss': loss.item(), 'global_step': global_step})\n",
    "            # wandb.log({'train/its': batch.format_dict['rate'], 'global_step': global_step})\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            global_step += 1\n",
    "            \n",
    "            # Log the loss\n",
    "            wandb.log({'train/ln_rate': optimizer.param_groups[0]['lr'], 'global_step': global_step})\n",
    "            if (batch_iterator.n +1) % config['batch_size'] == 0:\n",
    "                # Update the weights\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                lr_updated = get_lr(step=global_step, d_model=config['d_model'], warmup_steps=config['warmup_steps'])\n",
    "\n",
    "                change_lr(optimizer, new_lr=lr_updated)\n",
    "            \n",
    "            batch_iterator.set_postfix({\n",
    "                \"loss\": f\"{loss.item():6.3f}\",\n",
    "                'lr' : f\"{optimizer.param_groups[0]['lr']:.6g}\"\n",
    "                                        })\n",
    "            \n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step)\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_tgt = get_or_build_tokenizer( filtered_ds_raw, config['lang_tgt'],config['version'])\n",
    "tokenizer_src =  get_or_build_tokenizer( filtered_ds_raw, config['lang_src'],config['version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['vocab_size_tgt'] = tokenizer_tgt.get_vocab_size()\n",
    "config['vocab_size_src'] = tokenizer_src.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['vocab_size_tgt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_size = int(0.5 * len(filtered_ds_raw))\n",
    "val_ds_size = len(filtered_ds_raw) - train_ds_size\n",
    "train_ds_raw, val_ds_raw = random_split(filtered_ds_raw, [train_ds_size, val_ds_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75127"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_ds_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37563"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, src_lang = config['lang_src'],tgt_lang = config['lang_tgt'], seq_len = config['seq_len'])\n",
    "val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, src_lang = config['lang_src'],tgt_lang = config['lang_tgt'], seq_len = config['seq_len'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\wandb\\run-20241106_184245-v1zy2knr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu%28%27009%27%2C%29/runs/v1zy2knr' target=\"_blank\">wild-mountain-2</a></strong> to <a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu%28%27009%27%2C%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu%28%27009%27%2C%29' target=\"_blank\">https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu%28%27009%27%2C%29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu%28%27009%27%2C%29/runs/v1zy2knr' target=\"_blank\">https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu%28%27009%27%2C%29/runs/v1zy2knr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "No model to preload, starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|██████████| 2348/2348 [04:02<00:00,  9.70it/s, loss=8.364, lr=0.000284605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: • ¿Cuál fue el eje central de las enseñanzas de Jesús?\n",
      "    TARGET: • Imamantapas puntataqa, ¿imamantam Jesusqa yachachirqa?\n",
      "PREDICTED GREEDY: \n",
      "PREDICTED BEAM:   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Su magistral discurso finalizó con una declaración que debió de impactar a sus oyentes paganos.\n",
      "    TARGET: Yachaywan sumaq discurso qosqanqa admirachirqachá taytacha - mamacha yupaychaq runakunataqa.\n",
      "PREDICTED GREEDY: \n",
      "PREDICTED BEAM:   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 01:  38%|███▊      | 883/2348 [01:30<02:30,  9.72it/s, loss=8.037, lr=0.000396767]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 15\u001b[0m\n\u001b[0;32m      7\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreload\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      8\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# set the wandb project where this run will be logged\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch-transformer_es_qu\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[0;32m     14\u001b[0m )\n\u001b[1;32m---> 15\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_tgt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 86\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(config, train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt)\u001b[0m\n\u001b[0;32m     82\u001b[0m         lr_updated \u001b[38;5;241m=\u001b[39m get_lr(step\u001b[38;5;241m=\u001b[39mglobal_step, d_model\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m'\u001b[39m], warmup_steps\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarmup_steps\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     84\u001b[0m         change_lr(optimizer, new_lr\u001b[38;5;241m=\u001b[39mlr_updated)\n\u001b[1;32m---> 86\u001b[0m     \u001b[43mbatch_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_postfix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m6.3f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_groups\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.6g\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     89\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Run validation at the end of every epoch\u001b[39;00m\n\u001b[0;32m     93\u001b[0m run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m'\u001b[39m], device, \u001b[38;5;28;01mlambda\u001b[39;00m msg: batch_iterator\u001b[38;5;241m.\u001b[39mwrite(msg), global_step)\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\tqdm\\std.py:1431\u001b[0m, in \u001b[0;36mtqdm.set_postfix\u001b[1;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostfix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m postfix[key]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m   1429\u001b[0m                          \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m postfix\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refresh:\n\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\tqdm\\std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m-> 1347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\tqdm\\std.py:1495\u001b[0m, in \u001b[0;36mtqdm.display\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[1;32m-> 1495\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\tqdm\\std.py:459\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_status\u001b[39m(s):\n\u001b[0;32m    458\u001b[0m     len_s \u001b[38;5;241m=\u001b[39m disp_len(s)\n\u001b[1;32m--> 459\u001b[0m     \u001b[43mfp_write\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlast_len\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlen_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m     last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m len_s\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\tqdm\\std.py:452\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfp_write\u001b[39m(s):\n\u001b[1;32m--> 452\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m     fp_flush()\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\tqdm\\utils.py:196\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\wandb\\sdk\\lib\\redirect.py:648\u001b[0m, in \u001b[0;36mStreamRawWrapper.install.<locals>.write\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs:\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 648\u001b[0m         \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m         \u001b[38;5;66;03m# TODO: Figure out why this was needed and log or error out appropriately\u001b[39;00m\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;66;03m# it might have been strange terminals? maybe shutdown cases?\u001b[39;00m\n\u001b[0;32m    652\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2386\u001b[0m, in \u001b[0;36mRun._redirect.<locals>.<lambda>\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   2376\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrapping output streams.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2377\u001b[0m     out_redir \u001b[38;5;241m=\u001b[39m redirect\u001b[38;5;241m.\u001b[39mStreamRawWrapper(\n\u001b[0;32m   2378\u001b[0m         src\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2379\u001b[0m         cbs\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   2380\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m data: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_console_raw_callback(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m\"\u001b[39m, data),\n\u001b[0;32m   2381\u001b[0m         ],\n\u001b[0;32m   2382\u001b[0m     )\n\u001b[0;32m   2383\u001b[0m     err_redir \u001b[38;5;241m=\u001b[39m redirect\u001b[38;5;241m.\u001b[39mStreamRawWrapper(\n\u001b[0;32m   2384\u001b[0m         src\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2385\u001b[0m         cbs\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m-> 2386\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m data: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_console_raw_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstderr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   2387\u001b[0m         ],\n\u001b[0;32m   2388\u001b[0m     )\n\u001b[0;32m   2389\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m console \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:403\u001b[0m, in \u001b[0;36m_run_decorator._noop_on_finish.<locals>.decorator_fn.<locals>.wrapper_fn\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;28mtype\u001b[39m[Run], \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_finished\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m     default_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is finished. The call to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be ignored. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease make sure that you are using an active run.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m     )\n\u001b[0;32m    409\u001b[0m     resolved_message \u001b[38;5;241m=\u001b[39m message \u001b[38;5;129;01mor\u001b[39;00m default_message\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:1547\u001b[0m, in \u001b[0;36mRun._console_raw_callback\u001b[1;34m(self, name, data)\u001b[0m\n\u001b[0;32m   1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface:\n\u001b[1;32m-> 1547\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_output_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\wandb\\sdk\\interface\\interface.py:721\u001b[0m, in \u001b[0;36mInterfaceBase.publish_output_raw\u001b[1;34m(self, name, data)\u001b[0m\n\u001b[0;32m    719\u001b[0m o \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mOutputRawRecord(output_type\u001b[38;5;241m=\u001b[39motype, line\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m    720\u001b[0m o\u001b[38;5;241m.\u001b[39mtimestamp\u001b[38;5;241m.\u001b[39mGetCurrentTime()\n\u001b[1;32m--> 721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_output_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py:79\u001b[0m, in \u001b[0;36mInterfaceShared._publish_output_raw\u001b[1;34m(self, outdata)\u001b[0m\n\u001b[0;32m     77\u001b[0m rec \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mRecord()\n\u001b[0;32m     78\u001b[0m rec\u001b[38;5;241m.\u001b[39moutput_raw\u001b[38;5;241m.\u001b[39mCopyFrom(outdata)\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py:51\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[1;34m(self, record, local)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:225\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    223\u001b[0m server_req \u001b[38;5;241m=\u001b[39m spb\u001b[38;5;241m.\u001b[39mServerRequest()\n\u001b[0;32m    224\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[1;32m--> 225\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:157\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    152\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:132\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    130\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 132\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    config = get_config()\n",
    "    config['num_epochs'] = 10\n",
    "    config[\"batch_size\"] = 48\n",
    "    config[\"version\"] = '009'\n",
    "    config['preload'] = None\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"pytorch-transformer_es_qu{0}\".format(config['version']),\n",
    "        \n",
    "        # track hyperparameters and run metadata\n",
    "        config=config\n",
    "    )\n",
    "    train_model(config,train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = get_config()\n",
    "# train_dataloader, val_dataloader, vocab_src, vocab_tgt = get_ds(config)\n",
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = get_weights_file_path(config, f\"29\")\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_next_batch():\n",
    "    # Load a sample batch from the validation set\n",
    "    batch = next(iter(val_dataloader))\n",
    "    encoder_input = batch[\"encoder_input\"].to(device)\n",
    "    encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "    decoder_input = batch[\"decoder_input\"].to(device)\n",
    "    decoder_mask = batch[\"decoder_mask\"].to(device)\n",
    "\n",
    "    encoder_input_tokens = [tokenizer_src.id_to_token(idx) for idx in encoder_input[0].cpu().numpy()]\n",
    "    decoder_input_tokens = [tokenizer_tgt.id_to_token(idx) for idx in decoder_input[0].cpu().numpy()]\n",
    "\n",
    "    # check that the batch size is 1\n",
    "    assert encoder_input.size(\n",
    "        0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "    model_out = greedy_decode(\n",
    "        model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, config['seq_len'], device)\n",
    "    \n",
    "    return batch, encoder_input_tokens, decoder_input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "def mtx2df(m, max_row, max_col, row_tokens, col_tokens):\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            (\n",
    "                r,\n",
    "                c,\n",
    "                float(m[r, c]),\n",
    "                \"%.3d %s\" % (r, row_tokens[r] if len(row_tokens) > r else \"<blank>\"),\n",
    "                \"%.3d %s\" % (c, col_tokens[c] if len(col_tokens) > c else \"<blank>\"),\n",
    "            )\n",
    "            for r in range(m.shape[0])\n",
    "            for c in range(m.shape[1])\n",
    "            if r < max_row and c < max_col\n",
    "        ],\n",
    "        columns=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
    "    )\n",
    "\n",
    "def get_attn_map(attn_type: str, layer: int, head: int):\n",
    "    if attn_type == \"encoder\":\n",
    "        attn = model.encoder.layers[layer].self_attention_block.attention_scores\n",
    "    elif attn_type == \"decoder\":\n",
    "        attn = model.decoder.layers[layer].self_attention_block.attention_scores\n",
    "    elif attn_type == \"encoder-decoder\":\n",
    "        attn = model.decoder.layers[layer].cross_attention_block.attention_scores\n",
    "    return attn[0, head].data\n",
    "\n",
    "def attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len):\n",
    "    df = mtx2df(\n",
    "        get_attn_map(attn_type, layer, head),\n",
    "        max_sentence_len,\n",
    "        max_sentence_len,\n",
    "        row_tokens,\n",
    "        col_tokens,\n",
    "    )\n",
    "    return (\n",
    "        alt.Chart(data=df)\n",
    "        .mark_rect()\n",
    "        .encode(\n",
    "            x=alt.X(\"col_token\", axis=alt.Axis(title=\"\")),\n",
    "            y=alt.Y(\"row_token\", axis=alt.Axis(title=\"\")),\n",
    "            color=\"value\",\n",
    "            tooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
    "        )\n",
    "        #.title(f\"Layer {layer} Head {head}\")\n",
    "        .properties(height=400, width=400, title=f\"Layer {layer} Head {head}\")\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "def get_all_attention_maps(attn_type: str, layers: list[int], heads: list[int], row_tokens: list, col_tokens, max_sentence_len: int):\n",
    "    charts = []\n",
    "    for layer in layers:\n",
    "        rowCharts = []\n",
    "        for head in heads:\n",
    "            rowCharts.append(attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len))\n",
    "        charts.append(alt.hconcat(*rowCharts))\n",
    "    return alt.vconcat(*charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, encoder_input_tokens, decoder_input_tokens = load_next_batch()\n",
    "print(f'Source: {batch[\"src_text\"][0]}')\n",
    "print(f'Target: {batch[\"tgt_text\"][0]}')\n",
    "sentence_len = encoder_input_tokens.index(\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [0, 1, 2, 3, 4, 5]\n",
    "heads = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Encoder Self-Attention\n",
    "get_all_attention_maps(\"encoder\", layers, heads, encoder_input_tokens, encoder_input_tokens, min(20, sentence_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Self-Attention\n",
    "get_all_attention_maps(\"decoder\", layers, heads, decoder_input_tokens, decoder_input_tokens, min(20, sentence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Self-Attention\n",
    "get_all_attention_maps(\"encoder-decoder\", layers, heads, encoder_input_tokens, decoder_input_tokens, min(20, sentence_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translatepy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
