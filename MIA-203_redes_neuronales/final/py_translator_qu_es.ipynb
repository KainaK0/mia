{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model:int, vocabulary_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocabulary_size\n",
    "        self.embedding = nn.Embedding(vocabulary_size, d_model)\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, sequence_len:int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = sequence_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(sequence_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, sequence_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self,x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, parameters_shape: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(parameters_shape)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(parameters_shape)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "    \n",
    "        def forward(self, x, sublayer):\n",
    "            x = self.norm(x)\n",
    "            x = self.dropout(sublayer(x)) + x\n",
    "            return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N_layers: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N_layers):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N_layers):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mj-callomamani-b\u001b[0m (\u001b[33msalcantaratnaist\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\kainak0\\_netrc\n"
     ]
    }
   ],
   "source": [
    "# from model import build_transformer\n",
    "# from dataset import BilingualDataset, causal_mask\n",
    "# from config import get_config, get_weights_file_path\n",
    "\n",
    "# import torchtext.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=\"5364c808c25399d37be50f8e9227b609dad82d1b\")\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw = load_dataset(f\"somosnlp-hackathon-2022/spanish-to-quechua\", split='train')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['es', 'qu'],\n",
       "    num_rows: 102747\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 48,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 200,\n",
    "        \"d_model\": 512,\n",
    "        'N_layers': 2,\n",
    "        'heads': 8,\n",
    "        'dropout': 0.1,\n",
    "        'ffn_hidden': 2048,\n",
    "        \"datasource\": 'spanish-to-quechua',\n",
    "        \"lang_src\": \"qu\",\n",
    "        \"lang_tgt\": \"es\",\n",
    "        \"model_folder\": \"weights_qu_es\",\n",
    "        \"model_basename\": \"tmodel_qu_es\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_qu_es_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel_qu_es\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest weights file in the weights folder\n",
    "def latest_weights_file_path(config):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def hist_len_sentence(ds_raw, lang, min_sentence_len, max_sentence_len):\n",
    "    len_sentence = [len(x) for x in ds_raw[lang]]\n",
    "    len_sentence_ranged = [x for x in len_sentence if min_sentence_len < x < max_sentence_len]\n",
    "\n",
    "    # Generate sample data\n",
    "    data = np.array(len_sentence_ranged)\n",
    "\n",
    "    # Create the figure and axis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.hist(data, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('Normal Distribution Histogram', fontsize=14)\n",
    "    plt.xlabel('Value', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(np.arange(0, max_sentence_len, 20))\n",
    "    # Add some padding to the layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_len_sentence(ds_raw, lang = config['lang_src'], min_sentence_len = 0, max_sentence_len = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_length(dataset, min_len=1, max_len=1000):\n",
    "    def length_filter(example):\n",
    "        # Check length for all string/list features\n",
    "        for key, value in example.items():\n",
    "            if isinstance(value, (str, list)):\n",
    "                if len(value) < min_len or len(value) > max_len:\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    filtered_dataset = dataset.filter(length_filter)\n",
    "    return filtered_dataset\n",
    "\n",
    "# Usage example:\n",
    "filtered_ds_raw = filter_by_length(ds_raw, min_len=20, max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['es', 'qu'],\n",
       "    num_rows: 89047\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ds_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item[lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "    # if True:\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair[self.src_lang]\n",
    "        tgt_text = src_target_pair[self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "    \n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, beam_size, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_initial_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "\n",
    "    # Create a candidate list\n",
    "    candidates = [(decoder_initial_input, 1)]\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # If a candidate has reached the maximum length, it means we have run the decoding for at least max_len iterations, so stop the search\n",
    "        if any([cand.size(1) == max_len for cand, _ in candidates]):\n",
    "            break\n",
    "\n",
    "        # Create a new list of candidates\n",
    "        new_candidates = []\n",
    "\n",
    "        for candidate, score in candidates:\n",
    "\n",
    "            # Do not expand candidates that have reached the eos token\n",
    "            if candidate[0][-1].item() == eos_idx:\n",
    "                continue\n",
    "\n",
    "            # Build the candidate's mask\n",
    "            candidate_mask = causal_mask(candidate.size(1)).type_as(source_mask).to(device)\n",
    "            # calculate output\n",
    "            out = model.decode(encoder_output, source_mask, candidate, candidate_mask)\n",
    "            # get next token probabilities\n",
    "            prob = model.project(out[:, -1])\n",
    "            # get the top k candidates\n",
    "            topk_prob, topk_idx = torch.topk(prob, beam_size, dim=1)\n",
    "            for i in range(beam_size):\n",
    "                # for each of the top k candidates, get the token and its probability\n",
    "                token = topk_idx[0][i].unsqueeze(0).unsqueeze(0)\n",
    "                token_prob = topk_prob[0][i].item()\n",
    "                # create a new candidate by appending the token to the current candidate\n",
    "                new_candidate = torch.cat([candidate, token], dim=1)\n",
    "                # We sum the log probabilities because the probabilities are in log space\n",
    "                new_candidates.append((new_candidate, score + token_prob))\n",
    "\n",
    "        # Sort the new candidates by their score\n",
    "        candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
    "        # Keep only the top k candidates\n",
    "        candidates = candidates[:beam_size]\n",
    "\n",
    "        # If all the candidates have reached the eos token, stop\n",
    "        if all([cand[0][-1].item() == eos_idx for cand, _ in candidates]):\n",
    "            break\n",
    "\n",
    "    # Return the best candidate\n",
    "    return candidates[0][0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted_greedy = []\n",
    "    predicted_beam = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "            model_out_beam = beam_search_decode(model, 3, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text_beam = tokenizer_tgt.decode(model_out_beam.detach().cpu().numpy())\n",
    "            model_out_text_greedy = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted_greedy.append(model_out_text_greedy)\n",
    "            predicted_beam.append(model_out_text_beam)\n",
    "            \n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED GREEDY: ':>12}{model_out_text_greedy}\")\n",
    "            print_msg(f\"{f'PREDICTED BEAM:   ':>12}{model_out_text_beam}\")\n",
    "\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "    \n",
    "    \n",
    "    # Evaluate the character error rate\n",
    "    # Compute the char error rate \n",
    "    metric_greedy = torchmetrics.CharErrorRate()\n",
    "    cer = metric_greedy(predicted_greedy, expected)\n",
    "    wandb.log({'validation_greedy/cer': cer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the word error rate\n",
    "    metric_greedy = torchmetrics.WordErrorRate()\n",
    "    wer = metric_greedy(predicted_greedy, expected)\n",
    "    wandb.log({'validation_greedy/wer': wer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the BLEU metric\n",
    "    metric_greedy = torchmetrics.BLEUScore()\n",
    "    bleu = metric_greedy(predicted_greedy, expected)\n",
    "    wandb.log({'validation_greedy/BLEU': bleu, 'global_step': global_step})\n",
    "\n",
    "    # Evaluate the character error rate\n",
    "    # Compute the char error rate \n",
    "    metric_beam = torchmetrics.CharErrorRate()\n",
    "    cer = metric_beam(predicted_beam, expected)\n",
    "    wandb.log({'validation_beam/cer': cer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the word error rate\n",
    "    metric = torchmetrics.WordErrorRate()\n",
    "    wer = metric_beam(predicted_beam, expected)\n",
    "    wandb.log({'validation_beam/wer': wer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the BLEU metric\n",
    "    metric = torchmetrics.BLEUScore()\n",
    "    bleu = metric_beam(predicted_beam, expected)\n",
    "    wandb.log({'validation_beam/BLEU': bleu, 'global_step': global_step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'],\n",
    "        N_layers=config['N_layers'], h = config['heads'], dropout = config['dropout'], d_ff = config['ffn_hidden'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config,train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt ):\n",
    "    # Define the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    if (device == 'cuda'):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "     \n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "\n",
    "    if model_filename:\n",
    "    # if False:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    # define our custom x axis metric\n",
    "    wandb.define_metric(\"global_step\")\n",
    "    # define which metrics will be plotted against it\n",
    "    wandb.define_metric(\"validation_greedy/*\", step_metric=\"global_step\")\n",
    "    wandb.define_metric(\"validation_beam/*\", step_metric=\"global_step\")\n",
    "    wandb.define_metric(\"train/*\", step_metric=\"global_step\")\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            wandb.log({'train/loss': loss.item(), 'global_step': global_step})\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step)\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_tgt = get_or_build_tokenizer( filtered_ds_raw, config['lang_tgt'])\n",
    "tokenizer_src =  get_or_build_tokenizer( filtered_ds_raw, config['lang_src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_size = int(0.9 * len(filtered_ds_raw))\n",
    "val_ds_size = len(filtered_ds_raw) - train_ds_size\n",
    "train_ds_raw, val_ds_raw = random_split(filtered_ds_raw, [train_ds_size, val_ds_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, src_lang = config['lang_src'],tgt_lang = config['lang_tgt'], seq_len = 200)\n",
    "val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, src_lang = config['lang_src'],tgt_lang = config['lang_tgt'], seq_len = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6k4uv0d0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td></td></tr><tr><td>train/loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>21</td></tr><tr><td>train/loss</td><td>8.85838</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lunar-sponge-4</strong> at: <a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_qu_es/runs/6k4uv0d0' target=\"_blank\">https://wandb.ai/salcantaratnaist/pytorch-transformer_qu_es/runs/6k4uv0d0</a><br/> View project at: <a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_qu_es' target=\"_blank\">https://wandb.ai/salcantaratnaist/pytorch-transformer_qu_es</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241106_012004-6k4uv0d0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6k4uv0d0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\wandb\\run-20241106_012017-ndc66t8o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_qu_es/runs/ndc66t8o' target=\"_blank\">sunny-fog-5</a></strong> to <a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_qu_es' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_qu_es' target=\"_blank\">https://wandb.ai/salcantaratnaist/pytorch-transformer_qu_es</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_qu_es/runs/ndc66t8o' target=\"_blank\">https://wandb.ai/salcantaratnaist/pytorch-transformer_qu_es/runs/ndc66t8o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preloading model spanish-to-quechua_weights_qu_es\\tmodel_qu_es25.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 26: 100%|| 1670/1670 [06:43<00:00,  4.14it/s, loss=2.100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Llumpay onqoyninmanta sanoyachispanmi  aychanta rikurichirqa mozo kasqanpi hinataraq .\n",
      "    TARGET: Entre otras cosas, hizo que desapareciera su terrible enfermedad y que su carne se hiciera ms fresca que en la juventud .\n",
      "PREDICTED GREEDY: Entre otras cosas , hizo que desapareciera su terrible enfermedad y que su carne se hiciera  ms fresca que en la juventud .\n",
      "PREDICTED BEAM:   Entre otras cosas , hizo que desapareciera su terrible enfermedad y que su carne se hiciera  ms fresca que en la juventud .  Qu significa ? ( Mire la nota ).\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Ar, kunanmi kachkan tukuy nacionmanta humillakuqkuna, paykunam hanaq pachapaq otaq kay pachapaq suyakuyniyoq kaspankupas Jehov Diospa sutinpi amparakunku.\n",
      "    TARGET: En efecto, en la actualidad hay mansos de todas las naciones que, independientemente de que su esperanza sea celestial o terrenal, se refugian en el nombre de Jehov.\n",
      "PREDICTED GREEDY: En efecto , hoy es la verdadera paz de los dems , donde en realidad han sido ungidos o sea , sin importar el nmero de la resurreccin que haya en el nombre de Jehov .\n",
      "PREDICTED BEAM:   As es , en la actualidad hay gente de todas las naciones que , independientemente de que su esperanza sea celestial o terrenal , se refugian en el nombre de Jehov .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 27: 100%|| 1670/1670 [07:42<00:00,  3.61it/s, loss=2.053]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Iskay watamantaataqmi aparuwarqaku Zaragoza llaqtapi monjakunapa hatun wasinman, chaypiqa yuyaqkunatam uywaqku.\n",
      "    TARGET: Dos aos despus nos enviaron a un convento muy grande en Zaragoza, donde cuidaban a gente mayor.\n",
      "PREDICTED GREEDY: Dos aos despus nos enviaron a un convento muy grande en Zaragoza , donde cuidaban a gente mayor .\n",
      "PREDICTED BEAM:   Dos aos ms tarde nos enviaron a un convento muy grande en Zaragoza , donde cuidaban a gente con . * ( Lea la nota .)\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: TUVAM tarikun Rusiapi, Siberia hinaspa Mongolia nacionpa hichpallanpi.\n",
      "    TARGET: LA REPBLICA rusa de Tuva se encuentra en el extremo sur de Siberia, en la frontera con Mongolia.\n",
      "PREDICTED GREEDY: LA REPBLICA rusa de Tuva se encuentra en el extremo sur de Siberia , en la frontera con Mongolia .\n",
      "PREDICTED BEAM:   LA REPBLICA rusa de Tuva se encuentra en el extremo sur de Siberia , en la frontera con Mongolia de este sistema de navegacin y la frontera entre los sur de Siberia , la frontera con Mongolia . * ( Versin del universo ).\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 28: 100%|| 1670/1670 [06:44<00:00,  4.12it/s, loss=2.036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Paykunaqa manam iiqmasinkuna imatapas qoykunankutaqa suyaqkuchu, chaywanpas iiqmasinkunaqa yanapaqkum.\n",
      "    TARGET: No esperaban recibir nada de los hermanos, pero muchas veces estos los ayudaban.\n",
      "PREDICTED GREEDY: No los hicieron caso de otros , pero los hermanos de otros hermanos no los ayudaban .\n",
      "PREDICTED BEAM:   No esperaban recibir nada de los hermanos , pero los hermanos de otros pases los ayudaban mutuamente a ofrecer algn tipo de ayuda , incluso cuando los otros los hermanos lo estaban haciendo .  Cmo lo han hecho ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Chayna qellqasqa kasqanmi allinta yanapanman cheqaptapuni imanasqa iskayrayasqanmanta musyakunanpaq hinaspam kikin allichanman.\n",
      "    TARGET: Verlos escritos puede ayudarles a darse cuenta de la verdadera causa de sus dudas y a encontrar soluciones.\n",
      "PREDICTED GREEDY: Sin duda , puede ayudarlos a entender por qu se hace sabio y se le concedi el nombre de alguien que lo habla .\n",
      "PREDICTED BEAM:   Sin duda , este mtodo puede ayudarlo a entender por qu se habla Dios de este asunto y se pone a prueba , tal y como se lo habla el profeta de Dios a quien se lo habla .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 29: 100%|| 1670/1670 [06:41<00:00,  4.16it/s, loss=2.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Ichaqa, imanasqam mana iskayrayawaqchu bibliaqa Diospa palabran kasqanmanta hinaspa mana cuentokunalla kasqanmanta?.\n",
      "    TARGET: Pero cmo puede usted estar seguro de que este libro es de verdad la palabra de Dios  y no una recopilacin de mitos y leyendas?.\n",
      "PREDICTED GREEDY: Pero  por qu podra pensar que la Biblia dice que est basada en la Biblia y que no puede ?.\n",
      "PREDICTED BEAM:   Ahora bien ,  por qu no puedes pensar que la Biblia dice que la Biblia viene de Dios y que no le puede ?.  Por qu puede llegar a ser demasiado inteligente ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 6: 6 - 9). Jehova Diospa munayninqa haykapipas hina kaqllam, payqa munanmi awpaq tiempopi Israel runakuna hina ruwananchikta.\n",
      "    TARGET: Como Jehov nunca cambia su propsito, hoy l espera que sus siervos hagamos lo mismo que los israelitas.\n",
      "PREDICTED GREEDY: Como Jehov nunca cambia su propsito , l espera que sus siervos hagamos lo mismo .\n",
      "PREDICTED BEAM:   Como Jehov nunca cambia su propsito , l siempre espera que los israelitas hagamos lo mismo que a sus siervos de la antigedad .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    config = get_config()\n",
    "    config['num_epochs'] = 30\n",
    "    # config['preload'] = None\n",
    "\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"pytorch-transformer_qu_es\",\n",
    "        \n",
    "        # track hyperparameters and run metadata\n",
    "        config=config\n",
    "    )\n",
    "    train_model(config,train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translatepy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
