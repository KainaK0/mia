{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model:int, vocabulary_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocabulary_size\n",
    "        self.embedding = nn.Embedding(vocabulary_size, d_model)\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, sequence_len:int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = sequence_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(sequence_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, sequence_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self,x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, parameters_shape: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(parameters_shape)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(parameters_shape)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "    \n",
    "        def forward(self, x, sublayer):\n",
    "            x = self.norm(x)\n",
    "            x = self.dropout(sublayer(x)) + x\n",
    "            return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N_layers: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N_layers):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N_layers):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\kainak0\\_netrc\n"
     ]
    }
   ],
   "source": [
    "# from model import build_transformer\n",
    "# from dataset import BilingualDataset, causal_mask\n",
    "# from config import get_config, get_weights_file_path\n",
    "\n",
    "# import torchtext.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=\"5364c808c25399d37be50f8e9227b609dad82d1b\")\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"allenai/nllb\", \"quy_Latn-spa_Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quy_Latn': '- Huk llaqtapin karan huk juez Diosta mana manchakuq, hinallataq runakunatapas mana respetaq.',\n",
       " 'spa_Latn': 'Y dijo: Había un juez en una ciudad que no temía a Dios ni respetaba a los hombres.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']['translation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw['qu'] = [x['quy_Latn'] for x in ds['train']['translation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw['es'] = [x['spa_Latn'] for x in ds['train']['translation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "748091"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_raw['es'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw = datasets.Dataset.from_dict(ds_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['qu', 'es'],\n",
       "    num_rows: 748091\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 48,\n",
    "        \"num_epochs\": 10,\n",
    "        \"lr\": 1e-4,\n",
    "        \"seq_len\": 140,\n",
    "        \"d_model\": 512,\n",
    "        'N_layers': 6,\n",
    "        'heads': 8,\n",
    "        'dropout': 0.3,\n",
    "        'ffn_hidden': 2048,\n",
    "        \"datasource\": 'spanish-to-quechua',\n",
    "        \"version\": '003',\n",
    "        \"lang_src\": \"es\",\n",
    "        \"lang_tgt\": \"qu\",\n",
    "        \"model_folder\": \"weights_es_qu{0}\",\n",
    "        \"model_basename\": \"tmodel_es_qu{0}\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_es_qu_{0}{1}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel_es_qu{0}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest weights file in the weights folder\n",
    "def latest_weights_file_path(config):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder'].format(config['version'])}\"\n",
    "    model_filename = f\"{config['model_basename'].format(config['version'])}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder'].format(config['version'])}\"\n",
    "    model_filename = f\"{config['model_basename'].format(config['version'])}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def hist_len_sentence(ds_raw, lang, min_sentence_len, max_sentence_len):\n",
    "    len_sentence = [len(x) for x in ds_raw[lang]]\n",
    "    len_sentence_ranged = [x for x in len_sentence if min_sentence_len < x < max_sentence_len]\n",
    "\n",
    "    # Generate sample data\n",
    "    data = np.array(len_sentence_ranged)\n",
    "\n",
    "    # Create the figure and axis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.hist(data, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('Normal Distribution Histogram', fontsize=14)\n",
    "    plt.xlabel('Value', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(np.arange(0, max_sentence_len, 20))\n",
    "    # Add some padding to the layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97th percentile length quechua: 167.0\n",
      "97th percentile length spanish: 158.0\n"
     ]
    }
   ],
   "source": [
    "PERCENTILE = 97\n",
    "print( f\"{PERCENTILE}th percentile length quechua: {np.percentile([len(x) for x in ds_raw['qu']], PERCENTILE)}\" )\n",
    "print( f\"{PERCENTILE}th percentile length spanish: {np.percentile([len(x) for x in ds_raw['es']], PERCENTILE)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_len_sentence(ds_raw, lang = config['lang_tgt'], min_sentence_len = 0, max_sentence_len = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 748091/748091 [00:02<00:00, 280784.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def filter_by_length(dataset, min_len=20, max_len=200):\n",
    "    def length_filter(example):\n",
    "        # Check length for all string/list features\n",
    "        for key, value in example.items():\n",
    "            if isinstance(value, (str, list)):\n",
    "                if len(value) < min_len or len(value) > max_len:\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    filtered_dataset = dataset.filter(length_filter)\n",
    "    return filtered_dataset\n",
    "\n",
    "# Usage example:\n",
    "filtered_ds_raw = filter_by_length(ds_raw, min_len=0, max_len=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item[lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(ds, lang:str,version:str):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang,version))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "    # if True:\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair[self.src_lang]\n",
    "        tgt_text = src_target_pair[self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "    \n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, beam_size, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_initial_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "\n",
    "    # Create a candidate list\n",
    "    candidates = [(decoder_initial_input, 1)]\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # If a candidate has reached the maximum length, it means we have run the decoding for at least max_len iterations, so stop the search\n",
    "        if any([cand.size(1) == max_len for cand, _ in candidates]):\n",
    "            break\n",
    "\n",
    "        # Create a new list of candidates\n",
    "        new_candidates = []\n",
    "\n",
    "        for candidate, score in candidates:\n",
    "\n",
    "            # Do not expand candidates that have reached the eos token\n",
    "            if candidate[0][-1].item() == eos_idx:\n",
    "                continue\n",
    "\n",
    "            # Build the candidate's mask\n",
    "            candidate_mask = causal_mask(candidate.size(1)).type_as(source_mask).to(device)\n",
    "            # calculate output\n",
    "            out = model.decode(encoder_output, source_mask, candidate, candidate_mask)\n",
    "            # get next token probabilities\n",
    "            prob = model.project(out[:, -1])\n",
    "            # get the top k candidates\n",
    "            topk_prob, topk_idx = torch.topk(prob, beam_size, dim=1)\n",
    "            for i in range(beam_size):\n",
    "                # for each of the top k candidates, get the token and its probability\n",
    "                token = topk_idx[0][i].unsqueeze(0).unsqueeze(0)\n",
    "                token_prob = topk_prob[0][i].item()\n",
    "                # create a new candidate by appending the token to the current candidate\n",
    "                new_candidate = torch.cat([candidate, token], dim=1)\n",
    "                # We sum the log probabilities because the probabilities are in log space\n",
    "                new_candidates.append((new_candidate, score + token_prob))\n",
    "\n",
    "        # Sort the new candidates by their score\n",
    "        candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
    "        # Keep only the top k candidates\n",
    "        candidates = candidates[:beam_size]\n",
    "\n",
    "        # If all the candidates have reached the eos token, stop\n",
    "        if all([cand[0][-1].item() == eos_idx for cand, _ in candidates]):\n",
    "            break\n",
    "\n",
    "    # Return the best candidate\n",
    "    return candidates[0][0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted_greedy = []\n",
    "    predicted_beam = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "            model_out_beam = beam_search_decode(model, 3, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text_beam = tokenizer_tgt.decode(model_out_beam.detach().cpu().numpy())\n",
    "            model_out_text_greedy = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted_greedy.append(model_out_text_greedy)\n",
    "            predicted_beam.append(model_out_text_beam)\n",
    "            \n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED GREEDY: ':>12}{model_out_text_greedy}\")\n",
    "            print_msg(f\"{f'PREDICTED BEAM:   ':>12}{model_out_text_beam}\")\n",
    "\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "    \n",
    "    \n",
    "    # Evaluate the character error rate\n",
    "    # Compute the char error rate \n",
    "    metric_greedy = torchmetrics.CharErrorRate()\n",
    "    cer = metric_greedy(predicted_greedy, expected)\n",
    "    wandb.log({'validation_greedy/cer': cer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the word error rate\n",
    "    metric_greedy = torchmetrics.WordErrorRate()\n",
    "    wer = metric_greedy(predicted_greedy, expected)\n",
    "    wandb.log({'validation_greedy/wer': wer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the BLEU metric\n",
    "    metric_greedy = torchmetrics.BLEUScore()\n",
    "    bleu = metric_greedy(predicted_greedy, expected)\n",
    "    wandb.log({'validation_greedy/BLEU': bleu, 'global_step': global_step})\n",
    "\n",
    "    # Evaluate the character error rate\n",
    "    # Compute the char error rate \n",
    "    metric_beam = torchmetrics.CharErrorRate()\n",
    "    cer = metric_beam(predicted_beam, expected)\n",
    "    wandb.log({'validation_beam/cer': cer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the word error rate\n",
    "    metric = torchmetrics.WordErrorRate()\n",
    "    wer = metric_beam(predicted_beam, expected)\n",
    "    wandb.log({'validation_beam/wer': wer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the BLEU metric\n",
    "    metric = torchmetrics.BLEUScore()\n",
    "    bleu = metric_beam(predicted_beam, expected)\n",
    "    wandb.log({'validation_beam/BLEU': bleu, 'global_step': global_step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'],\n",
    "        N_layers=config['N_layers'], h = config['heads'], dropout = config['dropout'], d_ff = config['ffn_hidden'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config,train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt ):\n",
    "    # Define the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    if (device == 'cuda'):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "     \n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config['datasource']}_{config['model_folder'].format(config['version'])}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "\n",
    "    if model_filename:\n",
    "    # if False:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    # define our custom x axis metric\n",
    "    wandb.define_metric(\"global_step\")\n",
    "    # define which metrics will be plotted against it\n",
    "    wandb.define_metric(\"validation_greedy/*\", step_metric=\"global_step\")\n",
    "    wandb.define_metric(\"validation_beam/*\", step_metric=\"global_step\")\n",
    "    wandb.define_metric(\"train/*\", step_metric=\"global_step\")\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            wandb.log({'train/loss': loss.item(), 'global_step': global_step})\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step)\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_tgt = get_or_build_tokenizer( filtered_ds_raw, config['lang_tgt'],config['version'])\n",
    "tokenizer_src =  get_or_build_tokenizer( filtered_ds_raw, config['lang_src'],config['version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_size = int(0.8 * len(filtered_ds_raw))\n",
    "val_ds_size = len(filtered_ds_raw) - train_ds_size\n",
    "train_ds_raw, val_ds_raw = random_split(filtered_ds_raw, [train_ds_size, val_ds_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "690982"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_ds_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552785"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, src_lang = config['lang_src'],tgt_lang = config['lang_tgt'], seq_len = config['seq_len'])\n",
    "val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, src_lang = config['lang_src'],tgt_lang = config['lang_tgt'], seq_len = config['seq_len'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\wandb\\run-20241106_024217-9c6rfy54</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu003/runs/9c6rfy54' target=\"_blank\">wandering-waterfall-1</a></strong> to <a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu003' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu003' target=\"_blank\">https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu003</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu003/runs/9c6rfy54' target=\"_blank\">https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu003/runs/9c6rfy54</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "No model to preload, starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00:  14%|█▍        | 1627/11517 [13:22<1:21:18,  2.03it/s, loss=7.502]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 13\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# config['preload'] = None\u001b[39;00m\n\u001b[0;32m      6\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# set the wandb project where this run will be logged\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch-transformer_es_qu\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[0;32m     12\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_tgt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 61\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(config, train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt)\u001b[0m\n\u001b[0;32m     58\u001b[0m proj_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mproject(decoder_output) \u001b[38;5;66;03m# (B, seq_len, vocab_size)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Compare the output with the label\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, seq_len)\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Compute the loss using a simple cross entropy\u001b[39;00m\n\u001b[0;32m     64\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(proj_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, tokenizer_tgt\u001b[38;5;241m.\u001b[39mget_vocab_size()), label\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    config = get_config()\n",
    "    config['num_epochs'] = 10\n",
    "    # config['preload'] = None\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"pytorch-transformer_es_qu{0}\".format(config['version']),\n",
    "        \n",
    "        # track hyperparameters and run metadata\n",
    "        config=config\n",
    "    )\n",
    "    train_model(config,train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translatepy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
