{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model:int, vocabulary_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocabulary_size\n",
    "        self.embedding = nn.Embedding(vocabulary_size, d_model)\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, sequence_len:int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = sequence_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(sequence_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, sequence_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self,x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, parameters_shape: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(parameters_shape)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(parameters_shape)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "    \n",
    "        def forward(self, x, sublayer):\n",
    "            x = self.norm(x)\n",
    "            x = self.dropout(sublayer(x)) + x\n",
    "            return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N_layers: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N_layers):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N_layers):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mj-callomamani-b\u001b[0m (\u001b[33msalcantaratnaist\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\kainak0\\_netrc\n"
     ]
    }
   ],
   "source": [
    "# from model import build_transformer\n",
    "# from dataset import BilingualDataset, causal_mask\n",
    "# from config import get_config, get_weights_file_path\n",
    "\n",
    "# import torchtext.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset,DatasetDict\n",
    "import datasets\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import math\n",
    "import wandb\n",
    "wandb.login(key=\"5364c808c25399d37be50f8e9227b609dad82d1b\")\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"allenai/nllb\", \"quy_Latn-spa_Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quy_Latn': '- Huk llaqtapin karan huk juez Diosta mana manchakuq, hinallataq runakunatapas mana respetaq.',\n",
       " 'spa_Latn': 'Y dijo: Había un juez en una ciudad que no temía a Dios ni respetaba a los hombres.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']['translation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw['qu'] = [x['quy_Latn'] for x in ds['train']['translation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw['es'] = [x['spa_Latn'] for x in ds['train']['translation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "748091"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_raw['es'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw = datasets.Dataset.from_dict(ds_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['qu', 'es'],\n",
       "    num_rows: 748091\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(step = 1, d_model = 512, warmup_steps = 8000):\n",
    "    \"\"\"\n",
    "    The LR schedule. This version below is twice the definition in the paper, as used in the official T2T repository.\n",
    "\n",
    "    :param step: training step number\n",
    "    :param d_model: size of vectors throughout the transformer model\n",
    "    :param warmup_steps: number of warmup steps where learning rate is increased linearly; twice the value in the paper, as in the official T2T repo\n",
    "    :return: updated learning rate\n",
    "    \"\"\"\n",
    "    lr =  2.0 * math.pow(d_model, -0.5) * min(math.pow(step, -0.5), step * math.pow(warmup_steps, -1.5))\n",
    "\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 32,\n",
    "        \"num_epochs\": 10,\n",
    "        \"lr\": get_lr(),\n",
    "        \"seq_len\": 200,\n",
    "        \"d_model\": 512,\n",
    "        'N_layers': 2,\n",
    "        'heads': 8,\n",
    "        'dropout': 0.1,\n",
    "        'ffn_hidden': 2048,\n",
    "        'warmup_steps': 8000,\n",
    "        'min_sentence_len': 40,\n",
    "        'max_sentence_len': 200,\n",
    "        'tokenizer':WordLevel,\n",
    "        'tokenizer_trainer':WordLevelTrainer,\n",
    "        \"datasource\": 'allenai/nllb',\n",
    "        \"version\": '007',\n",
    "        \"lang_src\": \"es\",\n",
    "        \"lang_tgt\": \"qu\",\n",
    "        \"model_folder\": \"weights_es_qu{0}\",\n",
    "        \"model_basename\": \"tmodel_es_qu{0}\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_es_qu_{0}{1}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel_es_qu{0}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_lr(optimizer, new_lr):\n",
    "    \"\"\"\n",
    "    Scale learning rate by a specified factor.\n",
    "\n",
    "    :param optimizer: optimizer whose learning rate must be changed\n",
    "    :param new_lr: new learning rate\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.493856214843422e-07"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest weights file in the weights folder\n",
    "def latest_weights_file_path(config):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder'].format(config['version'])}\"\n",
    "    model_filename = f\"{config['model_basename'].format(config['version'])}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder'].format(config['version'])}\"\n",
    "    model_filename = f\"{config['model_basename'].format(config['version'])}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def hist_len_sentence(ds_raw, lang, min_sentence_len, max_sentence_len):\n",
    "    len_sentence = [len(x) for x in ds_raw[lang]]\n",
    "    len_sentence_ranged = [x for x in len_sentence if min_sentence_len < x < max_sentence_len]\n",
    "\n",
    "    # Generate sample data\n",
    "    data = np.array(len_sentence_ranged)\n",
    "\n",
    "    # Create the figure and axis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.hist(data, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('Normal Distribution Histogram', fontsize=14)\n",
    "    plt.xlabel('Value', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(np.arange(0, max_sentence_len, 20))\n",
    "    # Add some padding to the layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97th percentile length quechua: 167.0\n",
      "97th percentile length spanish: 158.0\n"
     ]
    }
   ],
   "source": [
    "PERCENTILE = 97\n",
    "print( f\"{PERCENTILE}th percentile length quechua: {np.percentile([len(x) for x in ds_raw['qu']], PERCENTILE)}\" )\n",
    "print( f\"{PERCENTILE}th percentile length spanish: {np.percentile([len(x) for x in ds_raw['es']], PERCENTILE)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_len_sentence(ds_raw, lang = config['lang_tgt'], \n",
    "                  min_sentence_len = config['min_sentence_len'], \n",
    "                  max_sentence_len = config['max_sentence_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 748091/748091 [00:01<00:00, 387020.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def filter_by_length(dataset, min_len=20, max_len=200):\n",
    "    def length_filter(example):\n",
    "        # Check length for all string/list features\n",
    "        for key, value in example.items():\n",
    "            if isinstance(value, (str, list)):\n",
    "                if len(value) < min_len or len(value) > max_len:\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    filtered_dataset = dataset.filter(length_filter)\n",
    "    return filtered_dataset\n",
    "\n",
    "# Usage example: \n",
    "filtered_ds_raw = filter_by_length(ds_raw, min_len = config['min_sentence_len'], max_len = config['max_sentence_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item[lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(ds, lang:str,version:str):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang,version))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "    # if True:\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair[self.src_lang]\n",
    "        tgt_text = src_target_pair[self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "    \n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, beam_size, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_initial_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "\n",
    "    # Create a candidate list\n",
    "    candidates = [(decoder_initial_input, 1)]\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # If a candidate has reached the maximum length, it means we have run the decoding for at least max_len iterations, so stop the search\n",
    "        if any([cand.size(1) == max_len for cand, _ in candidates]):\n",
    "            break\n",
    "\n",
    "        # Create a new list of candidates\n",
    "        new_candidates = []\n",
    "\n",
    "        for candidate, score in candidates:\n",
    "\n",
    "            # Do not expand candidates that have reached the eos token\n",
    "            if candidate[0][-1].item() == eos_idx:\n",
    "                continue\n",
    "\n",
    "            # Build the candidate's mask\n",
    "            candidate_mask = causal_mask(candidate.size(1)).type_as(source_mask).to(device)\n",
    "            # calculate output\n",
    "            out = model.decode(encoder_output, source_mask, candidate, candidate_mask)\n",
    "            # get next token probabilities\n",
    "            prob = model.project(out[:, -1])\n",
    "            # get the top k candidates\n",
    "            topk_prob, topk_idx = torch.topk(prob, beam_size, dim=1)\n",
    "            for i in range(beam_size):\n",
    "                # for each of the top k candidates, get the token and its probability\n",
    "                token = topk_idx[0][i].unsqueeze(0).unsqueeze(0)\n",
    "                token_prob = topk_prob[0][i].item()\n",
    "                # create a new candidate by appending the token to the current candidate\n",
    "                new_candidate = torch.cat([candidate, token], dim=1)\n",
    "                # We sum the log probabilities because the probabilities are in log space\n",
    "                new_candidates.append((new_candidate, score + token_prob))\n",
    "\n",
    "        # Sort the new candidates by their score\n",
    "        candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
    "        # Keep only the top k candidates\n",
    "        candidates = candidates[:beam_size]\n",
    "\n",
    "        # If all the candidates have reached the eos token, stop\n",
    "        if all([cand[0][-1].item() == eos_idx for cand, _ in candidates]):\n",
    "            break\n",
    "\n",
    "    # Return the best candidate\n",
    "    return candidates[0][0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted_greedy = []\n",
    "    predicted_beam = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "            model_out_beam = beam_search_decode(model, 3, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text_beam = tokenizer_tgt.decode(model_out_beam.detach().cpu().numpy())\n",
    "            model_out_text_greedy = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted_greedy.append(model_out_text_greedy)\n",
    "            predicted_beam.append(model_out_text_beam)\n",
    "            \n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED GREEDY: ':>12}{model_out_text_greedy}\")\n",
    "            print_msg(f\"{f'PREDICTED BEAM:   ':>12}{model_out_text_beam}\")\n",
    "\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "    \n",
    "    \n",
    "    # Evaluate the character error rate\n",
    "    # Compute the char error rate \n",
    "    metric_greedy = torchmetrics.CharErrorRate()\n",
    "    cer = metric_greedy(predicted_greedy, expected)\n",
    "    wandb.log({'validation_greedy/cer': cer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the word error rate\n",
    "    metric_greedy = torchmetrics.WordErrorRate()\n",
    "    wer = metric_greedy(predicted_greedy, expected)\n",
    "    wandb.log({'validation_greedy/wer': wer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the BLEU metric\n",
    "    metric_greedy = torchmetrics.BLEUScore()\n",
    "    bleu = metric_greedy(predicted_greedy, expected)\n",
    "    wandb.log({'validation_greedy/BLEU': bleu, 'global_step': global_step})\n",
    "\n",
    "    # Evaluate the character error rate\n",
    "    # Compute the char error rate \n",
    "    metric_beam = torchmetrics.CharErrorRate()\n",
    "    cer = metric_beam(predicted_beam, expected)\n",
    "    wandb.log({'validation_beam/cer': cer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the word error rate\n",
    "    metric = torchmetrics.WordErrorRate()\n",
    "    wer = metric_beam(predicted_beam, expected)\n",
    "    wandb.log({'validation_beam/wer': wer, 'global_step': global_step})\n",
    "\n",
    "    # Compute the BLEU metric\n",
    "    metric = torchmetrics.BLEUScore()\n",
    "    bleu = metric_beam(predicted_beam, expected)\n",
    "    wandb.log({'validation_beam/BLEU': bleu, 'global_step': global_step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'],\n",
    "        N_layers=config['N_layers'], h = config['heads'], dropout = config['dropout'], d_ff = config['ffn_hidden'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config,train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt ):\n",
    "    # Define the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    if (device == 'cuda'):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "     \n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config['datasource']}_{config['model_folder'].format(config['version'])}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9, betas = (0.9, 0.98))\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "\n",
    "    if model_filename:\n",
    "    # if False:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    # define our custom x axis metric\n",
    "    wandb.define_metric(\"global_step\")\n",
    "    # define which metrics will be plotted against it\n",
    "    wandb.define_metric(\"validation_greedy/*\", step_metric=\"global_step\")\n",
    "    wandb.define_metric(\"validation_beam/*\", step_metric=\"global_step\")\n",
    "    wandb.define_metric(\"train/*\", step_metric=\"global_step\")\n",
    "    # wandb.define_metric(\"ln_rate/*\", step_metric=\"global_step\")\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "\n",
    "\n",
    "            # Log the loss\n",
    "            wandb.log({'train/loss': loss.item(), 'global_step': global_step})\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            (loss / config['batch_size']).backward()\n",
    "\n",
    "            global_step += 1\n",
    "            \n",
    "            # Log the loss\n",
    "            wandb.log({'train/ln_rate': optimizer.param_groups[0]['lr'], 'global_step': global_step})\n",
    "            if (batch_iterator.n +1) % config['batch_size'] == 0:\n",
    "                # Update the weights\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                lr_updated = get_lr(step=global_step, d_model=config['d_model'], warmup_steps=config['warmup_steps'])\n",
    "\n",
    "                change_lr(optimizer, new_lr=lr_updated)\n",
    "            \n",
    "            batch_iterator.set_postfix({\n",
    "                \"loss\": f\"{loss.item():6.3f}\",\n",
    "                'lr' : f\"{optimizer.param_groups[0]['lr']:.6g}\"\n",
    "                                        })\n",
    "            \n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step)\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_tgt = get_or_build_tokenizer( filtered_ds_raw, config['lang_tgt'],config['version'])\n",
    "tokenizer_src =  get_or_build_tokenizer( filtered_ds_raw, config['lang_src'],config['version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['vocab_size_tgt'] = tokenizer_tgt.get_vocab_size()\n",
    "config['vocab_size_src'] = tokenizer_src.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_size = int(0.8 * len(filtered_ds_raw))\n",
    "val_ds_size = len(filtered_ds_raw) - train_ds_size\n",
    "train_ds_raw, val_ds_raw = random_split(filtered_ds_raw, [train_ds_size, val_ds_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420646"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_ds_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336516"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, src_lang = config['lang_src'],tgt_lang = config['lang_tgt'], seq_len = config['seq_len'])\n",
    "val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, src_lang = config['lang_src'],tgt_lang = config['lang_tgt'], seq_len = config['seq_len'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\wandb\\run-20241106_150032-xkkus8d2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu007/runs/xkkus8d2' target=\"_blank\">likely-tree-11</a></strong> to <a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu007' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu007' target=\"_blank\">https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu007</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu007/runs/xkkus8d2' target=\"_blank\">https://wandb.ai/salcantaratnaist/pytorch-transformer_es_qu007/runs/xkkus8d2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preloading model allenai\\nllb_weights_es_qu007\\tmodel_es_qu00700.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 01:  47%|████▋     | 4917/10517 [16:16<18:31,  5.04it/s, loss=6.738, lr=0.000711953]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 14\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# config['batch_size'] = 48\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# config['preload'] = None\u001b[39;00m\n\u001b[0;32m      7\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# set the wandb project where this run will be logged\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch-transformer_es_qu\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_tgt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[42], line 71\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(config, train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt)\u001b[0m\n\u001b[0;32m     68\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/loss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_step\u001b[39m\u001b[38;5;124m'\u001b[39m: global_step})\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Backpropagate the loss\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Log the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kainak0\\Documents\\gitProjects\\mia\\MIA-203_redes_neuronales\\final\\translatepy\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    config = get_config()\n",
    "    config['num_epochs'] = 10\n",
    "    # config['batch_size'] = 48\n",
    "    # config['preload'] = None\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"pytorch-transformer_es_qu{0}\".format(config['version']),\n",
    "        \n",
    "        # track hyperparameters and run metadata\n",
    "        config=config\n",
    "    )\n",
    "    train_model(config,train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = get_config()\n",
    "# train_dataloader, val_dataloader, vocab_src, vocab_tgt = get_ds(config)\n",
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = get_weights_file_path(config, f\"29\")\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_next_batch():\n",
    "    # Load a sample batch from the validation set\n",
    "    batch = next(iter(val_dataloader))\n",
    "    encoder_input = batch[\"encoder_input\"].to(device)\n",
    "    encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "    decoder_input = batch[\"decoder_input\"].to(device)\n",
    "    decoder_mask = batch[\"decoder_mask\"].to(device)\n",
    "\n",
    "    encoder_input_tokens = [tokenizer_src.id_to_token(idx) for idx in encoder_input[0].cpu().numpy()]\n",
    "    decoder_input_tokens = [tokenizer_tgt.id_to_token(idx) for idx in decoder_input[0].cpu().numpy()]\n",
    "\n",
    "    # check that the batch size is 1\n",
    "    assert encoder_input.size(\n",
    "        0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "    model_out = greedy_decode(\n",
    "        model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, config['seq_len'], device)\n",
    "    \n",
    "    return batch, encoder_input_tokens, decoder_input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "def mtx2df(m, max_row, max_col, row_tokens, col_tokens):\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            (\n",
    "                r,\n",
    "                c,\n",
    "                float(m[r, c]),\n",
    "                \"%.3d %s\" % (r, row_tokens[r] if len(row_tokens) > r else \"<blank>\"),\n",
    "                \"%.3d %s\" % (c, col_tokens[c] if len(col_tokens) > c else \"<blank>\"),\n",
    "            )\n",
    "            for r in range(m.shape[0])\n",
    "            for c in range(m.shape[1])\n",
    "            if r < max_row and c < max_col\n",
    "        ],\n",
    "        columns=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
    "    )\n",
    "\n",
    "def get_attn_map(attn_type: str, layer: int, head: int):\n",
    "    if attn_type == \"encoder\":\n",
    "        attn = model.encoder.layers[layer].self_attention_block.attention_scores\n",
    "    elif attn_type == \"decoder\":\n",
    "        attn = model.decoder.layers[layer].self_attention_block.attention_scores\n",
    "    elif attn_type == \"encoder-decoder\":\n",
    "        attn = model.decoder.layers[layer].cross_attention_block.attention_scores\n",
    "    return attn[0, head].data\n",
    "\n",
    "def attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len):\n",
    "    df = mtx2df(\n",
    "        get_attn_map(attn_type, layer, head),\n",
    "        max_sentence_len,\n",
    "        max_sentence_len,\n",
    "        row_tokens,\n",
    "        col_tokens,\n",
    "    )\n",
    "    return (\n",
    "        alt.Chart(data=df)\n",
    "        .mark_rect()\n",
    "        .encode(\n",
    "            x=alt.X(\"col_token\", axis=alt.Axis(title=\"\")),\n",
    "            y=alt.Y(\"row_token\", axis=alt.Axis(title=\"\")),\n",
    "            color=\"value\",\n",
    "            tooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
    "        )\n",
    "        #.title(f\"Layer {layer} Head {head}\")\n",
    "        .properties(height=400, width=400, title=f\"Layer {layer} Head {head}\")\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "def get_all_attention_maps(attn_type: str, layers: list[int], heads: list[int], row_tokens: list, col_tokens, max_sentence_len: int):\n",
    "    charts = []\n",
    "    for layer in layers:\n",
    "        rowCharts = []\n",
    "        for head in heads:\n",
    "            rowCharts.append(attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len))\n",
    "        charts.append(alt.hconcat(*rowCharts))\n",
    "    return alt.vconcat(*charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, encoder_input_tokens, decoder_input_tokens = load_next_batch()\n",
    "print(f'Source: {batch[\"src_text\"][0]}')\n",
    "print(f'Target: {batch[\"tgt_text\"][0]}')\n",
    "sentence_len = encoder_input_tokens.index(\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [0, 1, 2, 3, 4, 5]\n",
    "heads = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Encoder Self-Attention\n",
    "get_all_attention_maps(\"encoder\", layers, heads, encoder_input_tokens, encoder_input_tokens, min(20, sentence_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Self-Attention\n",
    "get_all_attention_maps(\"decoder\", layers, heads, decoder_input_tokens, decoder_input_tokens, min(20, sentence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Self-Attention\n",
    "get_all_attention_maps(\"encoder-decoder\", layers, heads, encoder_input_tokens, decoder_input_tokens, min(20, sentence_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translatepy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
