{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:red\"><center>Introducción al Procesamiento Superficial de Textos</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Latent Dirichlet Allocation - LDA</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Análisis superficial de textos](#Análisis-superficial-de-textos)\n",
    "* [Terminología general](#Terminología-general)\n",
    "* [Preprocesamiento de datos textuales](#Preprocesamiento-de-datos-textuales)\n",
    "* [Bag of Words](#BagofWords)\n",
    "* [TF-IDF](#TF-IDF)\n",
    "* [Semántica latente](#Semántica-latente)\n",
    "* [Modelos generativos: Latent Dirichlet Allocation](#Modelos-generativos:-Latent-Dirichlet-Allocation)\n",
    "* [Ejemplo:Un millón de titulares](#Ejemplo:-Un-millón-de-titulares)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los humanos nos  comunicamos utilizando lenguajes naturales. Los lenguajes naturales difieren de los lenguajes de programación en que éstos últimos siguen reglas sintácticas y semánticas extrictas, mientras que los primeros por su complejidad dependen del contexto.\n",
    "\n",
    "\n",
    "En general el análisis de textos tiene dos grandes subáreas: el análisis superficial de textos y el procesamiento del lenguaje natural.\n",
    "\n",
    "En esta lección nos ocupamos del análisis superficial de textos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Análisis superficial de textos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta subárea se desarrolló primero, debido a que los problemas asociados al lenguaje natural en este caso son mas simples. Se trata de técnicas en la cuales se busca encontar los tópicos subayacentes en los texto. En este sentido, son modelos de tipo no supervisado y consecuencia basados en técnicas de clasificación automática. \n",
    "\n",
    "Estas técnicas están orientadas a detectar clusters de palabras y documentos en grandes corpus de datos.\n",
    "\n",
    "Un documento es en este caso una unidad distinguible de otras en el corpus. Por ejemplo una respuesta abierta en una encuesta, un comentario en una revisión, un abstract de un documento, etc. \n",
    "\n",
    "Luego de omitidos términos que se considera que no aportan a la detección de tópicos (temáticas), usualmente conocidos como *palabras vacías* (`stop words`) y de otros procesos de preprocesamiento como lematización, recorte (`steeming`),  es común construir una matriz denominada documento-témino (`dtm`).\n",
    "\n",
    "Esta matriz `dtm` representa por las filas a cada uno de los documentos individuales del corpus y por las columnas a cada uno de los términos conservados en el análisis. Cada posición de la matriz contiene el número de veces que un término aparece en el documento. En algunos casos esta es una matriz binaria, en cuyo caso la dtm indica cuando un término aparece en un documento.\n",
    "\n",
    "La  `dtm` es la base de las técnicas conocidas genéricamente como *bolsa de palabras* (`word-bag`). El nombre deriva del hecho de que al organizar la dtm, el contexto de las palabras en cada documento se pierde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Terminología general</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras o términos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La palabra es la unidad mínima de información  en el trabajo con lenguaje natural. \n",
    "\n",
    "Desde una perspectiva muy moderna las palabras son objetos que puede pensarse como puntos que están en un espacio de alta dimensión, de tal manera que, puntos cercanos en algún sentido de distancia corresponde a palabras que tienen una cercanía dentro de un universo de palabras considerado.\n",
    "\n",
    "La siguiente imagen corresponde a uno conjunto de palabras de astrofísica, consideradas en un estudio de resumenes de artículos científicos. Este es un gráfico obtenido luego de un procesamiento como lo que mostramos hoy, usando una técnica de análisis basada en la teoría de respuesta al ítem multidimensional (TRIM).\n",
    "\n",
    "En este documento las palabaras se denotarán como $w_i, i =1,2,\\ldots,K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/cluster_kmeans_10.png\" width=\"700\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Areas de conocimiento Astrofísica, a partir de artículos científicos</p>\n",
    "</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los documentos son los sujetos en los análisis textual superficial. Suponemos que se tiene un conjunto de documentos individuales, cada uno de los cuales se denotará por $\\mathbf{w}$. Se considera que un documento es una sucesión  de $N$ palabras. Así se tiene que un documento se denota como $\\mathbf{w} = \\{w_1,\\ldots,w_N \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un corpus es una colección de documentos en un problema particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tópicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tópicos son áreas latentes a las cuales están asociados tanto las palabras como los documentos. Uno de los propósitos principales del análisis de textos es descubrir o poner en evidencia tales tópicos.\n",
    "\n",
    "La figura anterior muestra por ejemplo la presencia de 10 tópicos en el conjunto de documentos de astrofísica analizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Preprocesamiento de datos textuales</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lo que sigue, vamos a utilizar los términos token y tokenizar, que aún no son adoptados por la Real Academia de la Lengua, pero que creemos pronto lo serán como tantos otros provenientes del inglés debido a su enorme utilización actual, por razón de los desarrollos científicos y tecnológicos. \n",
    "\n",
    "Realizaremos los siguientes pasos:\n",
    "    \n",
    "- **Tokenización**: divide el texto en oraciones y las oraciones en palabras. Ponga las palabras en minúsculas y elimine la puntuación.\n",
    "- Se **eliminan las palabras que tienen menos de 3 caracteres**.\n",
    "- Se eliminan todas las **palabras vacías**.\n",
    "- Las palabras se **lematizan**: las palabras en tercera persona se cambian a primera persona y los verbos en tiempo pasado y futuro se cambian a presente.\n",
    "- Las palabras se recortan (**stemming**): las palabras se reducen a su forma raíz.\n",
    "\n",
    "Usaremos  las librerías *gensim* y *nltk* para hacer este trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos términos que se utilizarán con frecuencia son:\n",
    "\n",
    "- `Corpus`: cuerpo del texto, singular. Corpora es el plural de corpus.\n",
    "- `Léxico`: palabras y sus significados.\n",
    "- `Token`: cada *entidad* que es parte de lo que sea que se dividió según las reglas que establecemos para el análisis. Por ejemplo, cada palabra es un token cuando una oración se tokeniza en palabras. Cada oración también puede ser un token, si ha convertido las oraciones en un párrafo.\n",
    "\n",
    "Básicamente, tokenizar implica dividir oraciones y palabras del cuerpo del texto.\n",
    "\n",
    "Veá el siguiente ejemplo tomado de [Geek for Geeks](https://www.geeksforgeeks.org/tokenize-text-using-nltk-python/?ref=rp). Usamos la librería *nltk*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa recursos de `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kainak0/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kainak0/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/kainak0/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import the existing word and sentence tokenizing  \n",
    "# libraries \n",
    "import nltk\n",
    "\n",
    "\n",
    "# tokenizadores\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# diccionarios especiales para puntuación y palabras vacias\n",
    "nltk.download('punkt') # Manejo de puntuación\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# lematizador basado en WordNet de nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# steemer de nltk. Raiz de las palabras\n",
    "#from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa recursos de `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/kainak0/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages (4.3.3)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /home/kainak0/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/kainak0/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /home/kainak0/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (13.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.5\n",
      "    Uninstalling numpy-2.2.5:\n",
      "      Successfully uninstalled numpy-2.2.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparsing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m STOPWORDS\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/gensim/__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m4.3.3\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m     14\u001b[39m logger = logging.getLogger(\u001b[33m'\u001b[39m\u001b[33mgensim\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger.handlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/gensim/corpora/__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[32m     16\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIndexedCorpus\u001b[39;00m(interfaces.CorpusABC):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/gensim/interfaces.py:19\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[33;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[32m     22\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCorpusABC\u001b[39;00m(utils.SaveLoad):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/gensim/matutils.py:1034\u001b[39m\n\u001b[32m   1029\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1.\u001b[39m - \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(set1 & set2)) / \u001b[38;5;28mfloat\u001b[39m(union_cardinality)\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;66;03m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlogsumexp\u001b[39m(x):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/gensim/_matutils.pyx:1\u001b[39m, in \u001b[36minit gensim._matutils\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenización por sentencias:\n",
      "\n",
      "Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora. \n",
      "\n",
      "Challenges in natural language processing frequently involve natural language understanding, natural language generation frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof. \n",
      "\n",
      "There are 365 days usually. \n",
      "\n",
      "This year is 2020. \n",
      "\n",
      "Tokenización por sentencias:\n",
      "\n",
      "['Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.', 'Challenges in natural language processing frequently involve natural language understanding, natural language generation frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.', 'There are 365 days usually.', 'This year is 2020.']\n",
      "\n",
      " Tokenización por palabras:\n",
      "Natural language processing ( NLP ) is a field of computer science , artificial intelligence and computational linguistics concerned with the interactions between computers and human ( natural ) languages , and , in particular , concerned with programming computers to fruitfully process large natural language corpora . Challenges in natural language processing frequently involve natural language understanding , natural language generation frequently from formal , machine-readable logical forms ) , connecting language and machine perception , managing human-computer dialog systems , or some combination thereof . There are 365 days usually . This year is 2020 . \n",
      "\n",
      " Tokenización por palabras:\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.', 'There', 'are', '365', 'days', 'usually', '.', 'This', 'year', 'is', '2020', '.']\n",
      "\n",
      " Tokenización por caracteres:\n",
      "N a t u r a l   l a n g u a g e   p r o c e s s i n g   ( N L P )   i s   a   f i e l d   o f   c o m p u t e r   s c i e n c e ,   a r t i f i c i a l   i n t e l l i g e n c e   a n d   c o m p u t a t i o n a l   l i n g u i s t i c s   c o n c e r n e d   w i t h   t h e   i n t e r a c t i o n s   b e t w e e n   c o m p u t e r s   a n d   h u m a n   ( n a t u r a l )   l a n g u a g e s ,   a n d ,   i n   p a r t i c u l a r ,   c o n c e r n e d   w i t h   p r o g r a m m i n g   c o m p u t e r s   t o   f r u i t f u l l y   p r o c e s s   l a r g e   n a t u r a l   l a n g u a g e   c o r p o r a .   C h a l l e n g e s   i n   n a t u r a l   l a n g u a g e   p r o c e s s i n g   f r e q u e n t l y   i n v o l v e   n a t u r a l   l a n g u a g e   u n d e r s t a n d i n g ,   n a t u r a l   l a n g u a g e   g e n e r a t i o n   f r e q u e n t l y   f r o m   f o r m a l ,   m a c h i n e - r e a d a b l e   l o g i c a l   f o r m s ) ,   c o n n e c t i n g   l a n g u a g e   a n d   m a c h i n e   p e r c e p t i o n ,   m a n a g i n g   h u m a n - c o m p u t e r   d i a l o g   s y s t e m s ,   o r   s o m e   c o m b i n a t i o n   t h e r e o f .   T h e r e   a r e   3 6 5   d a y s   u s u a l l y .   T h i s   y e a r   i s   2 0 2 0 . \n",
      "\n",
      " Tokenización por caracteres:\n",
      "['N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', '(', 'N', 'L', 'P', ')', ' ', 'i', 's', ' ', 'a', ' ', 'f', 'i', 'e', 'l', 'd', ' ', 'o', 'f', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', ' ', 's', 'c', 'i', 'e', 'n', 'c', 'e', ',', ' ', 'a', 'r', 't', 'i', 'f', 'i', 'c', 'i', 'a', 'l', ' ', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', ' ', 'a', 'n', 'd', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'a', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'l', 'i', 'n', 'g', 'u', 'i', 's', 't', 'i', 'c', 's', ' ', 'c', 'o', 'n', 'c', 'e', 'r', 'n', 'e', 'd', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'i', 'n', 't', 'e', 'r', 'a', 'c', 't', 'i', 'o', 'n', 's', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'u', 'm', 'a', 'n', ' ', '(', 'n', 'a', 't', 'u', 'r', 'a', 'l', ')', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', 's', ',', ' ', 'a', 'n', 'd', ',', ' ', 'i', 'n', ' ', 'p', 'a', 'r', 't', 'i', 'c', 'u', 'l', 'a', 'r', ',', ' ', 'c', 'o', 'n', 'c', 'e', 'r', 'n', 'e', 'd', ' ', 'w', 'i', 't', 'h', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', 's', ' ', 't', 'o', ' ', 'f', 'r', 'u', 'i', 't', 'f', 'u', 'l', 'l', 'y', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', ' ', 'l', 'a', 'r', 'g', 'e', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'c', 'o', 'r', 'p', 'o', 'r', 'a', '.', ' ', 'C', 'h', 'a', 'l', 'l', 'e', 'n', 'g', 'e', 's', ' ', 'i', 'n', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', 'f', 'r', 'e', 'q', 'u', 'e', 'n', 't', 'l', 'y', ' ', 'i', 'n', 'v', 'o', 'l', 'v', 'e', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', 'i', 'n', 'g', ',', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'i', 'o', 'n', ' ', 'f', 'r', 'e', 'q', 'u', 'e', 'n', 't', 'l', 'y', ' ', 'f', 'r', 'o', 'm', ' ', 'f', 'o', 'r', 'm', 'a', 'l', ',', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', '-', 'r', 'e', 'a', 'd', 'a', 'b', 'l', 'e', ' ', 'l', 'o', 'g', 'i', 'c', 'a', 'l', ' ', 'f', 'o', 'r', 'm', 's', ')', ',', ' ', 'c', 'o', 'n', 'n', 'e', 'c', 't', 'i', 'n', 'g', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'a', 'n', 'd', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'p', 'e', 'r', 'c', 'e', 'p', 't', 'i', 'o', 'n', ',', ' ', 'm', 'a', 'n', 'a', 'g', 'i', 'n', 'g', ' ', 'h', 'u', 'm', 'a', 'n', '-', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', ' ', 'd', 'i', 'a', 'l', 'o', 'g', ' ', 's', 'y', 's', 't', 'e', 'm', 's', ',', ' ', 'o', 'r', ' ', 's', 'o', 'm', 'e', ' ', 'c', 'o', 'm', 'b', 'i', 'n', 'a', 't', 'i', 'o', 'n', ' ', 't', 'h', 'e', 'r', 'e', 'o', 'f', '.', ' ', 'T', 'h', 'e', 'r', 'e', ' ', 'a', 'r', 'e', ' ', '3', '6', '5', ' ', 'd', 'a', 'y', 's', ' ', 'u', 's', 'u', 'a', 'l', 'l', 'y', '.', ' ', 'T', 'h', 'i', 's', ' ', 'y', 'e', 'a', 'r', ' ', 'i', 's', ' ', '2', '0', '2', '0', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural language processing (NLP) is a field \" \\\n",
    "       + \"of computer science, artificial intelligence \" \\\n",
    "       + \"and computational linguistics concerned with \" \\\n",
    "       +\"the interactions between computers and human \" \\\n",
    "       + \"(natural) languages, and, in particular, \" \\\n",
    "       + \"concerned with programming computers to \" \\\n",
    "       + \"fruitfully process large natural language \" \\\n",
    "       + \"corpora. Challenges in natural language \" \\\n",
    "       + \"processing frequently involve natural \" \\\n",
    "       + \"language understanding, natural language \" \\\n",
    "       + \"generation frequently from formal, machine\" \\\n",
    "       + \"-readable logical forms), connecting language \" \\\n",
    "       + \"and machine perception, managing human-\" \\\n",
    "       + \"computer dialog systems, or some combination \" \\\n",
    "       + \"thereof. There are 365 days usually. \" \\\n",
    "       + \"This year is 2020.\"\n",
    "\n",
    "# sentencias\n",
    "print('Tokenización por sentencias:\\n')\n",
    "sentences = sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence,'\\n')\n",
    "print('Tokenización por sentencias:\\n')\n",
    "print(sent_tokenize(text)) \n",
    "\n",
    "# palabras\n",
    "tokens = word_tokenize(text)\n",
    "print('\\n Tokenización por palabras:')\n",
    "for token in tokens:\n",
    "    print(token, end =' ')\n",
    "print('')\n",
    "print('\\n Tokenización por palabras:')\n",
    "print(word_tokenize(text))\n",
    "\n",
    "# caracteres\n",
    "chars = [char for char in text]\n",
    "print('\\n Tokenización por caracteres:')\n",
    "for char in chars:\n",
    "    print(char, end =' ')\n",
    "print('')\n",
    "print('\\n Tokenización por caracteres:')\n",
    "print(chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cooool',\n",
       " '#dummysmiley',\n",
       " ':',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<',\n",
       " '>',\n",
       " '->',\n",
       " '<--']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tknzr.tokenize(s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizar tweets usando  los parámetros `strip_handles` y `reduce_len`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "s1 = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
    "tw = tknzr.tokenize(s1)\n",
    "print(tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cambiar texto a minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.', 'there', 'are', '365', 'days', 'usually', '.', 'this', 'year', 'is', '2020', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens[:] = [token.lower() for token in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remueve carateres especiales - expresiones regulares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las expresiones regulares son objetos matemáticos que permiten interpretar trozos de texto. Son claves en la construcción de los lenguajes de programación. Aquí vamos a usar la librería [re](https://docs.python.org/3/library/re.html) de Python creada para el manejo de expresiones regulares. Les sugerimos este [tutorial sobre re en Python](https://www.w3schools.com/python/python_regex.asp) para aprender a manejar la librería re.\n",
    "\n",
    "Usaremos aquí para eliminar algunos símbolos: los números y los paréntesis por ejemplo. No siempre es el caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '', 'nlp', '', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '', 'natural', '', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', '', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.', 'there', 'are', '', 'days', 'usually', '.', 'this', 'year', 'is', '', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# substituir digitos\n",
    "tokens = [re.sub(r'\\d+', '',token) for token in tokens]\n",
    "# substituir paréntesis\n",
    "tokens = [re.sub(r'[()]', '',token) for token in tokens]\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remueve palabras de longitud menor o igual a tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'with', 'interactions', 'between', 'computers', 'human', 'natural', 'languages', 'particular', 'concerned', 'with', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', 'machine-readable', 'logical', 'forms', 'connecting', 'language', 'machine', 'perception', 'managing', 'human-computer', 'dialog', 'systems', 'some', 'combination', 'thereof', 'there', 'days', 'usually', 'this', 'year']\n"
     ]
    }
   ],
   "source": [
    "tokens_4 = []\n",
    "for token in tokens:\n",
    "    if len(token) > 3:\n",
    "        tokens_4.append(token)\n",
    "tokens = tokens_4\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Palabras vacias (stop words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las palabras varias o stop words son palabras que en el lenguaje común se considera que no aportan al contenido semántico de los textos. En la técnica de bolsa de palabras son omitidos, debido a que causan clasificaciones confusas. En realidad el concepto de palabras vacía depende del contesto de utlización de las técnicas. \n",
    "\n",
    "El siguiente ejemplo muestra el diccionario de palabras vacías del inglés contenidas en la librería `gensim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m stop_words_g = []\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token  \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgensim\u001b[49m.parsing.preprocessing.STOPWORDS:\n\u001b[32m      3\u001b[39m     stop_words_g.append(token)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(stop_words_g)\n",
      "\u001b[31mNameError\u001b[39m: name 'gensim' is not defined"
     ]
    }
   ],
   "source": [
    "stop_words_g = []\n",
    "for token  in gensim.parsing.preprocessing.STOPWORDS:\n",
    "    stop_words_g.append(token)\n",
    "print(stop_words_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la librería *nltk* el diciconario de palabras vacías del inglés es actualmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'whom', 'yours', 'has', 'won', 'before', 'can', 'only', 'nor', 'ours', \"they're\", \"weren't\", 'for', 'but', \"that'll\", 'a', 'up', 'had', \"hasn't\", 't', 'such', 'isn', \"she'll\", 'further', \"we've\", 'themselves', 'm', \"shan't\", 'and', \"you've\", 'under', \"you'll\", \"haven't\", 'hers', 'i', 'while', 'mightn', 'where', 'more', 'my', 'until', 'yourself', 'over', 'yourselves', 'couldn', 'with', \"she'd\", 'most', 'is', 'all', 'some', \"couldn't\", 'how', 'now', 'will', 'didn', 'after', \"she's\", 'doesn', 'than', \"he's\", 'as', 'if', 'which', 'been', \"doesn't\", 'wasn', 'am', 'needn', 'on', 'its', 'she', \"should've\", \"i'll\", 'him', \"they'd\", 'who', 'shouldn', 'just', 'any', 'each', \"wouldn't\", 'against', 'these', 'ma', 'our', 'itself', 'an', 'out', 'what', 've', \"mightn't\", \"he'll\", 'your', 'when', 'the', 'between', 'having', 'll', 'hasn', 'too', 'herself', 'same', 'shan', 'there', 'again', 'himself', 'few', 'we', 'o', \"won't\", \"hadn't\", \"isn't\", 'ourselves', 'so', 'doing', \"we'd\", 'through', 'are', 'at', \"i'm\", 'of', 'own', 'should', \"he'd\", 'in', 'wouldn', 'why', 're', 'weren', \"wasn't\", 'then', 'those', 'y', 'did', 'during', 'were', \"you're\", \"you'd\", 'below', 'because', 'them', \"aren't\", 'you', \"we'll\", \"we're\", 'hadn', 'this', 'their', \"it'd\", 'it', 'myself', 'down', \"it'll\", 'theirs', 'here', 'mustn', 'haven', 'not', 'was', \"i've\", 'have', 'her', \"shouldn't\", \"mustn't\", 'above', 'by', 'both', 'do', \"don't\", 'his', 'once', 'don', 'from', \"didn't\", 'does', \"needn't\", \"it's\", 'very', 'd', 'no', \"they've\", \"they'll\", 'other', 'that', 'into', 'aren', 'he', 'being', 'be', 'about', 's', 'or', \"i'd\", 'me', 'ain', 'they', 'to', 'off'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#\n",
    "stopWords = set(stopwords.words('english'))\n",
    "#\n",
    "print(stopWords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que los dos conjuntos de palabara vacías son distintos.\n",
    "\n",
    "Como ejemplo vamos quitar la palabras vacias del objeto text tokenizado definido arriba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras vacías-Español nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hay', 'más', 'él', 'estuvisteis', 'en', 'ti', 'habidas', 'habías', 'quien', 'otros', 'estuviera', 'tus', 'tuya', 'todo', 'hayamos', 'tendréis', 'fui', 'fue', 'fueron', 'nos', 'estuviese', 'os', 'seríais', 'tengas', 'porque', 'había', 'como', 'mucho', 'estemos', 'estamos', 'vosotras', 'estábamos', 'tuvo', 'algunas', 'vosotros', 'eso', 'seré', 'o', 'tienen', 'hubiste', 'hube', 'habían', 'hayáis', 'ella', 'contra', 'mío', 'tuvieron', 'estaríamos', 'hubieseis', 'estuviéramos', 'hubieron', 'hubisteis', 'habrían', 'tendrás', 'tenías', 'nuestra', 'fuera', 'habida', 'estés', 'soy', 'suyo', 'habríais', 'estarían', 'tengo', 'estaríais', 'no', 'hemos', 'muchos', 'tendrán', 'antes', 'sus', 'me', 'tuyo', 'habrías', 'ni', 'estéis', 'habréis', 'también', 'suyos', 'esos', 'siente', 'tenemos', 'nuestro', 'estuviesen', 'ha', 'seríamos', 'estando', 'hayan', 'serían', 'habido', 'vuestro', 'algo', 'estadas', 'seréis', 'ellas', 'fuésemos', 'se', 'seas', 'tendríais', 'tenían', 'sentid', 'otra', 'estas', 'sobre', 'estaremos', 'habrán', 'hubiese', 'tuyos', 'estoy', 'estuvimos', 'suya', 'fuisteis', 'de', 'tuviste', 'la', 'fueseis', 'tu', 'estará', 'estaban', 'esa', 'habremos', 'estuviésemos', 'teníais', 'tuvisteis', 'durante', 'pero', 'cuando', 'estuvieran', 'tendré', 'les', 'fuiste', 'qué', 'al', 'nosotros', 'estuvieses', 'habíais', 'tanto', 'tuvierais', 'tuvieseis', 'tuviésemos', 'mi', 'seremos', 'le', 'sintiendo', 'estuvieseis', 'serías', 'tuvieras', 'será', 'estás', 'poco', 'habré', 'ellos', 'tenida', 'estuviste', 'son', 'fuesen', 'estabais', 'estáis', 'estaba', 'has', 'tuvimos', 'éramos', 'tened', 'sí', 'seáis', 'donde', 'a', 'estaré', 'habrá', 'estarán', 'este', 'estuvierais', 'sentidos', 'eran', 'ese', 'mía', 'tenidas', 'tuyas', 'hayas', 'otro', 'unos', 'tuviese', 'del', 'tenido', 'tengamos', 'fuese', 'estuvieras', 'nada', 'fueses', 'tengáis', 'habrás', 'fuéramos', 'con', 'tendrá', 'para', 'tendrían', 'nosotras', 'esta', 'entre', 'habría', 'somos', 'serás', 'mí', 'estaría', 'han', 'estuvieron', 'desde', 'tuviesen', 'e', 'hubiésemos', 'ya', 'y', 'estos', 'habéis', 'nuestros', 'estados', 'hubiéramos', 'tuvieran', 'mis', 'suyas', 'el', 'estarás', 'estabas', 'estad', 'eras', 'tendríamos', 'tuviera', 'vuestras', 'habidos', 'tuviéramos', 'serán', 'he', 'sea', 'sentida', 'hubiesen', 'vuestra', 'hubiera', 'sois', 'estarías', 'tendría', 'sentido', 'hasta', 'tú', 'muy', 'hubieses', 'mías', 'tuvieses', 'teniendo', 'fuerais', 'ante', 'esté', 'estuve', 'vuestros', 'hubierais', 'sean', 'lo', 'nuestras', 'erais', 'era', 'estuvo', 'un', 'seamos', 'habíamos', 'las', 'sería', 'está', 'hubimos', 'te', 'tenéis', 'estado', 'hubieras', 'estada', 'hubieran', 'esas', 'estén', 'habríamos', 'esto', 'tiene', 'eres', 'están', 'los', 'míos', 'fuimos', 'que', 'sin', 'fueras', 'haya', 'hubo', 'uno', 'tendrías', 'tenga', 'es', 'cual', 'tuve', 'teníamos', 'tendremos', 'tenía', 'estaréis', 'estar', 'tengan', 'una', 'quienes', 'habiendo', 'sentidas', 'yo', 'su', 'fueran', 'tienes', 'todos', 'tenidos', 'algunos', 'por', 'otras'}\n"
     ]
    }
   ],
   "source": [
    "palabrasVacias = set(stopwords.words('spanish'))\n",
    "#\n",
    "print(palabrasVacias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a quitar las palabras vacías del ejemplo usando nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'with', 'interactions', 'between', 'computers', 'human', 'natural', 'languages', 'particular', 'concerned', 'with', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', 'machine-readable', 'logical', 'forms', 'connecting', 'language', 'machine', 'perception', 'managing', 'human-computer', 'dialog', 'systems', 'some', 'combination', 'thereof', 'there', 'days', 'usually', 'this', 'year']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'interactions', 'computers', 'human', 'natural', 'languages', 'particular', 'concerned', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'frequently', 'formal', 'machine-readable', 'logical', 'forms', 'connecting', 'language', 'machine', 'perception', 'managing', 'human-computer', 'dialog', 'systems', 'combination', 'thereof', 'days', 'usually', 'year']\n"
     ]
    }
   ],
   "source": [
    "tokens_n_e = []\n",
    "\n",
    "for token in tokens:\n",
    "    if token not in stopWords:\n",
    "        tokens_n_e.append(token)\n",
    "#\n",
    "tokens = tokens_n_e\n",
    "print(tokens)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lematización es el proceso de agrupar las diferentes formas flexionadas de una palabra para que puedan analizarse como un solo elemento. La lematización es similar a la derivación, pero aporta contexto a las palabras. Por lo tanto, vincula palabras con un significado similar a una palabra.\n",
    "\n",
    "El preprocesamiento de texto incluye tanto `Stemming` como `Lemmatization`. \n",
    "\n",
    "Muchas veces las personas encuentran confusos estos dos términos. Algunos tratan a estos dos como iguales. \n",
    "\n",
    "En realidad, se prefiere la lematización a la derivación porque la lematización realiza un análisis morfológico de las palabras.\n",
    "\n",
    "Las aplicaciones de la lematización son:\n",
    "\n",
    "- Se utiliza en sistemas de recuperación integrales como motores de búsqueda.\n",
    "- Utilizado en indexación compacta\n",
    "- Ejemplos de lematización:\n",
    "\n",
    "* rocas -> roca\n",
    "*  corpora -> corpus\n",
    "* mejor -> bueno\n",
    "\n",
    "Una diferencia importante con la derivación es que lematizar toma una parte del parámetro de voz, \"pos\". Si no se proporciona, el valor predeterminado es \"sustantivo\". En el siguiente ejemplo vamos colocar *pos='a'* que significa adjetivo. Si se coloca *pos ='v'* significa verbo. Por defecto es *pos ='n'*, es decir sustantivo.\n",
    "\n",
    "A continuación se muestra la implementación de lematización de algunas palabras en inglés usando la librería *nltk*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/kainak0/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n",
      "better : better\n",
      "better : better\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "  \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"n\")) \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"v\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos lematizar el texto de ejemplo, primero con verbos y luego con sustantivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'interactions', 'computers', 'human', 'natural', 'languages', 'particular', 'concerned', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'frequently', 'formal', 'machine-readable', 'logical', 'forms', 'connecting', 'language', 'machine', 'perception', 'managing', 'human-computer', 'dialog', 'systems', 'combination', 'thereof', 'days', 'usually', 'year']\n",
      "\n",
      "\n",
      "['natural', 'language', 'process', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concern', 'interactions', 'computers', 'human', 'natural', 'languages', 'particular', 'concern', 'program', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenge', 'natural', 'language', 'process', 'frequently', 'involve', 'natural', 'language', 'understand', 'natural', 'language', 'generation', 'frequently', 'formal', 'machine-readable', 'logical', 'form', 'connect', 'language', 'machine', 'perception', 'manage', 'human-computer', 'dialog', 'systems', 'combination', 'thereof', 'days', 'usually', 'year']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#\n",
    "# verbs\n",
    "lemma_text =[]\n",
    "for token in tokens:\n",
    "    lemma_text.append(WordNetLemmatizer().lemmatize(token, pos='v'))\n",
    "   \n",
    "\n",
    "\n",
    "print(tokens)\n",
    "print('\\n')\n",
    "print(lemma_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'process', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concern', 'interaction', 'computer', 'human', 'natural', 'language', 'particular', 'concern', 'program', 'computer', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpus', 'challenge', 'natural', 'language', 'process', 'frequently', 'involve', 'natural', 'language', 'understand', 'natural', 'language', 'generation', 'frequently', 'formal', 'machine-readable', 'logical', 'form', 'connect', 'language', 'machine', 'perception', 'manage', 'human-computer', 'dialog', 'system', 'combination', 'thereof', 'day', 'usually', 'year']\n"
     ]
    }
   ],
   "source": [
    "# nouns\n",
    "for i in range(len(lemma_text )):\n",
    "    lemma_text[i] = WordNetLemmatizer().lemmatize(lemma_text[i], pos='n')\n",
    "print(lemma_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steeming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "La derivación (steeming) es el proceso de producir variantes morfológicas de una palabra raíz / base. Los programas de derivación se conocen comúnmente como algoritmos de steeming o derivaciones. Un algoritmo de stemming reduce las palabras como en los siguientes ejemplos\n",
    "\n",
    "+ \"chocolates\", \"chocolates\", \"choco\" a la raíz de la palabra, \"chocolate\"\n",
    "+ \"recuperación\", \"recuperado\", \"recupera\" se reduce a la raíz \"recuperar\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errores en la derivación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay principalmente dos errores en la derivación: la derivación excesiva y la derivación insuficiente. \n",
    "\n",
    "El sobre-recorte excesivo ocurre cuando dos palabras se derivan de la misma raíz que tienen raíces diferentes. \n",
    "\n",
    "El  sub-recorte ocurre cuando dos palabras se derivan de la misma raíz pero tienen raíces diferentes.\n",
    "\n",
    "Como ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'field', 'comput', 'scienc', 'artifici', 'intellig', 'comput', 'linguist', 'concern', 'interact', 'comput', 'human', 'natur', 'languag', 'particular', 'concern', 'program', 'comput', 'fruit', 'process', 'larg', 'natur', 'languag', 'corpu', 'challeng', 'natur', 'languag', 'process', 'frequent', 'involv', 'natur', 'languag', 'understand', 'natur', 'languag', 'gener', 'frequent', 'formal', 'machine-read', 'logic', 'form', 'connect', 'languag', 'machin', 'percept', 'manag', 'human-comput', 'dialog', 'system', 'combin', 'thereof', 'day', 'usual', 'year']\n"
     ]
    }
   ],
   "source": [
    "#from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer \n",
    "# crea una instancia de PorterStemmer \n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in range(len(lemma_text)):\n",
    "    lemma_text[i] = ps.stem(lemma_text[i])\n",
    "print(lemma_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Bag of Words</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/kainak0/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/kainak0/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/kainak0/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages (from scikit-learn) (1.5.0)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.1 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'broadcast_to' from 'numpy.lib.stride_tricks' (/home/kainak0/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/lib/stride_tricks.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Sample text documents\u001b[39;00m\n\u001b[32m      5\u001b[39m documents = [\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe cat sat on the mat\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe dog played with the ball\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe cat chased the dog\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe dog barked at the cat\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/sklearn/__init__.py:73\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     76\u001b[39m _submodules = [\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/sklearn/base.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/sklearn/utils/__init__.py:15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/sklearn/utils/_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/scipy/sparse/__init__.py:294\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# Original code by Travis Oliphant.\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m# Modified and extended by Ed Schofield, Robert Cimrman,\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# Nathan Bell, and Jake Vanderplas.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_warnings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/scipy/sparse/_base.py:5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VisibleDeprecationWarning\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[32m      8\u001b[39m                        get_sum_dtype, isdense, isscalarlike,\n\u001b[32m      9\u001b[39m                        matrix, validateaxis,)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/scipy/_lib/_util.py:18\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     Optional,\n\u001b[32m     12\u001b[39m     Union,\n\u001b[32m     13\u001b[39m     TYPE_CHECKING,\n\u001b[32m     14\u001b[39m     TypeVar,\n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_namespace\n\u001b[32m     21\u001b[39m AxisError: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mException\u001b[39;00m]\n\u001b[32m     22\u001b[39m ComplexWarning: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mWarning\u001b[39;00m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/scipy/_lib/_array_api.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     is_array_api_obj,\n\u001b[32m     19\u001b[39m     size,\n\u001b[32m     20\u001b[39m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[32m     21\u001b[39m )\n\u001b[32m     23\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33marray_namespace\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_asarray\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msize\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# To enable array API and strict array-like input validation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/scipy/_lib/array_api_compat/numpy/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# from numpy import * doesn't overwrite these builtin names\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m, \u001b[38;5;28mmax\u001b[39m, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mround\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/__init__.py:346\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    338\u001b[39m     public_symbols -= {\n\u001b[32m    339\u001b[39m         \"core\", \"matrixlib\",\n\u001b[32m    340\u001b[39m         # These were moved in 1.25 and may be deprecated eventually:\n\u001b[32m    341\u001b[39m         \"ModuleDeprecationWarning\", \"VisibleDeprecationWarning\",\n\u001b[32m    342\u001b[39m         \"ComplexWarning\", \"TooHardError\", \"AxisError\"\n\u001b[32m    343\u001b[39m     }\n\u001b[32m    344\u001b[39m     return list(public_symbols)\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m # Pytest testing\n\u001b[32m    347\u001b[39m from numpy._pytesttester import PytestTester\n\u001b[32m    348\u001b[39m test = PytestTester(__name__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/ma/__init__.py:42\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m=============\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mMasked Arrays\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m \n\u001b[32m     41\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extras\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/ma/core.py:37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ndarray, amax, amin, iscomplexobj, bool_, _NoValue\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array \u001b[38;5;28;01mas\u001b[39;00m narray\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunction_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m angle\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     39\u001b[39m     getargspec, formatargspec, long, unicode, \u001b[38;5;28mbytes\u001b[39m\n\u001b[32m     40\u001b[39m     )\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m expand_dims\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/lib/function_base.py:25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunction_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_newdoc\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtwodim_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m diag\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmultiarray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     27\u001b[39m     _place, add_docstring, bincount, normalize_axis_index, _monotonicity,\n\u001b[32m     28\u001b[39m     interp \u001b[38;5;28;01mas\u001b[39;00m compiled_interp, interp_complex \u001b[38;5;28;01mas\u001b[39;00m compiled_interp_complex\n\u001b[32m     29\u001b[39m     )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mumath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _add_newdoc_ufunc \u001b[38;5;28;01mas\u001b[39;00m add_newdoc_ufunc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/lib/twodim_base.py:15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iinfo\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstride_tricks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m broadcast_to\n\u001b[32m     18\u001b[39m __all__ = [\n\u001b[32m     19\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdiag\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdiagflat\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33meye\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfliplr\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mflipud\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtri\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtriu\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     20\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtril\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mvander\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhistogram2d\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmask_indices\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtril_indices\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     21\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtril_indices_from\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtriu_indices\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtriu_indices_from\u001b[39m\u001b[33m'\u001b[39m, ]\n\u001b[32m     24\u001b[39m array_function_dispatch = functools.partial(\n\u001b[32m     25\u001b[39m     overrides.array_function_dispatch, module=\u001b[33m'\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'broadcast_to' from 'numpy.lib.stride_tricks' (/home/kainak0/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/lib/stride_tricks.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog played with the ball\",\n",
    "    \"The cat chased the dog\",\n",
    "    \"The dog barked at the cat\"\n",
    "]\n",
    "\n",
    "# Step 1: Create CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Step 2: Fit and transform the documents into BoW vectors\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Step 3: Get feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display results\n",
    "print(\"Vocabulary (Feature Names):\")\n",
    "print(feature_names)\n",
    "print(\"\\nBag of Words Matrix:\")\n",
    "print(bow_matrix.toarray())\n",
    "\n",
    "# Convert to dense array for better visualization\n",
    "dense_matrix = bow_matrix.toarray()\n",
    "\n",
    "print(\"\\nDocument-Term Matrix:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1}: {doc}\")\n",
    "    print(\"Vector:\", dense_matrix[i])\n",
    "    print()\n",
    "\n",
    "# Example: Transform new text using the same vectorizer\n",
    "new_text = [\"The cat played with the dog\"]\n",
    "new_text_vector = vectorizer.transform(new_text)\n",
    "print(\"New text vector:\", new_text_vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector: [0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 2, 0]\n",
    "    'at': 0 (does not appear)\n",
    "    'ball': 0 (does not appear)\n",
    "    'barked': 0 (does not appear)\n",
    "    'cat': 1 (appears once)\n",
    "    'chased': 0 (does not appear)\n",
    "    'dog': 0 (does not appear)\n",
    "    'mat': 1 (appears once)\n",
    "    'on': 1 (appears once)\n",
    "    'played': 0 (does not appear)\n",
    "    'sat': 1 (appears once)\n",
    "    'the': 2 (appears twice)\n",
    "    'with': 0 (does not appear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">TF-IDF</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomado de [Wikipedia](https://es.wikipedia.org/wiki/Tf-idf).\n",
    "\n",
    "Tf-idf (del inglés Term frequency – Inverse document frequency), frecuencia de término – frecuencia inversa de documento (o sea, la frecuencia de ocurrencia del término en el corpus de documentos), es una medida numérica que expresa cuán relevante es una palabra para un documento en un corpus. Esta medida se utiliza a menudo como un factor de ponderación en la recuperación de información y la minería de textos. \n",
    "\n",
    "\n",
    "El valor tf-idf aumenta proporcionalmente al número de veces que una palabra aparece en el documento, pero es compensada por la frecuencia de la palabra en el corpus de documentos, lo que permite manejar el hecho de que algunas palabras son generalmente más comunes que otras.\n",
    "\n",
    "Variaciones del esquema de peso tf-idf son empleadas frecuentemente por los motores de búsqueda como herramienta fundamental para medir la relevancia de un documento dada una consulta del usuario, estableciendo así una ordenación o ranking de los mismos. \n",
    "\n",
    "\n",
    "Tf-idf puede utilizarse exitosamente para el filtrado de las palabras vacías (stop-words), en diferentes campos del pre-procesamiento de textos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detalles matemáticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf es el producto de dos medidas, *frecuencia de término* y *frecuencia inversa de documento*. Existen varias maneras de determinar el valor de ambas. \n",
    "\n",
    "En el caso de la frecuencia de término $\\text{tf}(t, d)$, la opción más sencilla es usar la frecuencia bruta del término $t$ en el documento $d$, o sea, el número de veces que el término $t$ ocurre en el documento $d$. Si denotamos la frecuencia bruta de $t$ por $f(t,d)$, entonces el esquema $\\text{tf}$ simple es $\\text{tf}(t, d) = f(t,d)$. \n",
    "\n",
    "\n",
    "Otras posibilidades son:\n",
    "\n",
    "- *frecuencias\" booleanas*: tf(t,d) = 1 si t ocurre en d, y 0 si no;\n",
    "- *frecuencia escalada logarítmicamente*: tf(t,d) = 1 + log f(t,d) (y 0 si f(t,d)=0);\n",
    "- *frecuencia normalizada*, para evitar una predisposición hacia los documentos largos. Por ejemplo, se divide la frecuencia bruta por la frecuencia máxima de algún término en el documento:\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\mathrm {tf} (t,d)={\\frac {\\mathrm {f} (t,d)}{\\max\\{\\mathrm {f} (t,d):t\\in d\\}}}}\n",
    "$$\n",
    "\n",
    "La frecuencia inversa de documento es una medida de si el término es común o no, en el corpus de documentos. Se obtiene dividiendo el número total de documentos por el número de documentos que contienen el término, y se toma el logaritmo de ese cociente:\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\mathrm {idf} (t,D)=\\log {\\frac {|D|}{|\\{d\\in D:t\\in d\\}|}}}\n",
    "$$\n",
    "\n",
    "donde\n",
    "\n",
    "- ${\\displaystyle |D|}$: cardinalidad de $D$, o número de documentos en el corpus.\n",
    "- ${\\displaystyle |\\{d\\in D:t\\in d\\}|}$ : número de documentos donde aparece el término $t$. Si el término no está en la colección se producirá una división-por-cero. Por lo tanto, es común ajustar esta fórmula a ${\\displaystyle 1+|\\{d\\in D:t\\in d\\}|}$.\n",
    "\n",
    "Matemáticamente, la base de la función logaritmo no es importante y constituye un factor constante en el resultado final.\n",
    "\n",
    "Luego, *tf-idf* se calcula como:\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\text{tf-idf} (t,d,D)=\\mathrm {tf} (t,d)\\times \\mathrm {idf} (t,D)}\n",
    "$$\n",
    "\n",
    "Un peso alto en *tf-idf* se alcanza con una elevada frecuencia de término (en el documento dado) y una pequeña frecuencia de ocurrencia del término en corpus de documentos. \n",
    "\n",
    "Como el cociente dentro de la función logaritmo del idf es siempre mayor o igual que 1, el valor del *idf* (y del *tf-idf*) es mayor o igual que 0. \n",
    "\n",
    "Cuando un término aparece en muchos documentos, el cociente dentro del logaritmo se acerca a 1, ofreciendo un valor de *idf* y de *tf-idf* cercano a 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Semántica latente</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta técnica es quizas de las primeras aparecidas en el anális de textos. La idea central es la construcción de análisis de componentes principales (ACP) seguida de un proceso de clasificación automática.\n",
    "\n",
    "Las componentes principales del ACP, que son construidas a partir de combinaciones lineales de las columnas de los términos se denominan las **componentes léxicas del corpus de datos**. \n",
    "\n",
    "Las herramientas habituales para la interpretación del ACP permiten determinar o mejor asignar un contenido semántico a cada componente. \n",
    "\n",
    "En consecuencia, es posible determinar las temáticas presentes en el corpus de textos, a partir de los ejes semánticos.\n",
    "\n",
    "\n",
    "Como es habitual en el ACP, una clasificación automática puede ser obtenida a partir de la representación factorial de la dtm.\n",
    "\n",
    "El siguente gráfico ilustra la técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/pca.png\" width=\"500\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Arquitectura del modelo Semática Latente</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: \n",
    "Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Básicamente lo que se hace es una proyección lineal desde el espacio vectores dispersos al espacio Euclideano. Modernamente se ha encontrado que tiene bastante error, básicamente por el tratamiento lineal. \n",
    "\n",
    "En general, se ha encontrado que estas técnicas permiten un primer acercamiento al descubrimieontos de las tématicas (tópicos), pero que en general se quedan cortas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Modelos generativos: Latent Dirichlet Allocation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La técnica Latent Dirichlet Allocation (LDA) es la más utilizada actualmente para la extracción de toṕicos de corpus de documentos y se debe a [Blei et al](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Las ideas centrales detrás de LDA, Blei et al.(2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las ideas centrales detrás de LDA son las siguientes. El modelo generativo supone que los documentos son gnerados como sigue:\n",
    "\n",
    "1. El tamaño $N$ del documento es generado por una distribución de Poisson $\\text{Poi}(\\xi)$.\n",
    "2. Los tópicos son generados a partir de una distribución multinomial con vector de probabilidades $\\mathbf{\\theta}$. \n",
    "3. A priori se asume que que el vector $\\mathbf{\\theta}$ es generado por una distribución de Dirichlet con vector de parámetros $\\boldsymbol{\\alpha}$. De aquí deriva el nombre de la técnica.\n",
    "4. Cada una de las $N$ palabras en un documentos es generada según el siguiente algoritmo.\n",
    "     - Se escoge un tópico $z_n \\sim \\text{Multinomial}(\\mathbf{\\theta})$.\n",
    "     - Se escoge la palabra $w_n \\sim \\text{P}(w_n|z_n,\\mathbf{\\beta})$. En donde $\\mathbf{\\beta}$ es una matriz de probabilidades de pertenencia de las palabras a los tópicos. $P$ es una probabilidad multinomial condicionada al tópico $z_n$ y al vector de parámetros $\\mathbf{\\beta}$.\n",
    "\n",
    "\n",
    "Al lector interesado en los detalles, lo remitimos al paper original de [Blei et al.](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente imagen intenta mostrar las ideas centrales detras  de la técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Diagram_Blei.png\" width=\"800\" height=\"700\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Intuición detrás de LDA</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: \n",
    "[Intuition behind LDA](http://www.cs.cornell.edu/courses/cs6784/2010sp/lecture/30-BleiEtAl03.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelado de temas (topic modeling) es un tipo de modelado estadístico para descubrir los “temas” abstractos que ocurren en una colección de documentos. La asignación de Dirichlet latente (LDA) es un ejemplo de modelo de tema y se utiliza para clasificar el texto de un documento en un tema en particular. \n",
    "\n",
    "Construye un modelo de tema por documento y palabras por modelo de tema, modelado como distribuciones de Dirichlet.\n",
    "\n",
    "Aquí vamos a aplicar LDA a un conjunto de documentos y dividirlos en temas. ¡Empecemos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'broadcast_to' from 'numpy.lib.stride_tricks' (/home/kainak0/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/lib/stride_tricks.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m simple_preprocess\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#from gensim.parsing.preprocessing import STOPWORDS\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/gensim/__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m4.3.3\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m     14\u001b[39m logger = logging.getLogger(\u001b[33m'\u001b[39m\u001b[33mgensim\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger.handlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/gensim/parsing/__init__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"This package contains functions to preprocess raw text\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mporter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      5\u001b[39m     preprocess_documents,\n\u001b[32m      6\u001b[39m     preprocess_string,\n\u001b[32m      7\u001b[39m     read_file,\n\u001b[32m      8\u001b[39m     read_files,\n\u001b[32m      9\u001b[39m     remove_stopwords,\n\u001b[32m     10\u001b[39m     split_alphanum,\n\u001b[32m     11\u001b[39m     stem_text,\n\u001b[32m     12\u001b[39m     strip_multiple_whitespaces,\n\u001b[32m     13\u001b[39m     strip_non_alphanum,\n\u001b[32m     14\u001b[39m     strip_numeric,\n\u001b[32m     15\u001b[39m     strip_punctuation,\n\u001b[32m     16\u001b[39m     strip_short,\n\u001b[32m     17\u001b[39m     strip_tags,\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/gensim/parsing/preprocessing.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstring\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparsing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mporter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[32m     30\u001b[39m STOPWORDS = \u001b[38;5;28mfrozenset\u001b[39m([\n\u001b[32m     31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mall\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msix\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mjust\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mless\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbeing\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mindeed\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mover\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmove\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33manyway\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfour\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnot\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mown\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mthrough\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     32\u001b[39m     \u001b[33m'\u001b[39m\u001b[33musing\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfifty\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwhere\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmill\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33monly\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfind\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbefore\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mone\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwhose\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhow\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msomewhere\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmake\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33monce\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     59\u001b[39m ])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/gensim/utils.py:35\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msmart_open\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mopen\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m gensim_version\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/scipy/sparse/__init__.py:294\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# Original code by Travis Oliphant.\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m# Modified and extended by Ed Schofield, Robert Cimrman,\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# Nathan Bell, and Jake Vanderplas.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_warnings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/scipy/sparse/_base.py:5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VisibleDeprecationWarning\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[32m      8\u001b[39m                        get_sum_dtype, isdense, isscalarlike,\n\u001b[32m      9\u001b[39m                        matrix, validateaxis,)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/scipy/_lib/_util.py:18\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     Optional,\n\u001b[32m     12\u001b[39m     Union,\n\u001b[32m     13\u001b[39m     TYPE_CHECKING,\n\u001b[32m     14\u001b[39m     TypeVar,\n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_namespace\n\u001b[32m     21\u001b[39m AxisError: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mException\u001b[39;00m]\n\u001b[32m     22\u001b[39m ComplexWarning: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mWarning\u001b[39;00m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/scipy/_lib/_array_api.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     is_array_api_obj,\n\u001b[32m     19\u001b[39m     size,\n\u001b[32m     20\u001b[39m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[32m     21\u001b[39m )\n\u001b[32m     23\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33marray_namespace\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_asarray\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msize\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# To enable array API and strict array-like input validation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/scipy/_lib/array_api_compat/numpy/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# from numpy import * doesn't overwrite these builtin names\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m, \u001b[38;5;28mmax\u001b[39m, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mround\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/__init__.py:346\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    338\u001b[39m     public_symbols -= {\n\u001b[32m    339\u001b[39m         \"core\", \"matrixlib\",\n\u001b[32m    340\u001b[39m         # These were moved in 1.25 and may be deprecated eventually:\n\u001b[32m    341\u001b[39m         \"ModuleDeprecationWarning\", \"VisibleDeprecationWarning\",\n\u001b[32m    342\u001b[39m         \"ComplexWarning\", \"TooHardError\", \"AxisError\"\n\u001b[32m    343\u001b[39m     }\n\u001b[32m    344\u001b[39m     return list(public_symbols)\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m # Pytest testing\n\u001b[32m    347\u001b[39m from numpy._pytesttester import PytestTester\n\u001b[32m    348\u001b[39m test = PytestTester(__name__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/ma/__init__.py:42\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m=============\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mMasked Arrays\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m \n\u001b[32m     41\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extras\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/ma/core.py:37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ndarray, amax, amin, iscomplexobj, bool_, _NoValue\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array \u001b[38;5;28;01mas\u001b[39;00m narray\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunction_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m angle\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     39\u001b[39m     getargspec, formatargspec, long, unicode, \u001b[38;5;28mbytes\u001b[39m\n\u001b[32m     40\u001b[39m     )\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m expand_dims\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/lib/function_base.py:25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunction_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_newdoc\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtwodim_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m diag\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmultiarray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     27\u001b[39m     _place, add_docstring, bincount, normalize_axis_index, _monotonicity,\n\u001b[32m     28\u001b[39m     interp \u001b[38;5;28;01mas\u001b[39;00m compiled_interp, interp_complex \u001b[38;5;28;01mas\u001b[39;00m compiled_interp_complex\n\u001b[32m     29\u001b[39m     )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mumath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _add_newdoc_ufunc \u001b[38;5;28;01mas\u001b[39;00m add_newdoc_ufunc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/lib/twodim_base.py:15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iinfo\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstride_tricks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m broadcast_to\n\u001b[32m     18\u001b[39m __all__ = [\n\u001b[32m     19\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdiag\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdiagflat\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33meye\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfliplr\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mflipud\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtri\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtriu\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     20\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtril\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mvander\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhistogram2d\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmask_indices\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtril_indices\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     21\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtril_indices_from\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtriu_indices\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtriu_indices_from\u001b[39m\u001b[33m'\u001b[39m, ]\n\u001b[32m     24\u001b[39m array_function_dispatch = functools.partial(\n\u001b[32m     25\u001b[39m     overrides.array_function_dispatch, module=\u001b[33m'\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'broadcast_to' from 'numpy.lib.stride_tricks' (/home/kainak0/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/lib/stride_tricks.py)"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "#from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a escribir una función que lematiza y hace el preprocesamintos del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    return ps.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text): #  gensim.utils.simple_preprocess tokeniza el texto\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Ejemplo: Un millón de titulares</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos que usaremos es una lista de más de un millón de titulares de noticias publicados durante un período de 15 años y se puede descargar de [Kaggle](https://www.kaggle.com/therohk/million-headlines/metadata)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo adaptado de [Topic Modeling and Latent Dirichlet Allocation (LDA) in Python](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m data = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33m../Datos/abcnews-date-text.csv\u001b[39m\u001b[33m'\u001b[39m, on_bad_lines=\u001b[33m'\u001b[39m\u001b[33mwarn\u001b[39m\u001b[33m'\u001b[39m);\n\u001b[32m      3\u001b[39m data_text = data[[\u001b[33m'\u001b[39m\u001b[33mheadline_text\u001b[39m\u001b[33m'\u001b[39m]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/pandas/__init__.py:37\u001b[39m\n\u001b[32m     30\u001b[39m     _module = _err.name\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     32\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC extension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not built. If you want to import \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpandas from the source directory, you may need to run \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     34\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mpython setup.py build_ext\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to build the C extensions first.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_err\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     38\u001b[39m     get_option,\n\u001b[32m     39\u001b[39m     set_option,\n\u001b[32m     40\u001b[39m     reset_option,\n\u001b[32m     41\u001b[39m     describe_option,\n\u001b[32m     42\u001b[39m     option_context,\n\u001b[32m     43\u001b[39m     options,\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/pandas/_config/__init__.py:20\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mpandas._config is considered explicitly upstream of everything else in pandas,\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mshould have no intra-pandas dependencies.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \u001b[33;03mare initialized.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m __all__ = [\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdetect_console_encoding\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwarn_copy_on_write\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dates  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     _global_config,\n\u001b[32m     24\u001b[39m     describe_option,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     set_option,\n\u001b[32m     30\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/pandas/_config/config.py:68\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     59\u001b[39m     TYPE_CHECKING,\n\u001b[32m     60\u001b[39m     Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     cast,\n\u001b[32m     65\u001b[39m )\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     69\u001b[39m     F,\n\u001b[32m     70\u001b[39m     T,\n\u001b[32m     71\u001b[39m )\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/pandas/_typing.py:198\u001b[39m\n\u001b[32m    192\u001b[39m Frequency = Union[\u001b[38;5;28mstr\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBaseOffset\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    193\u001b[39m Axes = ListLike\n\u001b[32m    195\u001b[39m RandomState = Union[\n\u001b[32m    196\u001b[39m     \u001b[38;5;28mint\u001b[39m,\n\u001b[32m    197\u001b[39m     np.ndarray,\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m.Generator,\n\u001b[32m    199\u001b[39m     np.random.BitGenerator,\n\u001b[32m    200\u001b[39m     np.random.RandomState,\n\u001b[32m    201\u001b[39m ]\n\u001b[32m    203\u001b[39m \u001b[38;5;66;03m# dtypes\u001b[39;00m\n\u001b[32m    204\u001b[39m NpDtype = Union[\u001b[38;5;28mstr\u001b[39m, np.dtype, type_t[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mobject\u001b[39m]]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/__init__.py:340\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__dir__\u001b[39m():\n\u001b[32m    337\u001b[39m     public_symbols = \u001b[38;5;28mglobals\u001b[39m().keys() | {\u001b[33m'\u001b[39m\u001b[33mtesting\u001b[39m\u001b[33m'\u001b[39m}\n\u001b[32m    338\u001b[39m     public_symbols -= {\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcore\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmatrixlib\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m         \u001b[38;5;66;03m# These were moved in 1.25 and may be deprecated eventually:\u001b[39;00m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mModuleDeprecationWarning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mVisibleDeprecationWarning\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    342\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mComplexWarning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTooHardError\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAxisError\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    343\u001b[39m     }\n\u001b[32m    344\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(public_symbols)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/random/__init__.py:180\u001b[39m\n\u001b[32m    126\u001b[39m __all__ = [\n\u001b[32m    127\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbeta\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    128\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbinomial\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    176\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mzipf\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    177\u001b[39m ]\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# add these for module-freeze analysis (like PyInstaller)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pickle\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _common\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _bounded_integers\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitProjects/mia/mia_302_pnl/.venv/lib/python3.12/site-packages/numpy/random/_pickle.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmtrand\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomState\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_philox\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Philox\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pcg64\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PCG64, PCG64DXSM\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:1\u001b[39m, in \u001b[36minit numpy.random.mtrand\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../Datos/abcnews-date-text.csv', on_bad_lines='warn');\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos algunos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mdocuments\u001b[49m))\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(documents[:\u001b[32m5\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m doc_sample = \u001b[43mdocuments\u001b[49m[documents[\u001b[33m'\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m'\u001b[39m] == \u001b[32m4310\u001b[39m].values[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mdocumento original: \u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m words = []\n",
      "\u001b[31mNameError\u001b[39m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "print('documento original: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n documento tokenizado y lematizado: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Vamos a procesar previamente los textos, guardando los resultados en el objeto *processed_docs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [decid, commun, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción del diccionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un diccionario a partir de *processed_docs* que contenga la cantidad de veces que aparece una palabra en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 commun\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "#.iteritems() method allows you to iterate over the dictionary, \n",
    "# where 'k' represents the word (key), and 'v' represents the corresponding integer ID (value).\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtra los tokens que aparecen en\n",
    "menos de 15 documentos (número absoluto) o\n",
    "más de 0,5 documentos (fracción del tamaño total del corpus, no número absoluto).\n",
    "después de los dos pasos anteriores, conserve solo los primeros 100000 tokens más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim doc2bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada documento creamos un diccionario que informa cuántos\n",
    "palabras y cuántas veces aparecen esas palabras. \n",
    "\n",
    "Colocamos esto en el objeto *bow_corpus*, luego verifique nuestro documento seleccionado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(162, 1), (240, 1), (292, 1), (589, 1), (839, 1), (3579, 1), (3580, 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esta es una vista preliminar de la bolsa de palabras del documento preprocesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 162 (\"govt\") appears 1 time.\n",
      "Word 240 (\"group\") appears 1 time.\n",
      "Word 292 (\"vote\") appears 1 time.\n",
      "Word 589 (\"local\") appears 1 time.\n",
      "Word 839 (\"want\") appears 1 time.\n",
      "Word 3579 (\"compulsori\") appears 1 time.\n",
      "Word 3580 (\"ratepay\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un objeto modelo *tf-idf* usando *models.TfidfModel* a partir de  \"bow_corpus\" y lo colocamos en *tfidf*, luego aplicamos la transformación a todo el corpus y lo llámamos *corpus_tfidf*. Finalmente, obtenemos una vista previa de las puntuaciones *TF-IDF* para nuestro primer documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5854395661274623),\n",
      " (1, 0.383252758688686),\n",
      " (2, 0.50230806644029),\n",
      " (3, 0.5080004367704987)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corriendo LDA usando la bolsa de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a entrener anuestro modelo LDA usando *gensim.models.LdaMulticore* and save it to ‘lda_model’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    lda_model: This is a variable that will store the trained LDA model.\n",
    "\n",
    "    gensim.models.LdaMulticore: This part of the code creates an LDA model using Gensim. LdaMulticore is a variant of LDA that utilizes multiple CPU cores to speed up training. It's suitable for large corpora or when you want to parallelize the computation.\n",
    "\n",
    "    bow_corpus: This is the Bag of Words representation of your documents. It's a corpus where each document is represented as a list of word IDs with their corresponding word counts (BoW format).\n",
    "\n",
    "    num_topics=10: This parameter specifies the number of topics the model should identify in the corpus. In this case, the LDA model will try to find 10 topics within the documents.\n",
    "\n",
    "    id2word=dictionary: This parameter links the LDA model to the Gensim dictionary created earlier. The dictionary maps words to unique integer IDs, and the LDA model uses this mapping to understand the words in the BoW representation.\n",
    "\n",
    "    passes=2: This parameter specifies the number of passes (iterations) over the entire corpus. In this case, the LDA model will make two passes over the corpus during training.\n",
    "\n",
    "    workers=2: This parameter indicates the number of CPU cores to use during training. In this example, it's set to 2, meaning that the training process will utilize two CPU cores if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Para cada tópico, exploraremos las palabras que ocurren en ese tema y su peso relativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    lda_model.print_topics(-1): The print_topics() method of the LDA model is used to retrieve a list of topics and their associated words. The -1 argument indicates that you want to retrieve information about all topics. Each topic is represented as a list of words with their associated probabilities.\n",
    "\n",
    "    idx and topic: In the loop, idx represents the topic index, and topic represents the list of words associated with that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.027*\"news\" + 0.023*\"market\" + 0.018*\"hospit\" + 0.017*\"morrison\" + 0.016*\"work\" + 0.015*\"fight\" + 0.013*\"countri\" + 0.013*\"close\" + 0.012*\"fall\" + 0.012*\"darwin\"\n",
      "Topic: 1 \n",
      "Words: 0.051*\"polic\" + 0.035*\"case\" + 0.025*\"charg\" + 0.025*\"court\" + 0.021*\"death\" + 0.020*\"murder\" + 0.018*\"face\" + 0.016*\"restrict\" + 0.014*\"trial\" + 0.013*\"investig\"\n",
      "Topic: 2 \n",
      "Words: 0.031*\"queensland\" + 0.027*\"live\" + 0.021*\"women\" + 0.018*\"victorian\" + 0.014*\"andrew\" + 0.012*\"need\" + 0.012*\"deal\" + 0.011*\"budget\" + 0.011*\"rural\" + 0.011*\"road\"\n",
      "Topic: 3 \n",
      "Words: 0.057*\"covid\" + 0.046*\"trump\" + 0.029*\"vaccin\" + 0.021*\"bushfir\" + 0.019*\"test\" + 0.015*\"australia\" + 0.013*\"say\" + 0.013*\"home\" + 0.012*\"support\" + 0.011*\"hotel\"\n",
      "Topic: 4 \n",
      "Words: 0.030*\"victoria\" + 0.019*\"world\" + 0.017*\"coast\" + 0.016*\"south\" + 0.016*\"australia\" + 0.015*\"peopl\" + 0.015*\"sydney\" + 0.015*\"canberra\" + 0.013*\"alleg\" + 0.013*\"busi\"\n",
      "Topic: 5 \n",
      "Words: 0.032*\"elect\" + 0.020*\"say\" + 0.017*\"minist\" + 0.015*\"miss\" + 0.014*\"speak\" + 0.013*\"labor\" + 0.013*\"care\" + 0.013*\"call\" + 0.011*\"claim\" + 0.011*\"age\"\n",
      "Topic: 6 \n",
      "Words: 0.075*\"coronaviru\" + 0.027*\"china\" + 0.021*\"kill\" + 0.018*\"die\" + 0.017*\"covid\" + 0.015*\"australia\" + 0.014*\"return\" + 0.013*\"jail\" + 0.013*\"crash\" + 0.013*\"attack\"\n",
      "Topic: 7 \n",
      "Words: 0.023*\"donald\" + 0.020*\"year\" + 0.018*\"chang\" + 0.015*\"lockdown\" + 0.013*\"school\" + 0.013*\"warn\" + 0.012*\"high\" + 0.012*\"travel\" + 0.011*\"tasmanian\" + 0.011*\"price\"\n",
      "Topic: 8 \n",
      "Words: 0.033*\"govern\" + 0.018*\"border\" + 0.015*\"plan\" + 0.015*\"commun\" + 0.014*\"indigen\" + 0.014*\"water\" + 0.013*\"region\" + 0.013*\"concern\" + 0.011*\"industri\" + 0.011*\"local\"\n",
      "Topic: 9 \n",
      "Words: 0.026*\"australia\" + 0.023*\"open\" + 0.016*\"final\" + 0.015*\"australian\" + 0.015*\"time\" + 0.012*\"lose\" + 0.012*\"tell\" + 0.011*\"releas\" + 0.010*\"pandem\" + 0.010*\"win\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Es posible distinguir diferentes temas usando las palabras en cada tema y sus pesos correspondientes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corriendo  LDA usando TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.015*\"govern\" + 0.013*\"elect\" + 0.010*\"morrison\" + 0.008*\"live\" + 0.007*\"feder\" + 0.007*\"age\" + 0.007*\"care\" + 0.007*\"budget\" + 0.007*\"labor\" + 0.006*\"say\"\n",
      "Topic: 1 Word: 0.014*\"interview\" + 0.010*\"tuesday\" + 0.009*\"mark\" + 0.009*\"david\" + 0.008*\"daniel\" + 0.007*\"know\" + 0.007*\"extend\" + 0.006*\"septemb\" + 0.006*\"appeal\" + 0.006*\"court\"\n",
      "Topic: 2 Word: 0.020*\"news\" + 0.017*\"market\" + 0.012*\"rural\" + 0.010*\"price\" + 0.009*\"coronaviru\" + 0.008*\"share\" + 0.008*\"scott\" + 0.008*\"stori\" + 0.008*\"busi\" + 0.008*\"covid\"\n",
      "Topic: 3 Word: 0.009*\"queensland\" + 0.008*\"drought\" + 0.006*\"mental\" + 0.006*\"august\" + 0.006*\"victoria\" + 0.006*\"univers\" + 0.006*\"onlin\" + 0.006*\"coal\" + 0.005*\"liber\" + 0.005*\"health\"\n",
      "Topic: 4 Word: 0.011*\"coronaviru\" + 0.011*\"countri\" + 0.010*\"covid\" + 0.010*\"vaccin\" + 0.010*\"restrict\" + 0.008*\"hour\" + 0.008*\"bushfir\" + 0.007*\"climat\" + 0.007*\"chang\" + 0.006*\"thursday\"\n",
      "Topic: 5 Word: 0.011*\"drum\" + 0.010*\"lockdown\" + 0.010*\"australia\" + 0.009*\"world\" + 0.009*\"final\" + 0.007*\"leagu\" + 0.007*\"south\" + 0.007*\"michael\" + 0.007*\"wednesday\" + 0.006*\"open\"\n",
      "Topic: 6 Word: 0.011*\"royal\" + 0.010*\"andrew\" + 0.009*\"kill\" + 0.009*\"commiss\" + 0.006*\"indonesia\" + 0.005*\"china\" + 0.005*\"attack\" + 0.005*\"jam\" + 0.005*\"australian\" + 0.005*\"protest\"\n",
      "Topic: 7 Word: 0.019*\"polic\" + 0.015*\"charg\" + 0.014*\"crash\" + 0.014*\"woman\" + 0.011*\"murder\" + 0.009*\"shoot\" + 0.009*\"death\" + 0.008*\"assault\" + 0.008*\"arrest\" + 0.007*\"miss\"\n",
      "Topic: 8 Word: 0.028*\"trump\" + 0.017*\"donald\" + 0.012*\"coronaviru\" + 0.009*\"covid\" + 0.008*\"weather\" + 0.008*\"friday\" + 0.008*\"quarantin\" + 0.008*\"victorian\" + 0.007*\"sport\" + 0.007*\"zealand\"\n",
      "Topic: 9 Word: 0.013*\"border\" + 0.012*\"coronaviru\" + 0.010*\"updat\" + 0.008*\"covid\" + 0.007*\"hill\" + 0.007*\"social\" + 0.006*\"grandstand\" + 0.006*\"footag\" + 0.006*\"june\" + 0.006*\"reopen\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, ¿podemos distinguir diferentes temas usando las palabras en cada tema y sus pesos correspondientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del desempeño clasificando el documento de muestra usando el modelo LDA de bolsa de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobaremos dónde se clasificaría nuestro documento de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "\n",
    "    lda_model[bow_corpus[4310]]: This part of the code applies the LDA model to a specific document represented as a Bag of Words (BoW) vector. bow_corpus[4310] refers to the BoW representation of the 4310th document in your corpus.\n",
    "\n",
    "    sorted(..., key=lambda tup: -1*tup[1]): The sorted() function is used to sort the topics by their scores in descending order. The key parameter is set to a lambda function that takes a tuple tup and sorts based on the negative of the second element (tup[1]). This results in sorting topics by their scores in descending order.\n",
    "\n",
    "    for index, score in ...: This loop iterates through the sorted topics and their associated scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8874766230583191\t \n",
      "Topic: 0.033*\"govern\" + 0.018*\"border\" + 0.015*\"plan\" + 0.015*\"commun\" + 0.014*\"indigen\" + 0.014*\"water\" + 0.013*\"region\" + 0.013*\"concern\" + 0.011*\"industri\" + 0.011*\"local\"\n",
      "\n",
      "Score: 0.01250580232590437\t \n",
      "Topic: 0.032*\"elect\" + 0.020*\"say\" + 0.017*\"minist\" + 0.015*\"miss\" + 0.014*\"speak\" + 0.013*\"labor\" + 0.013*\"care\" + 0.013*\"call\" + 0.011*\"claim\" + 0.011*\"age\"\n",
      "\n",
      "Score: 0.012502484954893589\t \n",
      "Topic: 0.023*\"donald\" + 0.020*\"year\" + 0.018*\"chang\" + 0.015*\"lockdown\" + 0.013*\"school\" + 0.013*\"warn\" + 0.012*\"high\" + 0.012*\"travel\" + 0.011*\"tasmanian\" + 0.011*\"price\"\n",
      "\n",
      "Score: 0.01250243280082941\t \n",
      "Topic: 0.057*\"covid\" + 0.046*\"trump\" + 0.029*\"vaccin\" + 0.021*\"bushfir\" + 0.019*\"test\" + 0.015*\"australia\" + 0.013*\"say\" + 0.013*\"home\" + 0.012*\"support\" + 0.011*\"hotel\"\n",
      "\n",
      "Score: 0.012502348981797695\t \n",
      "Topic: 0.031*\"queensland\" + 0.027*\"live\" + 0.021*\"women\" + 0.018*\"victorian\" + 0.014*\"andrew\" + 0.012*\"need\" + 0.012*\"deal\" + 0.011*\"budget\" + 0.011*\"rural\" + 0.011*\"road\"\n",
      "\n",
      "Score: 0.01250219251960516\t \n",
      "Topic: 0.051*\"polic\" + 0.035*\"case\" + 0.025*\"charg\" + 0.025*\"court\" + 0.021*\"death\" + 0.020*\"murder\" + 0.018*\"face\" + 0.016*\"restrict\" + 0.014*\"trial\" + 0.013*\"investig\"\n",
      "\n",
      "Score: 0.012502162717282772\t \n",
      "Topic: 0.027*\"news\" + 0.023*\"market\" + 0.018*\"hospit\" + 0.017*\"morrison\" + 0.016*\"work\" + 0.015*\"fight\" + 0.013*\"countri\" + 0.013*\"close\" + 0.012*\"fall\" + 0.012*\"darwin\"\n",
      "\n",
      "Score: 0.012501981109380722\t \n",
      "Topic: 0.030*\"victoria\" + 0.019*\"world\" + 0.017*\"coast\" + 0.016*\"south\" + 0.016*\"australia\" + 0.015*\"peopl\" + 0.015*\"sydney\" + 0.015*\"canberra\" + 0.013*\"alleg\" + 0.013*\"busi\"\n",
      "\n",
      "Score: 0.012501980178058147\t \n",
      "Topic: 0.075*\"coronaviru\" + 0.027*\"china\" + 0.021*\"kill\" + 0.018*\"die\" + 0.017*\"covid\" + 0.015*\"australia\" + 0.014*\"return\" + 0.013*\"jail\" + 0.013*\"crash\" + 0.013*\"attack\"\n",
      "\n",
      "Score: 0.012501980178058147\t \n",
      "Topic: 0.026*\"australia\" + 0.023*\"open\" + 0.016*\"final\" + 0.015*\"australian\" + 0.015*\"time\" + 0.012*\"lose\" + 0.012*\"tell\" + 0.011*\"releas\" + 0.010*\"pandem\" + 0.010*\"win\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro documento de prueba tiene la mayor probabilidad de ser parte del tema que asignó nuestro modelo, que es la clasificación precisa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del desempeño clasificando el documento de muestra usando el modelo LDA TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7453249096870422\t \n",
      "Topic: 0.011*\"coronaviru\" + 0.011*\"countri\" + 0.010*\"covid\" + 0.010*\"vaccin\" + 0.010*\"restrict\" + 0.008*\"hour\" + 0.008*\"bushfir\" + 0.007*\"climat\" + 0.007*\"chang\" + 0.006*\"thursday\"\n",
      "\n",
      "Score: 0.15462540090084076\t \n",
      "Topic: 0.014*\"interview\" + 0.010*\"tuesday\" + 0.009*\"mark\" + 0.009*\"david\" + 0.008*\"daniel\" + 0.007*\"know\" + 0.007*\"extend\" + 0.006*\"septemb\" + 0.006*\"appeal\" + 0.006*\"court\"\n",
      "\n",
      "Score: 0.01250949315726757\t \n",
      "Topic: 0.015*\"govern\" + 0.013*\"elect\" + 0.010*\"morrison\" + 0.008*\"live\" + 0.007*\"feder\" + 0.007*\"age\" + 0.007*\"care\" + 0.007*\"budget\" + 0.007*\"labor\" + 0.006*\"say\"\n",
      "\n",
      "Score: 0.012507026083767414\t \n",
      "Topic: 0.020*\"news\" + 0.017*\"market\" + 0.012*\"rural\" + 0.010*\"price\" + 0.009*\"coronaviru\" + 0.008*\"share\" + 0.008*\"scott\" + 0.008*\"stori\" + 0.008*\"busi\" + 0.008*\"covid\"\n",
      "\n",
      "Score: 0.012505996972322464\t \n",
      "Topic: 0.013*\"border\" + 0.012*\"coronaviru\" + 0.010*\"updat\" + 0.008*\"covid\" + 0.007*\"hill\" + 0.007*\"social\" + 0.006*\"grandstand\" + 0.006*\"footag\" + 0.006*\"june\" + 0.006*\"reopen\"\n",
      "\n",
      "Score: 0.012505937367677689\t \n",
      "Topic: 0.028*\"trump\" + 0.017*\"donald\" + 0.012*\"coronaviru\" + 0.009*\"covid\" + 0.008*\"weather\" + 0.008*\"friday\" + 0.008*\"quarantin\" + 0.008*\"victorian\" + 0.007*\"sport\" + 0.007*\"zealand\"\n",
      "\n",
      "Score: 0.012505920603871346\t \n",
      "Topic: 0.009*\"queensland\" + 0.008*\"drought\" + 0.006*\"mental\" + 0.006*\"august\" + 0.006*\"victoria\" + 0.006*\"univers\" + 0.006*\"onlin\" + 0.006*\"coal\" + 0.005*\"liber\" + 0.005*\"health\"\n",
      "\n",
      "Score: 0.01250588521361351\t \n",
      "Topic: 0.011*\"royal\" + 0.010*\"andrew\" + 0.009*\"kill\" + 0.009*\"commiss\" + 0.006*\"indonesia\" + 0.005*\"china\" + 0.005*\"attack\" + 0.005*\"jam\" + 0.005*\"australian\" + 0.005*\"protest\"\n",
      "\n",
      "Score: 0.012504743412137032\t \n",
      "Topic: 0.011*\"drum\" + 0.010*\"lockdown\" + 0.010*\"australia\" + 0.009*\"world\" + 0.009*\"final\" + 0.007*\"leagu\" + 0.007*\"south\" + 0.007*\"michael\" + 0.007*\"wednesday\" + 0.006*\"open\"\n",
      "\n",
      "Score: 0.01250470895320177\t \n",
      "Topic: 0.019*\"polic\" + 0.015*\"charg\" + 0.014*\"crash\" + 0.014*\"woman\" + 0.011*\"murder\" + 0.009*\"shoot\" + 0.009*\"death\" + 0.008*\"assault\" + 0.008*\"arrest\" + 0.007*\"miss\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro documento de prueba tiene la mayor probabilidad de ser parte del tema que asignó nuestro modelo, que es la clasificación precisa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del modelo con un documento no visto antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.447002112865448\t Topic: 0.032*\"elect\" + 0.020*\"say\" + 0.017*\"minist\" + 0.015*\"miss\" + 0.014*\"speak\"\n",
      "Score: 0.41961172223091125\t Topic: 0.023*\"donald\" + 0.020*\"year\" + 0.018*\"chang\" + 0.015*\"lockdown\" + 0.013*\"school\"\n",
      "Score: 0.01667659915983677\t Topic: 0.030*\"victoria\" + 0.019*\"world\" + 0.017*\"coast\" + 0.016*\"south\" + 0.016*\"australia\"\n",
      "Score: 0.01667509786784649\t Topic: 0.031*\"queensland\" + 0.027*\"live\" + 0.021*\"women\" + 0.018*\"victorian\" + 0.014*\"andrew\"\n",
      "Score: 0.01667420007288456\t Topic: 0.075*\"coronaviru\" + 0.027*\"china\" + 0.021*\"kill\" + 0.018*\"die\" + 0.017*\"covid\"\n",
      "Score: 0.016673114150762558\t Topic: 0.033*\"govern\" + 0.018*\"border\" + 0.015*\"plan\" + 0.015*\"commun\" + 0.014*\"indigen\"\n",
      "Score: 0.016672847792506218\t Topic: 0.057*\"covid\" + 0.046*\"trump\" + 0.029*\"vaccin\" + 0.021*\"bushfir\" + 0.019*\"test\"\n",
      "Score: 0.016672180965542793\t Topic: 0.027*\"news\" + 0.023*\"market\" + 0.018*\"hospit\" + 0.017*\"morrison\" + 0.016*\"work\"\n",
      "Score: 0.016671841964125633\t Topic: 0.026*\"australia\" + 0.023*\"open\" + 0.016*\"final\" + 0.015*\"australian\" + 0.015*\"time\"\n",
      "Score: 0.016670262441039085\t Topic: 0.051*\"polic\" + 0.035*\"case\" + 0.025*\"charg\" + 0.025*\"court\" + 0.021*\"death\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'Melbourne man Suat Bayram dies in Türkiye earthquake, fears for other Australians'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8499851822853088\t Topic: 0.075*\"coronaviru\" + 0.027*\"china\" + 0.021*\"kill\" + 0.018*\"die\" + 0.017*\"covid\"\n",
      "Score: 0.01666956953704357\t Topic: 0.030*\"victoria\" + 0.019*\"world\" + 0.017*\"coast\" + 0.016*\"south\" + 0.016*\"australia\"\n",
      "Score: 0.016669245436787605\t Topic: 0.026*\"australia\" + 0.023*\"open\" + 0.016*\"final\" + 0.015*\"australian\" + 0.015*\"time\"\n",
      "Score: 0.016668954864144325\t Topic: 0.023*\"donald\" + 0.020*\"year\" + 0.018*\"chang\" + 0.015*\"lockdown\" + 0.013*\"school\"\n",
      "Score: 0.01666867360472679\t Topic: 0.027*\"news\" + 0.023*\"market\" + 0.018*\"hospit\" + 0.017*\"morrison\" + 0.016*\"work\"\n",
      "Score: 0.016668150201439857\t Topic: 0.057*\"covid\" + 0.046*\"trump\" + 0.029*\"vaccin\" + 0.021*\"bushfir\" + 0.019*\"test\"\n",
      "Score: 0.016668127849698067\t Topic: 0.051*\"polic\" + 0.035*\"case\" + 0.025*\"charg\" + 0.025*\"court\" + 0.021*\"death\"\n",
      "Score: 0.0166675616055727\t Topic: 0.032*\"elect\" + 0.020*\"say\" + 0.017*\"minist\" + 0.015*\"miss\" + 0.014*\"speak\"\n",
      "Score: 0.016667475923895836\t Topic: 0.031*\"queensland\" + 0.027*\"live\" + 0.021*\"women\" + 0.018*\"victorian\" + 0.014*\"andrew\"\n",
      "Score: 0.016667088493704796\t Topic: 0.033*\"govern\" + 0.018*\"border\" + 0.015*\"plan\" + 0.015*\"commun\" + 0.014*\"indigen\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'Melbourne man Suat Bayram dies in Türkiye earthquake, fears for other Australians'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Blei et al.,[Latent Dirichlet Allocation, 2003](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
    "2. [Topic Modeling and Latent Dirichlet Allocation (LDA) in Python, 2018](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24), en Toward data science.\n",
    "3. https://www.youtube.com/watch?v=T05t-SqKArY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
