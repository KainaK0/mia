{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:red\"><center>Introducción al Procesamiento Superficial de Textos</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Latent Dirichlet Allocation - LDA</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Análisis superficial de textos](#Análisis-superficial-de-textos)\n",
    "* [Terminología general](#Terminología-general)\n",
    "* [Preprocesamiento de datos textuales](#Preprocesamiento-de-datos-textuales)\n",
    "* [Bag of Words](#BagofWords)\n",
    "* [TF-IDF](#TF-IDF)\n",
    "* [Semántica latente](#Semántica-latente)\n",
    "* [Modelos generativos: Latent Dirichlet Allocation](#Modelos-generativos:-Latent-Dirichlet-Allocation)\n",
    "* [Ejemplo:Un millón de titulares](#Ejemplo:-Un-millón-de-titulares)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los humanos nos  comunicamos utilizando lenguajes naturales. Los lenguajes naturales difieren de los lenguajes de programación en que éstos últimos siguen reglas sintácticas y semánticas extrictas, mientras que los primeros por su complejidad dependen del contexto.\n",
    "\n",
    "\n",
    "En general el análisis de textos tiene dos grandes subáreas: el análisis superficial de textos y el procesamiento del lenguaje natural.\n",
    "\n",
    "En esta lección nos ocupamos del análisis superficial de textos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Análisis superficial de textos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta subárea se desarrolló primero, debido a que los problemas asociados al lenguaje natural en este caso son mas simples. Se trata de técnicas en la cuales se busca encontar los tópicos subayacentes en los texto. En este sentido, son modelos de tipo no supervisado y consecuencia basados en técnicas de clasificación automática. \n",
    "\n",
    "Estas técnicas están orientadas a detectar clusters de palabras y documentos en grandes corpus de datos.\n",
    "\n",
    "Un documento es en este caso una unidad distinguible de otras en el corpus. Por ejemplo una respuesta abierta en una encuesta, un comentario en una revisión, un abstract de un documento, etc. \n",
    "\n",
    "Luego de omitidos términos que se considera que no aportan a la detección de tópicos (temáticas), usualmente conocidos como *palabras vacías* (`stop words`) y de otros procesos de preprocesamiento como lematización, recorte (`steeming`),  es común construir una matriz denominada documento-témino (`dtm`).\n",
    "\n",
    "Esta matriz `dtm` representa por las filas a cada uno de los documentos individuales del corpus y por las columnas a cada uno de los términos conservados en el análisis. Cada posición de la matriz contiene el número de veces que un término aparece en el documento. En algunos casos esta es una matriz binaria, en cuyo caso la dtm indica cuando un término aparece en un documento.\n",
    "\n",
    "La  `dtm` es la base de las técnicas conocidas genéricamente como *bolsa de palabras* (`word-bag`). El nombre deriva del hecho de que al organizar la dtm, el contexto de las palabras en cada documento se pierde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Terminología general</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras o términos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La palabra es la unidad mínima de información  en el trabajo con lenguaje natural. \n",
    "\n",
    "Desde una perspectiva muy moderna las palabras son objetos que puede pensarse como puntos que están en un espacio de alta dimensión, de tal manera que, puntos cercanos en algún sentido de distancia corresponde a palabras que tienen una cercanía dentro de un universo de palabras considerado.\n",
    "\n",
    "La siguiente imagen corresponde a uno conjunto de palabras de astrofísica, consideradas en un estudio de resumenes de artículos científicos. Este es un gráfico obtenido luego de un procesamiento como lo que mostramos hoy, usando una técnica de análisis basada en la teoría de respuesta al ítem multidimensional (TRIM).\n",
    "\n",
    "En este documento las palabaras se denotarán como $w_i, i =1,2,\\ldots,K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/cluster_kmeans_10.png\" width=\"700\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Areas de conocimiento Astrofísica, a partir de artículos científicos</p>\n",
    "</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los documentos son los sujetos en los análisis textual superficial. Suponemos que se tiene un conjunto de documentos individuales, cada uno de los cuales se denotará por $\\mathbf{w}$. Se considera que un documento es una sucesión  de $N$ palabras. Así se tiene que un documento se denota como $\\mathbf{w} = \\{w_1,\\ldots,w_N \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un corpus es una colección de documentos en un problema particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tópicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tópicos son áreas latentes a las cuales están asociados tanto las palabras como los documentos. Uno de los propósitos principales del análisis de textos es descubrir o poner en evidencia tales tópicos.\n",
    "\n",
    "La figura anterior muestra por ejemplo la presencia de 10 tópicos en el conjunto de documentos de astrofísica analizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Preprocesamiento de datos textuales</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lo que sigue, vamos a utilizar los términos token y tokenizar, que aún no son adoptados por la Real Academia de la Lengua, pero que creemos pronto lo serán como tantos otros provenientes del inglés debido a su enorme utilización actual, por razón de los desarrollos científicos y tecnológicos. \n",
    "\n",
    "Realizaremos los siguientes pasos:\n",
    "    \n",
    "- **Tokenización**: divide el texto en oraciones y las oraciones en palabras. Ponga las palabras en minúsculas y elimine la puntuación.\n",
    "- Se **eliminan las palabras que tienen menos de 3 caracteres**.\n",
    "- Se eliminan todas las **palabras vacías**.\n",
    "- Las palabras se **lematizan**: las palabras en tercera persona se cambian a primera persona y los verbos en tiempo pasado y futuro se cambian a presente.\n",
    "- Las palabras se recortan (**stemming**): las palabras se reducen a su forma raíz.\n",
    "\n",
    "Usaremos  las librerías *gensim* y *nltk* para hacer este trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos términos que se utilizarán con frecuencia son:\n",
    "\n",
    "- `Corpus`: cuerpo del texto, singular. Corpora es el plural de corpus.\n",
    "- `Léxico`: palabras y sus significados.\n",
    "- `Token`: cada *entidad* que es parte de lo que sea que se dividió según las reglas que establecemos para el análisis. Por ejemplo, cada palabra es un token cuando una oración se tokeniza en palabras. Cada oración también puede ser un token, si ha convertido las oraciones en un párrafo.\n",
    "\n",
    "Básicamente, tokenizar implica dividir oraciones y palabras del cuerpo del texto.\n",
    "\n",
    "Veá el siguiente ejemplo tomado de [Geek for Geeks](https://www.geeksforgeeks.org/tokenize-text-using-nltk-python/?ref=rp). Usamos la librería *nltk*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa recursos de `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import the existing word and sentence tokenizing  \n",
    "# libraries \n",
    "import nltk\n",
    "\n",
    "\n",
    "# tokenizadores\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# diccionarios especiales para puntuación y palabras vacias\n",
    "nltk.download('punkt') # Manejo de puntuación\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# lematizador basado en WordNet de nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# steemer de nltk. Raiz de las palabras\n",
    "#from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa recursos de `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in d:\\anaconda\\envs\\nlp311_v1\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in d:\\anaconda\\envs\\nlp311_v1\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/24.0 MB 24.6 MB/s eta 0:00:01\n",
      "   - -------------------------------------- 1.0/24.0 MB 24.6 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 3.1/24.0 MB 5.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 4.2/24.0 MB 5.3 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 4.2/24.0 MB 5.3 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.3/24.0 MB 4.9 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 8.4/24.0 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.4/24.0 MB 5.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 9.7/24.0 MB 4.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.8/24.0 MB 5.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.6/24.0 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 14.7/24.0 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 14.7/24.0 MB 5.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 16.0/24.0 MB 5.2 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 16.8/24.0 MB 5.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 17.8/24.0 MB 5.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 17.8/24.0 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 18.9/24.0 MB 4.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 19.9/24.0 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.0/24.0 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.0/24.0 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.0/24.0 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.8/24.0 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 4.6 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 4.7/46.2 MB 23.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 9.7/46.2 MB 23.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 15.7/46.2 MB 24.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 22.3/46.2 MB 26.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 29.1/46.2 MB 27.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 36.4/46.2 MB 28.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 44.6/46.2 MB 29.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.1/46.2 MB 29.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.1/46.2 MB 29.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.1/46.2 MB 29.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 19.6 MB/s eta 0:00:00\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: smart-open, scipy, gensim\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.2\n",
      "    Uninstalling scipy-1.15.2:\n",
      "      Successfully uninstalled scipy-1.15.2\n",
      "Successfully installed gensim-4.3.3 scipy-1.13.1 smart-open-7.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\Anaconda\\envs\\NLP311_v1\\Lib\\site-packages\\~cipy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\Anaconda\\envs\\NLP311_v1\\Lib\\site-packages\\~cipy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenización por sentencias:\n",
      "\n",
      "Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora. \n",
      "\n",
      "Challenges in natural language processing frequently involve natural language understanding, natural language generation frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof. \n",
      "\n",
      "There are 365 days usually. \n",
      "\n",
      "This year is 2020. \n",
      "\n",
      "Tokenización por sentencias:\n",
      "\n",
      "['Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.', 'Challenges in natural language processing frequently involve natural language understanding, natural language generation frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.', 'There are 365 days usually.', 'This year is 2020.']\n",
      "\n",
      " Tokenización por palabras:\n",
      "Natural language processing ( NLP ) is a field of computer science , artificial intelligence and computational linguistics concerned with the interactions between computers and human ( natural ) languages , and , in particular , concerned with programming computers to fruitfully process large natural language corpora . Challenges in natural language processing frequently involve natural language understanding , natural language generation frequently from formal , machine-readable logical forms ) , connecting language and machine perception , managing human-computer dialog systems , or some combination thereof . There are 365 days usually . This year is 2020 . \n",
      "\n",
      " Tokenización por palabras:\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.', 'There', 'are', '365', 'days', 'usually', '.', 'This', 'year', 'is', '2020', '.']\n",
      "\n",
      " Tokenización por caracteres:\n",
      "N a t u r a l   l a n g u a g e   p r o c e s s i n g   ( N L P )   i s   a   f i e l d   o f   c o m p u t e r   s c i e n c e ,   a r t i f i c i a l   i n t e l l i g e n c e   a n d   c o m p u t a t i o n a l   l i n g u i s t i c s   c o n c e r n e d   w i t h   t h e   i n t e r a c t i o n s   b e t w e e n   c o m p u t e r s   a n d   h u m a n   ( n a t u r a l )   l a n g u a g e s ,   a n d ,   i n   p a r t i c u l a r ,   c o n c e r n e d   w i t h   p r o g r a m m i n g   c o m p u t e r s   t o   f r u i t f u l l y   p r o c e s s   l a r g e   n a t u r a l   l a n g u a g e   c o r p o r a .   C h a l l e n g e s   i n   n a t u r a l   l a n g u a g e   p r o c e s s i n g   f r e q u e n t l y   i n v o l v e   n a t u r a l   l a n g u a g e   u n d e r s t a n d i n g ,   n a t u r a l   l a n g u a g e   g e n e r a t i o n   f r e q u e n t l y   f r o m   f o r m a l ,   m a c h i n e - r e a d a b l e   l o g i c a l   f o r m s ) ,   c o n n e c t i n g   l a n g u a g e   a n d   m a c h i n e   p e r c e p t i o n ,   m a n a g i n g   h u m a n - c o m p u t e r   d i a l o g   s y s t e m s ,   o r   s o m e   c o m b i n a t i o n   t h e r e o f .   T h e r e   a r e   3 6 5   d a y s   u s u a l l y .   T h i s   y e a r   i s   2 0 2 0 . \n",
      "\n",
      " Tokenización por caracteres:\n",
      "['N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', '(', 'N', 'L', 'P', ')', ' ', 'i', 's', ' ', 'a', ' ', 'f', 'i', 'e', 'l', 'd', ' ', 'o', 'f', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', ' ', 's', 'c', 'i', 'e', 'n', 'c', 'e', ',', ' ', 'a', 'r', 't', 'i', 'f', 'i', 'c', 'i', 'a', 'l', ' ', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', ' ', 'a', 'n', 'd', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'a', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'l', 'i', 'n', 'g', 'u', 'i', 's', 't', 'i', 'c', 's', ' ', 'c', 'o', 'n', 'c', 'e', 'r', 'n', 'e', 'd', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'i', 'n', 't', 'e', 'r', 'a', 'c', 't', 'i', 'o', 'n', 's', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'u', 'm', 'a', 'n', ' ', '(', 'n', 'a', 't', 'u', 'r', 'a', 'l', ')', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', 's', ',', ' ', 'a', 'n', 'd', ',', ' ', 'i', 'n', ' ', 'p', 'a', 'r', 't', 'i', 'c', 'u', 'l', 'a', 'r', ',', ' ', 'c', 'o', 'n', 'c', 'e', 'r', 'n', 'e', 'd', ' ', 'w', 'i', 't', 'h', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', 's', ' ', 't', 'o', ' ', 'f', 'r', 'u', 'i', 't', 'f', 'u', 'l', 'l', 'y', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', ' ', 'l', 'a', 'r', 'g', 'e', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'c', 'o', 'r', 'p', 'o', 'r', 'a', '.', ' ', 'C', 'h', 'a', 'l', 'l', 'e', 'n', 'g', 'e', 's', ' ', 'i', 'n', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', 'f', 'r', 'e', 'q', 'u', 'e', 'n', 't', 'l', 'y', ' ', 'i', 'n', 'v', 'o', 'l', 'v', 'e', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', 'i', 'n', 'g', ',', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'i', 'o', 'n', ' ', 'f', 'r', 'e', 'q', 'u', 'e', 'n', 't', 'l', 'y', ' ', 'f', 'r', 'o', 'm', ' ', 'f', 'o', 'r', 'm', 'a', 'l', ',', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', '-', 'r', 'e', 'a', 'd', 'a', 'b', 'l', 'e', ' ', 'l', 'o', 'g', 'i', 'c', 'a', 'l', ' ', 'f', 'o', 'r', 'm', 's', ')', ',', ' ', 'c', 'o', 'n', 'n', 'e', 'c', 't', 'i', 'n', 'g', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'a', 'n', 'd', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'p', 'e', 'r', 'c', 'e', 'p', 't', 'i', 'o', 'n', ',', ' ', 'm', 'a', 'n', 'a', 'g', 'i', 'n', 'g', ' ', 'h', 'u', 'm', 'a', 'n', '-', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', ' ', 'd', 'i', 'a', 'l', 'o', 'g', ' ', 's', 'y', 's', 't', 'e', 'm', 's', ',', ' ', 'o', 'r', ' ', 's', 'o', 'm', 'e', ' ', 'c', 'o', 'm', 'b', 'i', 'n', 'a', 't', 'i', 'o', 'n', ' ', 't', 'h', 'e', 'r', 'e', 'o', 'f', '.', ' ', 'T', 'h', 'e', 'r', 'e', ' ', 'a', 'r', 'e', ' ', '3', '6', '5', ' ', 'd', 'a', 'y', 's', ' ', 'u', 's', 'u', 'a', 'l', 'l', 'y', '.', ' ', 'T', 'h', 'i', 's', ' ', 'y', 'e', 'a', 'r', ' ', 'i', 's', ' ', '2', '0', '2', '0', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural language processing (NLP) is a field \" \\\n",
    "       + \"of computer science, artificial intelligence \" \\\n",
    "       + \"and computational linguistics concerned with \" \\\n",
    "       +\"the interactions between computers and human \" \\\n",
    "       + \"(natural) languages, and, in particular, \" \\\n",
    "       + \"concerned with programming computers to \" \\\n",
    "       + \"fruitfully process large natural language \" \\\n",
    "       + \"corpora. Challenges in natural language \" \\\n",
    "       + \"processing frequently involve natural \" \\\n",
    "       + \"language understanding, natural language \" \\\n",
    "       + \"generation frequently from formal, machine\" \\\n",
    "       + \"-readable logical forms), connecting language \" \\\n",
    "       + \"and machine perception, managing human-\" \\\n",
    "       + \"computer dialog systems, or some combination \" \\\n",
    "       + \"thereof. There are 365 days usually. \" \\\n",
    "       + \"This year is 2020.\"\n",
    "\n",
    "# sentencias\n",
    "print('Tokenización por sentencias:\\n')\n",
    "sentences = sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence,'\\n')\n",
    "print('Tokenización por sentencias:\\n')\n",
    "print(sent_tokenize(text)) \n",
    "\n",
    "# palabras\n",
    "tokens = word_tokenize(text)\n",
    "print('\\n Tokenización por palabras:')\n",
    "for token in tokens:\n",
    "    print(token, end =' ')\n",
    "print('')\n",
    "print('\\n Tokenización por palabras:')\n",
    "print(word_tokenize(text))\n",
    "\n",
    "# caracteres\n",
    "chars = [char for char in text]\n",
    "print('\\n Tokenización por caracteres:')\n",
    "for char in chars:\n",
    "    print(char, end =' ')\n",
    "print('')\n",
    "print('\\n Tokenización por caracteres:')\n",
    "print(chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cooool',\n",
       " '#dummysmiley',\n",
       " ':',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<',\n",
       " '>',\n",
       " '->',\n",
       " '<--']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tknzr.tokenize(s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizar tweets usando  los parámetros `strip_handles` y `reduce_len`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "s1 = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
    "tw = tknzr.tokenize(s1)\n",
    "print(tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cambiar texto a minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.', 'there', 'are', '365', 'days', 'usually', '.', 'this', 'year', 'is', '2020', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens[:] = [token.lower() for token in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remueve carateres especiales - expresiones regulares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las expresiones regulares son objetos matemáticos que permiten interpretar trozos de texto. Son claves en la construcción de los lenguajes de programación. Aquí vamos a usar la librería [re](https://docs.python.org/3/library/re.html) de Python creada para el manejo de expresiones regulares. Les sugerimos este [tutorial sobre re en Python](https://www.w3schools.com/python/python_regex.asp) para aprender a manejar la librería re.\n",
    "\n",
    "Usaremos aquí para eliminar algunos símbolos: los números y los paréntesis por ejemplo. No siempre es el caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '', 'nlp', '', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '', 'natural', '', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', '', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.', 'there', 'are', '', 'days', 'usually', '.', 'this', 'year', 'is', '', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# substituir digitos\n",
    "tokens = [re.sub(r'\\d+', '',token) for token in tokens]\n",
    "# substituir paréntesis\n",
    "tokens = [re.sub(r'[()]', '',token) for token in tokens]\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remueve palabras de longitud menor o igual a tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'with', 'interactions', 'between', 'computers', 'human', 'natural', 'languages', 'particular', 'concerned', 'with', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', 'machine-readable', 'logical', 'forms', 'connecting', 'language', 'machine', 'perception', 'managing', 'human-computer', 'dialog', 'systems', 'some', 'combination', 'thereof', 'there', 'days', 'usually', 'this', 'year']\n"
     ]
    }
   ],
   "source": [
    "tokens_4 = []\n",
    "for token in tokens:\n",
    "    if len(token) > 3:\n",
    "        tokens_4.append(token)\n",
    "tokens = tokens_4\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Palabras vacias (stop words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las palabras varias o stop words son palabras que en el lenguaje común se considera que no aportan al contenido semántico de los textos. En la técnica de bolsa de palabras son omitidos, debido a que causan clasificaciones confusas. En realidad el concepto de palabras vacía depende del contesto de utlización de las técnicas. \n",
    "\n",
    "El siguiente ejemplo muestra el diccionario de palabras vacías del inglés contenidas en la librería `gensim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amongst', 'behind', 'say', 'less', 'out', 'see', 'inc', 'con', 'yourself', 'when', 'bill', 'its', 'seemed', 'often', 'him', 'yourselves', 'since', 're', 'does', 'amoungst', 'nobody', 'anyone', 'three', 'too', 'fifty', 'de', 'are', 'used', 'under', 'many', 'neither', 'across', 'whom', 'thence', 'part', 'while', 'co', 'whether', 'few', 'beforehand', 'hence', 'didn', 'mine', 'describe', 'indeed', 'unless', 'nothing', 'top', 'seeming', 'these', 'somewhere', 'anyway', 'yet', 'also', 'don', 'find', 'still', 'itself', 'four', 'about', 'would', 'except', 'detail', 'those', 'enough', 'by', 'go', 'is', 'hereupon', 'one', 'wherein', 'amount', 'seems', 'formerly', 'his', 'sometime', 'how', 'me', 'himself', 'below', 'mill', 'could', 'all', 'of', 'thereafter', 'further', 'above', 'whose', 'whoever', 'do', 'nor', 'already', 'than', 'for', 'twenty', 'thin', 'besides', 'to', 'some', 'never', 'nine', 'elsewhere', 'he', 'if', 'forty', 'hers', 'through', 'up', 'who', 'keep', 'they', 'again', 'their', 'therein', 'a', 'be', 'via', 'otherwise', 'doing', 'becoming', 'after', 'name', 'every', 'can', 'twelve', 'moreover', 'ie', 'throughout', 'system', 'with', 'ten', 'sincere', 'and', 'were', 'give', 'etc', 'on', 'un', 'her', 'put', 'once', 'within', 'hasnt', 'from', 'meanwhile', 'become', 'bottom', 'whereupon', 'computer', 'everything', 'former', 'whereafter', 'cry', 'off', 'cannot', 'third', 'there', 'must', 'around', 'full', 'ever', 'upon', 'sometimes', 'always', 'per', 'in', 'whereby', 'us', 'regarding', 'into', 'nowhere', 'another', 'show', 'namely', 'only', 'km', 'have', 'using', 'make', 'almost', 'alone', 'either', 'whereas', 'hereafter', 'two', 'beyond', 'until', 'herein', 'seem', 'but', 'call', 'afterwards', 'this', 'both', 'down', 'although', 'wherever', 'i', 'anyhow', 'due', 'during', 'back', 'latterly', 'should', 'hundred', 'that', 'side', 'eg', 'whenever', 'much', 'herself', 'against', 'none', 'perhaps', 'over', 'anything', 'others', 'we', 'them', 'or', 'myself', 'please', 'ourselves', 'though', 'why', 'thereupon', 'serious', 'thus', 'it', 'so', 'has', 'empty', 'yours', 'thick', 'ours', 'really', 'rather', 'most', 'other', 'becomes', 'more', 'even', 'between', 'next', 'will', 'am', 'own', 'get', 'couldnt', 'any', 'been', 'because', 'you', 'thereby', 'eleven', 'hereby', 'which', 'onto', 'last', 'whatever', 'beside', 'well', 'towards', 'noone', 'therefore', 'found', 'same', 'everywhere', 'what', 'our', 'somehow', 'your', 'where', 'before', 'something', 'the', 'fifteen', 'themselves', 'had', 'however', 'no', 'first', 'anywhere', 'very', 'someone', 'being', 'various', 'front', 'take', 'sixty', 'thru', 'whence', 'may', 'five', 'ltd', 'kg', 'among', 'as', 'such', 'whole', 'toward', 'here', 'then', 'doesn', 'now', 'was', 'several', 'did', 'fire', 'along', 'she', 'together', 'nevertheless', 'made', 'fill', 'became', 'might', 'else', 'my', 'move', 'at', 'just', 'each', 'an', 'six', 'latter', 'eight', 'everyone', 'mostly', 'not', 'interest', 'quite', 'cant', 'without', 'whither', 'least', 'done']\n"
     ]
    }
   ],
   "source": [
    "stop_words_g = []\n",
    "for token  in gensim.parsing.preprocessing.STOPWORDS:\n",
    "    stop_words_g.append(token)\n",
    "print(stop_words_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la librería *nltk* el diciconario de palabras vacías del inglés es actualmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"he'd\", 'all', 'into', 'which', 'of', 'further', 'above', 'haven', \"wouldn't\", 'do', 'nor', 'than', \"he's\", 'for', 'mustn', 'out', 'to', 'only', 'have', 'some', \"you'll\", 't', \"i'd\", \"we'd\", 'he', 'same', 'if', 'shan', 'yourself', \"i've\", \"aren't\", 'what', 'when', 'our', 'ma', \"we'll\", 'your', 'having', 'before', 'hers', \"should've\", 'where', 'through', \"they've\", 'up', 'its', 'won', 'who', 'm', 'o', 'they', 'until', 'the', 'him', \"it'll\", 'themselves', \"that'll\", 'again', 'but', 'their', \"they'll\", 'yourselves', 'a', 'had', 're', \"she's\", 'no', \"you'd\", 'does', \"wasn't\", 'be', 'both', 'this', 'down', \"haven't\", 'very', 'doing', 'i', 'too', 'after', 'are', 'can', 'couldn', 'during', 'being', \"shouldn't\", 'hasn', 'should', \"hasn't\", \"hadn't\", \"she'd\", 'under', \"we're\", 'that', 'whom', 'herself', 'against', 'd', \"mustn't\", 'over', 'as', 'while', 'we', \"mightn't\", 'or', 'such', \"doesn't\", \"didn't\", 'them', 'with', \"you've\", 'myself', 'and', 'y', 'weren', 'few', 'were', 'ourselves', 'here', 'why', \"weren't\", 'wasn', 'isn', 'then', 'doesn', \"she'll\", 'now', 'didn', 'on', \"needn't\", 'was', \"shan't\", 'did', \"couldn't\", \"he'll\", 'her', 'it', 'needn', 'so', 'theirs', 'once', 'these', 'has', 'she', 'yours', 'from', \"won't\", 'don', 'ours', \"i'm\", 'itself', 'about', \"isn't\", \"it'd\", 'ain', 'most', 'other', 'aren', \"we've\", 'more', 'wouldn', \"they're\", 'off', 'my', 'between', 'those', \"i'll\", 'at', 'by', 'is', 'an', 'each', 'just', 'there', 'am', 'will', 'own', 'hadn', \"it's\", 'any', \"they'd\", 'not', 'been', 'because', 've', 'you', 'his', 'how', 'mightn', 'me', 'himself', 's', 'll', \"don't\", 'in', 'shouldn', 'below', \"you're\"}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#\n",
    "stopWords = set(stopwords.words('english'))\n",
    "#\n",
    "print(stopWords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que los dos conjuntos de palabara vacías son distintos.\n",
    "\n",
    "Como ejemplo vamos quitar la palabras vacias del objeto text tokenizado definido arriba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras vacías-Español nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fuésemos', 'sentid', 'tuyos', 'vosotras', 'habíamos', 'mucho', 'fueras', 'estad', 'otras', 'fuiste', 'estés', 'con', 'estuvierais', 'para', 'hasta', 'ellos', 'siente', 'fuesen', 'tuvieras', 'suyas', 'o', 'qué', 'erais', 'fue', 'sus', 'hubisteis', 'quien', 'donde', 'de', 'eso', 'seamos', 'sea', 'mías', 'hubimos', 'era', 'habrá', 'otra', 'os', 'hubieran', 'fuera', 'tendrá', 'estuviera', 'una', 'cuando', 'sentido', 'estuviéramos', 'seré', 'se', 'sobre', 'tuviese', 'vuestros', 'e', 'estuviésemos', 'otro', 'esa', 'sentidos', 'estarías', 'todo', 'tuviesen', 'contra', 'habíais', 'estaríamos', 'estada', 'en', 'estabas', 'estando', 'seríais', 'pero', 'tenéis', 'sí', 'nosotros', 'está', 'estuvieras', 'habré', 'somos', 'tenías', 'me', 'tendrían', 'eres', 'nada', 'suyos', 'fuerais', 'vosotros', 'están', 'habiendo', 'serías', 'antes', 'tus', 'tenga', 'he', 'fueseis', 'otros', 'soy', 'porque', 'hubiéramos', 'hube', 'cual', 'tú', 'estuvo', 'tuvieseis', 'hubieras', 'hubiésemos', 'esta', 'estuvieran', 'habrán', 'a', 'estáis', 'suyo', 'fuese', 'estuviste', 'tendré', 'estamos', 'fueran', 'estuvieses', 'estaría', 'quienes', 'hayas', 'habían', 'fui', 'tuvieron', 'teniendo', 'ellas', 'vuestras', 'eras', 'habríamos', 'te', 'y', 'esas', 'serás', 'hemos', 'tengas', 'tendrás', 'un', 'estás', 'ni', 'sintiendo', 'tuvieran', 'tendréis', 'habrías', 'nuestras', 'mi', 'tenían', 'tuvo', 'mía', 'tienes', 'tuviera', 'fueron', 'estabais', 'hubiesen', 'habréis', 'hubiste', 'tenía', 'él', 'muy', 'tienen', 'ella', 'mis', 'habremos', 'tendremos', 'algunas', 'tuviéramos', 'son', 'tuyo', 'sean', 'estaban', 'teníais', 'también', 'éramos', 'ese', 'seáis', 'nuestra', 'será', 'todos', 'habidos', 'mío', 'habida', 'al', 'les', 'fuéramos', 'estábamos', 'estuvieseis', 'estuvieron', 'sería', 'estaréis', 'poco', 'hubieses', 'vuestra', 'es', 'hayáis', 'tuve', 'estaré', 'hubierais', 'estuviese', 'algunos', 'lo', 'tanto', 'tendríais', 'habido', 'fueses', 'el', 'estarían', 'estas', 'teníamos', 'estarán', 'sentidas', 'hubiese', 'habría', 'que', 'serían', 'estarás', 'tengo', 'tenidas', 'habrían', 'has', 'fuimos', 'su', 'habías', 'estaríais', 'esos', 'ha', 'hubo', 'estoy', 'estén', 'habrás', 'durante', 'tenemos', 'tuvimos', 'por', 'estará', 'esto', 'tuya', 'haya', 'estéis', 'más', 'hay', 'estuve', 'estos', 'este', 'había', 'hayamos', 'tengan', 'hubiera', 'seas', 'sentida', 'míos', 'tenida', 'estado', 'seremos', 'tuvierais', 'tengáis', 'seríamos', 'suya', 'hayan', 'como', 'habidas', 'nos', 'tuviésemos', 'habéis', 'estados', 'estemos', 'muchos', 'no', 'desde', 'le', 'la', 'estadas', 'estuviesen', 'tuyas', 'nuestro', 'tened', 'tiene', 'algo', 'del', 'entre', 'estaremos', 'ya', 'tendrán', 'estuvisteis', 'hubieseis', 'serán', 'estaba', 'fuisteis', 'seréis', 'uno', 'estuvimos', 'hubieron', 'eran', 'tenido', 'ante', 'tuvisteis', 'nosotras', 'nuestros', 'mí', 'sin', 'tuviste', 'esté', 'tenidos', 'habríais', 'tendría', 'tendrías', 'ti', 'tuvieses', 'han', 'los', 'tengamos', 'tu', 'unos', 'tendríamos', 'yo', 'vuestro', 'las', 'estar', 'sois'}\n"
     ]
    }
   ],
   "source": [
    "palabrasVacias = set(stopwords.words('spanish'))\n",
    "#\n",
    "print(palabrasVacias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a quitar las palabras vacías del ejemplo usando nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'with', 'interactions', 'between', 'computers', 'human', 'natural', 'languages', 'particular', 'concerned', 'with', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', 'machine-readable', 'logical', 'forms', 'connecting', 'language', 'machine', 'perception', 'managing', 'human-computer', 'dialog', 'systems', 'some', 'combination', 'thereof', 'there', 'days', 'usually', 'this', 'year']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'interactions', 'computers', 'human', 'natural', 'languages', 'particular', 'concerned', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'frequently', 'formal', 'machine-readable', 'logical', 'forms', 'connecting', 'language', 'machine', 'perception', 'managing', 'human-computer', 'dialog', 'systems', 'combination', 'thereof', 'days', 'usually', 'year']\n"
     ]
    }
   ],
   "source": [
    "tokens_n_e = []\n",
    "\n",
    "for token in tokens:\n",
    "    if token not in stopWords:\n",
    "        tokens_n_e.append(token)\n",
    "#\n",
    "tokens = tokens_n_e\n",
    "print(tokens)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lematización es el proceso de agrupar las diferentes formas flexionadas de una palabra para que puedan analizarse como un solo elemento. La lematización es similar a la derivación, pero aporta contexto a las palabras. Por lo tanto, vincula palabras con un significado similar a una palabra.\n",
    "\n",
    "El preprocesamiento de texto incluye tanto `Stemming` como `Lemmatization`. \n",
    "\n",
    "Muchas veces las personas encuentran confusos estos dos términos. Algunos tratan a estos dos como iguales. \n",
    "\n",
    "En realidad, se prefiere la lematización a la derivación porque la lematización realiza un análisis morfológico de las palabras.\n",
    "\n",
    "Las aplicaciones de la lematización son:\n",
    "\n",
    "- Se utiliza en sistemas de recuperación integrales como motores de búsqueda.\n",
    "- Utilizado en indexación compacta\n",
    "- Ejemplos de lematización:\n",
    "\n",
    "* rocas -> roca\n",
    "*  corpora -> corpus\n",
    "* mejor -> bueno\n",
    "\n",
    "Una diferencia importante con la derivación es que lematizar toma una parte del parámetro de voz, \"pos\". Si no se proporciona, el valor predeterminado es \"sustantivo\". En el siguiente ejemplo vamos colocar *pos='a'* que significa adjetivo. Si se coloca *pos ='v'* significa verbo. Por defecto es *pos ='n'*, es decir sustantivo.\n",
    "\n",
    "A continuación se muestra la implementación de lematización de algunas palabras en inglés usando la librería *nltk*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n",
      "better : better\n",
      "better : better\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "  \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"n\")) \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"v\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos lematizar el texto de ejemplo, primero con verbos y luego con sustantivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'interactions', 'computers', 'human', 'natural', 'languages', 'particular', 'concerned', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'frequently', 'formal', 'machine-readable', 'logical', 'forms', 'connecting', 'language', 'machine', 'perception', 'managing', 'human-computer', 'dialog', 'systems', 'combination', 'thereof', 'days', 'usually', 'year']\n",
      "\n",
      "\n",
      "['natural', 'language', 'process', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concern', 'interactions', 'computers', 'human', 'natural', 'languages', 'particular', 'concern', 'program', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenge', 'natural', 'language', 'process', 'frequently', 'involve', 'natural', 'language', 'understand', 'natural', 'language', 'generation', 'frequently', 'formal', 'machine-readable', 'logical', 'form', 'connect', 'language', 'machine', 'perception', 'manage', 'human-computer', 'dialog', 'systems', 'combination', 'thereof', 'days', 'usually', 'year']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#\n",
    "# verbs\n",
    "lemma_text =[]\n",
    "for token in tokens:\n",
    "    lemma_text.append(WordNetLemmatizer().lemmatize(token, pos='v'))\n",
    "   \n",
    "\n",
    "\n",
    "print(tokens)\n",
    "print('\\n')\n",
    "print(lemma_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'process', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concern', 'interaction', 'computer', 'human', 'natural', 'language', 'particular', 'concern', 'program', 'computer', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpus', 'challenge', 'natural', 'language', 'process', 'frequently', 'involve', 'natural', 'language', 'understand', 'natural', 'language', 'generation', 'frequently', 'formal', 'machine-readable', 'logical', 'form', 'connect', 'language', 'machine', 'perception', 'manage', 'human-computer', 'dialog', 'system', 'combination', 'thereof', 'day', 'usually', 'year']\n"
     ]
    }
   ],
   "source": [
    "# nouns\n",
    "for i in range(len(lemma_text )):\n",
    "    lemma_text[i] = WordNetLemmatizer().lemmatize(lemma_text[i], pos='n')\n",
    "print(lemma_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steeming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "La derivación (steeming) es el proceso de producir variantes morfológicas de una palabra raíz / base. Los programas de derivación se conocen comúnmente como algoritmos de steeming o derivaciones. Un algoritmo de stemming reduce las palabras como en los siguientes ejemplos\n",
    "\n",
    "+ \"chocolates\", \"chocolates\", \"choco\" a la raíz de la palabra, \"chocolate\"\n",
    "+ \"recuperación\", \"recuperado\", \"recupera\" se reduce a la raíz \"recuperar\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errores en la derivación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay principalmente dos errores en la derivación: la derivación excesiva y la derivación insuficiente. \n",
    "\n",
    "El sobre-recorte excesivo ocurre cuando dos palabras se derivan de la misma raíz que tienen raíces diferentes. \n",
    "\n",
    "El  sub-recorte ocurre cuando dos palabras se derivan de la misma raíz pero tienen raíces diferentes.\n",
    "\n",
    "Como ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'field', 'comput', 'scienc', 'artifici', 'intellig', 'comput', 'linguist', 'concern', 'interact', 'comput', 'human', 'natur', 'languag', 'particular', 'concern', 'program', 'comput', 'fruit', 'process', 'larg', 'natur', 'languag', 'corpu', 'challeng', 'natur', 'languag', 'process', 'frequent', 'involv', 'natur', 'languag', 'understand', 'natur', 'languag', 'gener', 'frequent', 'formal', 'machine-read', 'logic', 'form', 'connect', 'languag', 'machin', 'percept', 'manag', 'human-comput', 'dialog', 'system', 'combin', 'thereof', 'day', 'usual', 'year']\n"
     ]
    }
   ],
   "source": [
    "#from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer \n",
    "# crea una instancia de PorterStemmer \n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in range(len(lemma_text)):\n",
    "    lemma_text[i] = ps.stem(lemma_text[i])\n",
    "print(lemma_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Bag of Words</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\envs\\nlp311_v1\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\anaconda\\envs\\nlp311_v1\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\anaconda\\envs\\nlp311_v1\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\anaconda\\envs\\nlp311_v1\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\anaconda\\envs\\nlp311_v1\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (Feature Names):\n",
      "['at' 'ball' 'barked' 'cat' 'chased' 'dog' 'mat' 'on' 'played' 'sat' 'the'\n",
      " 'with']\n",
      "\n",
      "Bag of Words Matrix:\n",
      "[[0 0 0 1 0 0 1 1 0 1 2 0]\n",
      " [0 1 0 0 0 1 0 0 1 0 2 1]\n",
      " [0 0 0 1 1 1 0 0 0 0 2 0]\n",
      " [1 0 1 1 0 1 0 0 0 0 2 0]]\n",
      "\n",
      "Document-Term Matrix:\n",
      "Document 1: The cat sat on the mat\n",
      "Vector: [0 0 0 1 0 0 1 1 0 1 2 0]\n",
      "\n",
      "Document 2: The dog played with the ball\n",
      "Vector: [0 1 0 0 0 1 0 0 1 0 2 1]\n",
      "\n",
      "Document 3: The cat chased the dog\n",
      "Vector: [0 0 0 1 1 1 0 0 0 0 2 0]\n",
      "\n",
      "Document 4: The dog barked at the cat\n",
      "Vector: [1 0 1 1 0 1 0 0 0 0 2 0]\n",
      "\n",
      "New text vector: [[0 0 0 1 0 1 0 0 1 0 2 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog played with the ball\",\n",
    "    \"The cat chased the dog\",\n",
    "    \"The dog barked at the cat\"\n",
    "]\n",
    "\n",
    "# Step 1: Create CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Step 2: Fit and transform the documents into BoW vectors\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Step 3: Get feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display results\n",
    "print(\"Vocabulary (Feature Names):\")\n",
    "print(feature_names)\n",
    "print(\"\\nBag of Words Matrix:\")\n",
    "print(bow_matrix.toarray())\n",
    "\n",
    "# Convert to dense array for better visualization\n",
    "dense_matrix = bow_matrix.toarray()\n",
    "\n",
    "print(\"\\nDocument-Term Matrix:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1}: {doc}\")\n",
    "    print(\"Vector:\", dense_matrix[i])\n",
    "    print()\n",
    "\n",
    "# Example: Transform new text using the same vectorizer\n",
    "new_text = [\"The cat played with the dog\"]\n",
    "new_text_vector = vectorizer.transform(new_text)\n",
    "print(\"New text vector:\", new_text_vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector: [0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 2, 0]\n",
    "    'at': 0 (does not appear)\n",
    "    'ball': 0 (does not appear)\n",
    "    'barked': 0 (does not appear)\n",
    "    'cat': 1 (appears once)\n",
    "    'chased': 0 (does not appear)\n",
    "    'dog': 0 (does not appear)\n",
    "    'mat': 1 (appears once)\n",
    "    'on': 1 (appears once)\n",
    "    'played': 0 (does not appear)\n",
    "    'sat': 1 (appears once)\n",
    "    'the': 2 (appears twice)\n",
    "    'with': 0 (does not appear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">TF-IDF</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomado de [Wikipedia](https://es.wikipedia.org/wiki/Tf-idf).\n",
    "\n",
    "Tf-idf (del inglés Term frequency – Inverse document frequency), frecuencia de término – frecuencia inversa de documento (o sea, la frecuencia de ocurrencia del término en el corpus de documentos), es una medida numérica que expresa cuán relevante es una palabra para un documento en un corpus. Esta medida se utiliza a menudo como un factor de ponderación en la recuperación de información y la minería de textos. \n",
    "\n",
    "\n",
    "El valor tf-idf aumenta proporcionalmente al número de veces que una palabra aparece en el documento, pero es compensada por la frecuencia de la palabra en el corpus de documentos, lo que permite manejar el hecho de que algunas palabras son generalmente más comunes que otras.\n",
    "\n",
    "Variaciones del esquema de peso tf-idf son empleadas frecuentemente por los motores de búsqueda como herramienta fundamental para medir la relevancia de un documento dada una consulta del usuario, estableciendo así una ordenación o ranking de los mismos. \n",
    "\n",
    "\n",
    "Tf-idf puede utilizarse exitosamente para el filtrado de las palabras vacías (stop-words), en diferentes campos del pre-procesamiento de textos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detalles matemáticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf es el producto de dos medidas, *frecuencia de término* y *frecuencia inversa de documento*. Existen varias maneras de determinar el valor de ambas. \n",
    "\n",
    "En el caso de la frecuencia de término $\\text{tf}(t, d)$, la opción más sencilla es usar la frecuencia bruta del término $t$ en el documento $d$, o sea, el número de veces que el término $t$ ocurre en el documento $d$. Si denotamos la frecuencia bruta de $t$ por $f(t,d)$, entonces el esquema $\\text{tf}$ simple es $\\text{tf}(t, d) = f(t,d)$. \n",
    "\n",
    "\n",
    "Otras posibilidades son:\n",
    "\n",
    "- *frecuencias\" booleanas*: tf(t,d) = 1 si t ocurre en d, y 0 si no;\n",
    "- *frecuencia escalada logarítmicamente*: tf(t,d) = 1 + log f(t,d) (y 0 si f(t,d)=0);\n",
    "- *frecuencia normalizada*, para evitar una predisposición hacia los documentos largos. Por ejemplo, se divide la frecuencia bruta por la frecuencia máxima de algún término en el documento:\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\mathrm {tf} (t,d)={\\frac {\\mathrm {f} (t,d)}{\\max\\{\\mathrm {f} (t,d):t\\in d\\}}}}\n",
    "$$\n",
    "\n",
    "La frecuencia inversa de documento es una medida de si el término es común o no, en el corpus de documentos. Se obtiene dividiendo el número total de documentos por el número de documentos que contienen el término, y se toma el logaritmo de ese cociente:\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\mathrm {idf} (t,D)=\\log {\\frac {|D|}{|\\{d\\in D:t\\in d\\}|}}}\n",
    "$$\n",
    "\n",
    "donde\n",
    "\n",
    "- ${\\displaystyle |D|}$: cardinalidad de $D$, o número de documentos en el corpus.\n",
    "- ${\\displaystyle |\\{d\\in D:t\\in d\\}|}$ : número de documentos donde aparece el término $t$. Si el término no está en la colección se producirá una división-por-cero. Por lo tanto, es común ajustar esta fórmula a ${\\displaystyle 1+|\\{d\\in D:t\\in d\\}|}$.\n",
    "\n",
    "Matemáticamente, la base de la función logaritmo no es importante y constituye un factor constante en el resultado final.\n",
    "\n",
    "Luego, *tf-idf* se calcula como:\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\text{tf-idf} (t,d,D)=\\mathrm {tf} (t,d)\\times \\mathrm {idf} (t,D)}\n",
    "$$\n",
    "\n",
    "Un peso alto en *tf-idf* se alcanza con una elevada frecuencia de término (en el documento dado) y una pequeña frecuencia de ocurrencia del término en corpus de documentos. \n",
    "\n",
    "Como el cociente dentro de la función logaritmo del idf es siempre mayor o igual que 1, el valor del *idf* (y del *tf-idf*) es mayor o igual que 0. \n",
    "\n",
    "Cuando un término aparece en muchos documentos, el cociente dentro del logaritmo se acerca a 1, ofreciendo un valor de *idf* y de *tf-idf* cercano a 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Semántica latente</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta técnica es quizas de las primeras aparecidas en el anális de textos. La idea central es la construcción de análisis de componentes principales (ACP) seguida de un proceso de clasificación automática.\n",
    "\n",
    "Las componentes principales del ACP, que son construidas a partir de combinaciones lineales de las columnas de los términos se denominan las **componentes léxicas del corpus de datos**. \n",
    "\n",
    "Las herramientas habituales para la interpretación del ACP permiten determinar o mejor asignar un contenido semántico a cada componente. \n",
    "\n",
    "En consecuencia, es posible determinar las temáticas presentes en el corpus de textos, a partir de los ejes semánticos.\n",
    "\n",
    "\n",
    "Como es habitual en el ACP, una clasificación automática puede ser obtenida a partir de la representación factorial de la dtm.\n",
    "\n",
    "El siguente gráfico ilustra la técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/pca.png\" width=\"500\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Arquitectura del modelo Semática Latente</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: \n",
    "Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Básicamente lo que se hace es una proyección lineal desde el espacio vectores dispersos al espacio Euclideano. Modernamente se ha encontrado que tiene bastante error, básicamente por el tratamiento lineal. \n",
    "\n",
    "En general, se ha encontrado que estas técnicas permiten un primer acercamiento al descubrimieontos de las tématicas (tópicos), pero que en general se quedan cortas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Modelos generativos: Latent Dirichlet Allocation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La técnica Latent Dirichlet Allocation (LDA) es la más utilizada actualmente para la extracción de toṕicos de corpus de documentos y se debe a [Blei et al](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Las ideas centrales detrás de LDA, Blei et al.(2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las ideas centrales detrás de LDA son las siguientes. El modelo generativo supone que los documentos son gnerados como sigue:\n",
    "\n",
    "1. El tamaño $N$ del documento es generado por una distribución de Poisson $\\text{Poi}(\\xi)$.\n",
    "2. Los tópicos son generados a partir de una distribución multinomial con vector de probabilidades $\\mathbf{\\theta}$. \n",
    "3. A priori se asume que que el vector $\\mathbf{\\theta}$ es generado por una distribución de Dirichlet con vector de parámetros $\\boldsymbol{\\alpha}$. De aquí deriva el nombre de la técnica.\n",
    "4. Cada una de las $N$ palabras en un documentos es generada según el siguiente algoritmo.\n",
    "     - Se escoge un tópico $z_n \\sim \\text{Multinomial}(\\mathbf{\\theta})$.\n",
    "     - Se escoge la palabra $w_n \\sim \\text{P}(w_n|z_n,\\mathbf{\\beta})$. En donde $\\mathbf{\\beta}$ es una matriz de probabilidades de pertenencia de las palabras a los tópicos. $P$ es una probabilidad multinomial condicionada al tópico $z_n$ y al vector de parámetros $\\mathbf{\\beta}$.\n",
    "\n",
    "\n",
    "Al lector interesado en los detalles, lo remitimos al paper original de [Blei et al.](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente imagen intenta mostrar las ideas centrales detras  de la técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Diagram_Blei.png\" width=\"800\" height=\"700\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Intuición detrás de LDA</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: \n",
    "[Intuition behind LDA](http://www.cs.cornell.edu/courses/cs6784/2010sp/lecture/30-BleiEtAl03.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelado de temas (topic modeling) es un tipo de modelado estadístico para descubrir los “temas” abstractos que ocurren en una colección de documentos. La asignación de Dirichlet latente (LDA) es un ejemplo de modelo de tema y se utiliza para clasificar el texto de un documento en un tema en particular. \n",
    "\n",
    "Construye un modelo de tema por documento y palabras por modelo de tema, modelado como distribuciones de Dirichlet.\n",
    "\n",
    "Aquí vamos a aplicar LDA a un conjunto de documentos y dividirlos en temas. ¡Empecemos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "#from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a escribir una función que lematiza y hace el preprocesamintos del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    return ps.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text): #  gensim.utils.simple_preprocess tokeniza el texto\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Ejemplo: Un millón de titulares</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos que usaremos es una lista de más de un millón de titulares de noticias publicados durante un período de 15 años y se puede descargar de [Kaggle](https://www.kaggle.com/therohk/million-headlines/metadata)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo adaptado de [Topic Modeling and Latent Dirichlet Allocation (LDA) in Python](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../Datos/abcnews-date-text.csv', on_bad_lines='warn');\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos algunos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1244184\n",
      "                                       headline_text  index\n",
      "0  aba decides against community broadcasting lic...      0\n",
      "1     act fire witnesses must be aware of defamation      1\n",
      "2     a g calls for infrastructure protection summit      2\n",
      "3           air nz staff in aust strike for pay rise      3\n",
      "4      air nz strike to affect australian travellers      4\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documento original: \n",
      "['ratepayers', 'group', 'wants', 'compulsory', 'local', 'govt', 'voting']\n",
      "\n",
      "\n",
      " documento tokenizado y lematizado: \n",
      "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "print('documento original: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n documento tokenizado y lematizado: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Vamos a procesar previamente los textos, guardando los resultados en el objeto *processed_docs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [decid, commun, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción del diccionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un diccionario a partir de *processed_docs* que contenga la cantidad de veces que aparece una palabra en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 commun\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "#.iteritems() method allows you to iterate over the dictionary, \n",
    "# where 'k' represents the word (key), and 'v' represents the corresponding integer ID (value).\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtra los tokens que aparecen en\n",
    "menos de 15 documentos (número absoluto) o\n",
    "más de 0,5 documentos (fracción del tamaño total del corpus, no número absoluto).\n",
    "después de los dos pasos anteriores, conserve solo los primeros 100000 tokens más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim doc2bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada documento creamos un diccionario que informa cuántos\n",
    "palabras y cuántas veces aparecen esas palabras. \n",
    "\n",
    "Colocamos esto en el objeto *bow_corpus*, luego verifique nuestro documento seleccionado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(162, 1), (240, 1), (292, 1), (589, 1), (839, 1), (3579, 1), (3580, 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esta es una vista preliminar de la bolsa de palabras del documento preprocesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 162 (\"govt\") appears 1 time.\n",
      "Word 240 (\"group\") appears 1 time.\n",
      "Word 292 (\"vote\") appears 1 time.\n",
      "Word 589 (\"local\") appears 1 time.\n",
      "Word 839 (\"want\") appears 1 time.\n",
      "Word 3579 (\"compulsori\") appears 1 time.\n",
      "Word 3580 (\"ratepay\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un objeto modelo *tf-idf* usando *models.TfidfModel* a partir de  \"bow_corpus\" y lo colocamos en *tfidf*, luego aplicamos la transformación a todo el corpus y lo llámamos *corpus_tfidf*. Finalmente, obtenemos una vista previa de las puntuaciones *TF-IDF* para nuestro primer documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5854395661274623),\n",
      " (1, 0.383252758688686),\n",
      " (2, 0.50230806644029),\n",
      " (3, 0.5080004367704987)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corriendo LDA usando la bolsa de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a entrener anuestro modelo LDA usando *gensim.models.LdaMulticore* and save it to ‘lda_model’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    lda_model: This is a variable that will store the trained LDA model.\n",
    "\n",
    "    gensim.models.LdaMulticore: This part of the code creates an LDA model using Gensim. LdaMulticore is a variant of LDA that utilizes multiple CPU cores to speed up training. It's suitable for large corpora or when you want to parallelize the computation.\n",
    "\n",
    "    bow_corpus: This is the Bag of Words representation of your documents. It's a corpus where each document is represented as a list of word IDs with their corresponding word counts (BoW format).\n",
    "\n",
    "    num_topics=10: This parameter specifies the number of topics the model should identify in the corpus. In this case, the LDA model will try to find 10 topics within the documents.\n",
    "\n",
    "    id2word=dictionary: This parameter links the LDA model to the Gensim dictionary created earlier. The dictionary maps words to unique integer IDs, and the LDA model uses this mapping to understand the words in the BoW representation.\n",
    "\n",
    "    passes=2: This parameter specifies the number of passes (iterations) over the entire corpus. In this case, the LDA model will make two passes over the corpus during training.\n",
    "\n",
    "    workers=2: This parameter indicates the number of CPU cores to use during training. In this example, it's set to 2, meaning that the training process will utilize two CPU cores if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Para cada tópico, exploraremos las palabras que ocurren en ese tema y su peso relativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    lda_model.print_topics(-1): The print_topics() method of the LDA model is used to retrieve a list of topics and their associated words. The -1 argument indicates that you want to retrieve information about all topics. Each topic is represented as a list of words with their associated probabilities.\n",
    "\n",
    "    idx and topic: In the loop, idx represents the topic index, and topic represents the list of words associated with that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.027*\"news\" + 0.023*\"market\" + 0.018*\"hospit\" + 0.017*\"morrison\" + 0.016*\"work\" + 0.015*\"fight\" + 0.013*\"countri\" + 0.013*\"close\" + 0.012*\"fall\" + 0.012*\"darwin\"\n",
      "Topic: 1 \n",
      "Words: 0.051*\"polic\" + 0.035*\"case\" + 0.025*\"charg\" + 0.025*\"court\" + 0.021*\"death\" + 0.020*\"murder\" + 0.018*\"face\" + 0.016*\"restrict\" + 0.014*\"trial\" + 0.013*\"investig\"\n",
      "Topic: 2 \n",
      "Words: 0.031*\"queensland\" + 0.027*\"live\" + 0.021*\"women\" + 0.018*\"victorian\" + 0.014*\"andrew\" + 0.012*\"need\" + 0.012*\"deal\" + 0.011*\"budget\" + 0.011*\"rural\" + 0.011*\"road\"\n",
      "Topic: 3 \n",
      "Words: 0.057*\"covid\" + 0.046*\"trump\" + 0.029*\"vaccin\" + 0.021*\"bushfir\" + 0.019*\"test\" + 0.015*\"australia\" + 0.013*\"say\" + 0.013*\"home\" + 0.012*\"support\" + 0.011*\"hotel\"\n",
      "Topic: 4 \n",
      "Words: 0.030*\"victoria\" + 0.019*\"world\" + 0.017*\"coast\" + 0.016*\"south\" + 0.016*\"australia\" + 0.015*\"peopl\" + 0.015*\"sydney\" + 0.015*\"canberra\" + 0.013*\"alleg\" + 0.013*\"busi\"\n",
      "Topic: 5 \n",
      "Words: 0.032*\"elect\" + 0.020*\"say\" + 0.017*\"minist\" + 0.015*\"miss\" + 0.014*\"speak\" + 0.013*\"labor\" + 0.013*\"care\" + 0.013*\"call\" + 0.011*\"claim\" + 0.011*\"age\"\n",
      "Topic: 6 \n",
      "Words: 0.075*\"coronaviru\" + 0.027*\"china\" + 0.021*\"kill\" + 0.018*\"die\" + 0.017*\"covid\" + 0.015*\"australia\" + 0.014*\"return\" + 0.013*\"jail\" + 0.013*\"crash\" + 0.013*\"attack\"\n",
      "Topic: 7 \n",
      "Words: 0.023*\"donald\" + 0.020*\"year\" + 0.018*\"chang\" + 0.015*\"lockdown\" + 0.013*\"school\" + 0.013*\"warn\" + 0.012*\"high\" + 0.012*\"travel\" + 0.011*\"tasmanian\" + 0.011*\"price\"\n",
      "Topic: 8 \n",
      "Words: 0.033*\"govern\" + 0.018*\"border\" + 0.015*\"plan\" + 0.015*\"commun\" + 0.014*\"indigen\" + 0.014*\"water\" + 0.013*\"region\" + 0.013*\"concern\" + 0.011*\"industri\" + 0.011*\"local\"\n",
      "Topic: 9 \n",
      "Words: 0.026*\"australia\" + 0.023*\"open\" + 0.016*\"final\" + 0.015*\"australian\" + 0.015*\"time\" + 0.012*\"lose\" + 0.012*\"tell\" + 0.011*\"releas\" + 0.010*\"pandem\" + 0.010*\"win\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Es posible distinguir diferentes temas usando las palabras en cada tema y sus pesos correspondientes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corriendo  LDA usando TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.015*\"govern\" + 0.013*\"elect\" + 0.010*\"morrison\" + 0.008*\"live\" + 0.007*\"feder\" + 0.007*\"age\" + 0.007*\"care\" + 0.007*\"budget\" + 0.007*\"labor\" + 0.006*\"say\"\n",
      "Topic: 1 Word: 0.014*\"interview\" + 0.010*\"tuesday\" + 0.009*\"mark\" + 0.009*\"david\" + 0.008*\"daniel\" + 0.007*\"know\" + 0.007*\"extend\" + 0.006*\"septemb\" + 0.006*\"appeal\" + 0.006*\"court\"\n",
      "Topic: 2 Word: 0.020*\"news\" + 0.017*\"market\" + 0.012*\"rural\" + 0.010*\"price\" + 0.009*\"coronaviru\" + 0.008*\"share\" + 0.008*\"scott\" + 0.008*\"stori\" + 0.008*\"busi\" + 0.008*\"covid\"\n",
      "Topic: 3 Word: 0.009*\"queensland\" + 0.008*\"drought\" + 0.006*\"mental\" + 0.006*\"august\" + 0.006*\"victoria\" + 0.006*\"univers\" + 0.006*\"onlin\" + 0.006*\"coal\" + 0.005*\"liber\" + 0.005*\"health\"\n",
      "Topic: 4 Word: 0.011*\"coronaviru\" + 0.011*\"countri\" + 0.010*\"covid\" + 0.010*\"vaccin\" + 0.010*\"restrict\" + 0.008*\"hour\" + 0.008*\"bushfir\" + 0.007*\"climat\" + 0.007*\"chang\" + 0.006*\"thursday\"\n",
      "Topic: 5 Word: 0.011*\"drum\" + 0.010*\"lockdown\" + 0.010*\"australia\" + 0.009*\"world\" + 0.009*\"final\" + 0.007*\"leagu\" + 0.007*\"south\" + 0.007*\"michael\" + 0.007*\"wednesday\" + 0.006*\"open\"\n",
      "Topic: 6 Word: 0.011*\"royal\" + 0.010*\"andrew\" + 0.009*\"kill\" + 0.009*\"commiss\" + 0.006*\"indonesia\" + 0.005*\"china\" + 0.005*\"attack\" + 0.005*\"jam\" + 0.005*\"australian\" + 0.005*\"protest\"\n",
      "Topic: 7 Word: 0.019*\"polic\" + 0.015*\"charg\" + 0.014*\"crash\" + 0.014*\"woman\" + 0.011*\"murder\" + 0.009*\"shoot\" + 0.009*\"death\" + 0.008*\"assault\" + 0.008*\"arrest\" + 0.007*\"miss\"\n",
      "Topic: 8 Word: 0.028*\"trump\" + 0.017*\"donald\" + 0.012*\"coronaviru\" + 0.009*\"covid\" + 0.008*\"weather\" + 0.008*\"friday\" + 0.008*\"quarantin\" + 0.008*\"victorian\" + 0.007*\"sport\" + 0.007*\"zealand\"\n",
      "Topic: 9 Word: 0.013*\"border\" + 0.012*\"coronaviru\" + 0.010*\"updat\" + 0.008*\"covid\" + 0.007*\"hill\" + 0.007*\"social\" + 0.006*\"grandstand\" + 0.006*\"footag\" + 0.006*\"june\" + 0.006*\"reopen\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, ¿podemos distinguir diferentes temas usando las palabras en cada tema y sus pesos correspondientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del desempeño clasificando el documento de muestra usando el modelo LDA de bolsa de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobaremos dónde se clasificaría nuestro documento de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "\n",
    "    lda_model[bow_corpus[4310]]: This part of the code applies the LDA model to a specific document represented as a Bag of Words (BoW) vector. bow_corpus[4310] refers to the BoW representation of the 4310th document in your corpus.\n",
    "\n",
    "    sorted(..., key=lambda tup: -1*tup[1]): The sorted() function is used to sort the topics by their scores in descending order. The key parameter is set to a lambda function that takes a tuple tup and sorts based on the negative of the second element (tup[1]). This results in sorting topics by their scores in descending order.\n",
    "\n",
    "    for index, score in ...: This loop iterates through the sorted topics and their associated scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8874766230583191\t \n",
      "Topic: 0.033*\"govern\" + 0.018*\"border\" + 0.015*\"plan\" + 0.015*\"commun\" + 0.014*\"indigen\" + 0.014*\"water\" + 0.013*\"region\" + 0.013*\"concern\" + 0.011*\"industri\" + 0.011*\"local\"\n",
      "\n",
      "Score: 0.01250580232590437\t \n",
      "Topic: 0.032*\"elect\" + 0.020*\"say\" + 0.017*\"minist\" + 0.015*\"miss\" + 0.014*\"speak\" + 0.013*\"labor\" + 0.013*\"care\" + 0.013*\"call\" + 0.011*\"claim\" + 0.011*\"age\"\n",
      "\n",
      "Score: 0.012502484954893589\t \n",
      "Topic: 0.023*\"donald\" + 0.020*\"year\" + 0.018*\"chang\" + 0.015*\"lockdown\" + 0.013*\"school\" + 0.013*\"warn\" + 0.012*\"high\" + 0.012*\"travel\" + 0.011*\"tasmanian\" + 0.011*\"price\"\n",
      "\n",
      "Score: 0.01250243280082941\t \n",
      "Topic: 0.057*\"covid\" + 0.046*\"trump\" + 0.029*\"vaccin\" + 0.021*\"bushfir\" + 0.019*\"test\" + 0.015*\"australia\" + 0.013*\"say\" + 0.013*\"home\" + 0.012*\"support\" + 0.011*\"hotel\"\n",
      "\n",
      "Score: 0.012502348981797695\t \n",
      "Topic: 0.031*\"queensland\" + 0.027*\"live\" + 0.021*\"women\" + 0.018*\"victorian\" + 0.014*\"andrew\" + 0.012*\"need\" + 0.012*\"deal\" + 0.011*\"budget\" + 0.011*\"rural\" + 0.011*\"road\"\n",
      "\n",
      "Score: 0.01250219251960516\t \n",
      "Topic: 0.051*\"polic\" + 0.035*\"case\" + 0.025*\"charg\" + 0.025*\"court\" + 0.021*\"death\" + 0.020*\"murder\" + 0.018*\"face\" + 0.016*\"restrict\" + 0.014*\"trial\" + 0.013*\"investig\"\n",
      "\n",
      "Score: 0.012502162717282772\t \n",
      "Topic: 0.027*\"news\" + 0.023*\"market\" + 0.018*\"hospit\" + 0.017*\"morrison\" + 0.016*\"work\" + 0.015*\"fight\" + 0.013*\"countri\" + 0.013*\"close\" + 0.012*\"fall\" + 0.012*\"darwin\"\n",
      "\n",
      "Score: 0.012501981109380722\t \n",
      "Topic: 0.030*\"victoria\" + 0.019*\"world\" + 0.017*\"coast\" + 0.016*\"south\" + 0.016*\"australia\" + 0.015*\"peopl\" + 0.015*\"sydney\" + 0.015*\"canberra\" + 0.013*\"alleg\" + 0.013*\"busi\"\n",
      "\n",
      "Score: 0.012501980178058147\t \n",
      "Topic: 0.075*\"coronaviru\" + 0.027*\"china\" + 0.021*\"kill\" + 0.018*\"die\" + 0.017*\"covid\" + 0.015*\"australia\" + 0.014*\"return\" + 0.013*\"jail\" + 0.013*\"crash\" + 0.013*\"attack\"\n",
      "\n",
      "Score: 0.012501980178058147\t \n",
      "Topic: 0.026*\"australia\" + 0.023*\"open\" + 0.016*\"final\" + 0.015*\"australian\" + 0.015*\"time\" + 0.012*\"lose\" + 0.012*\"tell\" + 0.011*\"releas\" + 0.010*\"pandem\" + 0.010*\"win\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro documento de prueba tiene la mayor probabilidad de ser parte del tema que asignó nuestro modelo, que es la clasificación precisa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del desempeño clasificando el documento de muestra usando el modelo LDA TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7453249096870422\t \n",
      "Topic: 0.011*\"coronaviru\" + 0.011*\"countri\" + 0.010*\"covid\" + 0.010*\"vaccin\" + 0.010*\"restrict\" + 0.008*\"hour\" + 0.008*\"bushfir\" + 0.007*\"climat\" + 0.007*\"chang\" + 0.006*\"thursday\"\n",
      "\n",
      "Score: 0.15462540090084076\t \n",
      "Topic: 0.014*\"interview\" + 0.010*\"tuesday\" + 0.009*\"mark\" + 0.009*\"david\" + 0.008*\"daniel\" + 0.007*\"know\" + 0.007*\"extend\" + 0.006*\"septemb\" + 0.006*\"appeal\" + 0.006*\"court\"\n",
      "\n",
      "Score: 0.01250949315726757\t \n",
      "Topic: 0.015*\"govern\" + 0.013*\"elect\" + 0.010*\"morrison\" + 0.008*\"live\" + 0.007*\"feder\" + 0.007*\"age\" + 0.007*\"care\" + 0.007*\"budget\" + 0.007*\"labor\" + 0.006*\"say\"\n",
      "\n",
      "Score: 0.012507026083767414\t \n",
      "Topic: 0.020*\"news\" + 0.017*\"market\" + 0.012*\"rural\" + 0.010*\"price\" + 0.009*\"coronaviru\" + 0.008*\"share\" + 0.008*\"scott\" + 0.008*\"stori\" + 0.008*\"busi\" + 0.008*\"covid\"\n",
      "\n",
      "Score: 0.012505996972322464\t \n",
      "Topic: 0.013*\"border\" + 0.012*\"coronaviru\" + 0.010*\"updat\" + 0.008*\"covid\" + 0.007*\"hill\" + 0.007*\"social\" + 0.006*\"grandstand\" + 0.006*\"footag\" + 0.006*\"june\" + 0.006*\"reopen\"\n",
      "\n",
      "Score: 0.012505937367677689\t \n",
      "Topic: 0.028*\"trump\" + 0.017*\"donald\" + 0.012*\"coronaviru\" + 0.009*\"covid\" + 0.008*\"weather\" + 0.008*\"friday\" + 0.008*\"quarantin\" + 0.008*\"victorian\" + 0.007*\"sport\" + 0.007*\"zealand\"\n",
      "\n",
      "Score: 0.012505920603871346\t \n",
      "Topic: 0.009*\"queensland\" + 0.008*\"drought\" + 0.006*\"mental\" + 0.006*\"august\" + 0.006*\"victoria\" + 0.006*\"univers\" + 0.006*\"onlin\" + 0.006*\"coal\" + 0.005*\"liber\" + 0.005*\"health\"\n",
      "\n",
      "Score: 0.01250588521361351\t \n",
      "Topic: 0.011*\"royal\" + 0.010*\"andrew\" + 0.009*\"kill\" + 0.009*\"commiss\" + 0.006*\"indonesia\" + 0.005*\"china\" + 0.005*\"attack\" + 0.005*\"jam\" + 0.005*\"australian\" + 0.005*\"protest\"\n",
      "\n",
      "Score: 0.012504743412137032\t \n",
      "Topic: 0.011*\"drum\" + 0.010*\"lockdown\" + 0.010*\"australia\" + 0.009*\"world\" + 0.009*\"final\" + 0.007*\"leagu\" + 0.007*\"south\" + 0.007*\"michael\" + 0.007*\"wednesday\" + 0.006*\"open\"\n",
      "\n",
      "Score: 0.01250470895320177\t \n",
      "Topic: 0.019*\"polic\" + 0.015*\"charg\" + 0.014*\"crash\" + 0.014*\"woman\" + 0.011*\"murder\" + 0.009*\"shoot\" + 0.009*\"death\" + 0.008*\"assault\" + 0.008*\"arrest\" + 0.007*\"miss\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro documento de prueba tiene la mayor probabilidad de ser parte del tema que asignó nuestro modelo, que es la clasificación precisa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del modelo con un documento no visto antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.447002112865448\t Topic: 0.032*\"elect\" + 0.020*\"say\" + 0.017*\"minist\" + 0.015*\"miss\" + 0.014*\"speak\"\n",
      "Score: 0.41961172223091125\t Topic: 0.023*\"donald\" + 0.020*\"year\" + 0.018*\"chang\" + 0.015*\"lockdown\" + 0.013*\"school\"\n",
      "Score: 0.01667659915983677\t Topic: 0.030*\"victoria\" + 0.019*\"world\" + 0.017*\"coast\" + 0.016*\"south\" + 0.016*\"australia\"\n",
      "Score: 0.01667509786784649\t Topic: 0.031*\"queensland\" + 0.027*\"live\" + 0.021*\"women\" + 0.018*\"victorian\" + 0.014*\"andrew\"\n",
      "Score: 0.01667420007288456\t Topic: 0.075*\"coronaviru\" + 0.027*\"china\" + 0.021*\"kill\" + 0.018*\"die\" + 0.017*\"covid\"\n",
      "Score: 0.016673114150762558\t Topic: 0.033*\"govern\" + 0.018*\"border\" + 0.015*\"plan\" + 0.015*\"commun\" + 0.014*\"indigen\"\n",
      "Score: 0.016672847792506218\t Topic: 0.057*\"covid\" + 0.046*\"trump\" + 0.029*\"vaccin\" + 0.021*\"bushfir\" + 0.019*\"test\"\n",
      "Score: 0.016672180965542793\t Topic: 0.027*\"news\" + 0.023*\"market\" + 0.018*\"hospit\" + 0.017*\"morrison\" + 0.016*\"work\"\n",
      "Score: 0.016671841964125633\t Topic: 0.026*\"australia\" + 0.023*\"open\" + 0.016*\"final\" + 0.015*\"australian\" + 0.015*\"time\"\n",
      "Score: 0.016670262441039085\t Topic: 0.051*\"polic\" + 0.035*\"case\" + 0.025*\"charg\" + 0.025*\"court\" + 0.021*\"death\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'Melbourne man Suat Bayram dies in Türkiye earthquake, fears for other Australians'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8499851822853088\t Topic: 0.075*\"coronaviru\" + 0.027*\"china\" + 0.021*\"kill\" + 0.018*\"die\" + 0.017*\"covid\"\n",
      "Score: 0.01666956953704357\t Topic: 0.030*\"victoria\" + 0.019*\"world\" + 0.017*\"coast\" + 0.016*\"south\" + 0.016*\"australia\"\n",
      "Score: 0.016669245436787605\t Topic: 0.026*\"australia\" + 0.023*\"open\" + 0.016*\"final\" + 0.015*\"australian\" + 0.015*\"time\"\n",
      "Score: 0.016668954864144325\t Topic: 0.023*\"donald\" + 0.020*\"year\" + 0.018*\"chang\" + 0.015*\"lockdown\" + 0.013*\"school\"\n",
      "Score: 0.01666867360472679\t Topic: 0.027*\"news\" + 0.023*\"market\" + 0.018*\"hospit\" + 0.017*\"morrison\" + 0.016*\"work\"\n",
      "Score: 0.016668150201439857\t Topic: 0.057*\"covid\" + 0.046*\"trump\" + 0.029*\"vaccin\" + 0.021*\"bushfir\" + 0.019*\"test\"\n",
      "Score: 0.016668127849698067\t Topic: 0.051*\"polic\" + 0.035*\"case\" + 0.025*\"charg\" + 0.025*\"court\" + 0.021*\"death\"\n",
      "Score: 0.0166675616055727\t Topic: 0.032*\"elect\" + 0.020*\"say\" + 0.017*\"minist\" + 0.015*\"miss\" + 0.014*\"speak\"\n",
      "Score: 0.016667475923895836\t Topic: 0.031*\"queensland\" + 0.027*\"live\" + 0.021*\"women\" + 0.018*\"victorian\" + 0.014*\"andrew\"\n",
      "Score: 0.016667088493704796\t Topic: 0.033*\"govern\" + 0.018*\"border\" + 0.015*\"plan\" + 0.015*\"commun\" + 0.014*\"indigen\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'Melbourne man Suat Bayram dies in Türkiye earthquake, fears for other Australians'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Blei et al.,[Latent Dirichlet Allocation, 2003](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
    "2. [Topic Modeling and Latent Dirichlet Allocation (LDA) in Python, 2018](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24), en Toward data science.\n",
    "3. https://www.youtube.com/watch?v=T05t-SqKArY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
