{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Modelado de Temas con LDA",
   "id": "214fdd3e41d9e1d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Instalación de Librerías",
   "id": "7a4e4415acf177cf"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-01T15:53:58.061869Z",
     "start_time": "2025-06-01T15:53:51.946010Z"
    }
   },
   "source": "!pip install pandas gensim spacy nltk",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\programdata\\miniconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: gensim in c:\\programdata\\miniconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: spacy in c:\\programdata\\miniconda3\\lib\\site-packages (3.8.5)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\miniconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from gensim) (1.13.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (0.15.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (2.11.4)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (71.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\miniconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\miniconda3\\lib\\site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\miniconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\miniconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\miniconda3\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Implementación LDA",
   "id": "d8198c2df8708582"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T16:26:33.741317Z",
     "start_time": "2025-06-01T16:26:33.727594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS as GENSIM_STOP\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Descargas necesarias (solo la primera vez)\n",
    "# -------------------------------------------------\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Definir stopwords en español y stemmer\n",
    "nltk_stop = set(stopwords.words('spanish'))\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Función de preprocesado (tokenización + stemming)\n",
    "# -------------------------------------------------\n",
    "def preprocess(texto: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    1) Convierte a minúsculas y tokeniza quitando puntuación y acentos (deacc=True).\n",
    "    2) Filtra tokens de longitud <= 3.\n",
    "    3) Elimina stopwords de gensim y NLTK.\n",
    "    4) Aplica stemming en español.\n",
    "    Devuelve la lista de stems resultantes.\n",
    "    \"\"\"\n",
    "    tokens = simple_preprocess(texto, deacc=True)\n",
    "    tokens = [t for t in tokens if len(t) > 3 and t not in GENSIM_STOP and t not in nltk_stop]\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. (Opcional) Generación de bigramas\n",
    "# -------------------------------------------------\n",
    "def aplicar_bigramas(\n",
    "        documentos: list[list[str]],\n",
    "        min_count: int = 5,\n",
    "        threshold: float = 100.0\n",
    ") -> tuple[list[list[str]], Phraser]:\n",
    "    \"\"\"\n",
    "    1) Entrena un modelo de bigramas sobre los documentos tokenizados.\n",
    "    2) Devuelve los documentos enriquecidos con bigramas (uni_bi) y el modelo Phraser.\n",
    "    Parámetros:\n",
    "      - min_count: número mínimo de apariciones conjuntas para considerar bigrama.\n",
    "      - threshold: umbral de formación de bigramas (valor alto = más conservador).\n",
    "    \"\"\"\n",
    "    bigram = Phrases(documentos, min_count=min_count, threshold=threshold, progress_per=10000)\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    documentos_bi = [bigram_mod[doc] for doc in documentos]\n",
    "    return documentos_bi, bigram_mod\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Construcción de diccionario y corpus (BoW)\n",
    "# -------------------------------------------------\n",
    "def construir_diccionario_corpus(\n",
    "        textos_tokenizados: list[list[str]],\n",
    "        no_below: int = 5,\n",
    "        no_above: float = 0.5\n",
    ") -> tuple[corpora.Dictionary, list]:\n",
    "    \"\"\"\n",
    "    1) Crea un Dictionary a partir de la lista de documentos (lista de tokens).\n",
    "    2) Filtra palabras muy raras (no_below) o demasiado frecuentes (no_above).\n",
    "    3) Construye el corpus en formato BoW (lista de tuplas (id_token, frecuencia)).\n",
    "    Devuelve el diccionario filtrado y el corpus BoW.\n",
    "    \"\"\"\n",
    "    diccionario = corpora.Dictionary(textos_tokenizados)\n",
    "    diccionario.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    corpus = [diccionario.doc2bow(doc) for doc in textos_tokenizados]\n",
    "    return diccionario, corpus\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. Entrenamiento y evaluación del modelo LDA\n",
    "# -------------------------------------------------\n",
    "def entrenar_lda(\n",
    "        corpus: list,\n",
    "        diccionario: corpora.Dictionary,\n",
    "        num_topics: int = 7,\n",
    "        passes: int = 10,\n",
    "        random_state: int = 42,\n",
    "        alpha: str = 'auto'\n",
    ") -> LdaModel:\n",
    "    \"\"\"\n",
    "    Entrena un modelo LDA con los parámetros indicados y devuelve el objeto LdaModel.\n",
    "    \"\"\"\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=diccionario,\n",
    "        num_topics=num_topics,\n",
    "        random_state=random_state,\n",
    "        passes=passes,\n",
    "        alpha=alpha,\n",
    "        per_word_topics=True\n",
    "    )\n",
    "    return lda_model\n",
    "\n",
    "\n",
    "def evaluar_lda(\n",
    "        lda_model: LdaModel,\n",
    "        corpus: list,\n",
    "        textos_tokenizados: list[list[str]],\n",
    "        diccionario: corpora.Dictionary\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    1) Calcula la perplejidad del modelo sobre el corpus.\n",
    "    2) Calcula la coherencia (c_v) usando CoherenceModel.\n",
    "    Devuelve (perplejidad, coherencia).\n",
    "    \"\"\"\n",
    "    perplejidad = lda_model.log_perplexity(corpus)\n",
    "    coh_model = CoherenceModel(\n",
    "        model=lda_model,\n",
    "        texts=textos_tokenizados,\n",
    "        dictionary=diccionario,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    coherencia = coh_model.get_coherence()\n",
    "    return perplejidad, coherencia\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. Asignación de tópico dominante a cada documento\n",
    "# -------------------------------------------------\n",
    "def asignar_topic_principal(lda_model: LdaModel, corpus: list) -> list[int]:\n",
    "    \"\"\"\n",
    "    Para cada documento en formato BoW, obtiene la lista de tópicos con sus probabilidades\n",
    "    y retorna el ID del tópico con mayor probabilidad (dominante).\n",
    "    \"\"\"\n",
    "    topicos_principales = []\n",
    "    for bow in corpus:\n",
    "        distribucion = lda_model.get_document_topics(bow, minimum_probability=0.0)\n",
    "        topico_dom = max(distribucion, key=lambda x: x[1])[0]\n",
    "        topicos_principales.append(topico_dom)\n",
    "    return topicos_principales\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7. Leer JSON y preparar DataFrame con 'description' y 'category'\n",
    "# -------------------------------------------------\n",
    "def cargar_descriptions_desde_json(ruta_json: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lee un archivo JSON con estructura de lista de objetos:\n",
    "    [\n",
    "      {\n",
    "        \"title\": \"...\",\n",
    "        \"category\": \"...\",\n",
    "        \"summit\": \"...\",\n",
    "        \"description\": \"...\",\n",
    "        \"date\": \"...\",\n",
    "        \"autor\": \"...\",\n",
    "        \"tags\": \"['Seguro Social', 'Estados Unidos']\",\n",
    "        \"url\": \"...\"\n",
    "      },\n",
    "      ...\n",
    "    ]\n",
    "    Extrae las columnas 'description' y 'category', elimina filas con description vacía.\n",
    "    Devuelve un DataFrame con dichas columnas.\n",
    "    \"\"\"\n",
    "    # Cargar JSON en un DataFrame\n",
    "    df = pd.read_json(ruta_json, orient='records', encoding='utf-8')\n",
    "    # Filtrar filas donde 'description' exista y no esté vacío\n",
    "    df = df[df['description'].notna() & (df['description'].str.strip() != \"\")].reset_index(drop=True)\n",
    "    return df[['description', 'category']]"
   ],
   "id": "1a8e6c8b4d585da0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Identificación de 7 tópicos mediante LDA y evaluación del modelo",
   "id": "9b423727d1f7990d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T16:29:14.135522Z",
     "start_time": "2025-06-01T16:28:48.691827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------------------------------\n",
    "# 8. Función principal: pipeline completo adaptado a JSON\n",
    "# Parámetros ajustables\n",
    "RUTA_JSON = \"gestionspider5.json\"        # Nombre del JSON a leer\n",
    "NUM_TOPICS = 7                     # Número de tópicos para LDA\n",
    "PASSES = 10                        # Número de pasadas (iteraciones) en el entrenamiento LDA\n",
    "NO_BELOW = 5                       # Filtrar tokens que aparezcan en menos de NO_BELOW documentos\n",
    "NO_ABOVE = 0.5                     # Filtrar tokens que aparezcan en más del 50% de documentos\n",
    "BIGRAM = True                      # Si quieres incluir n-gramas (True o False)\n",
    "BIGRAM_MIN_COUNT = 5               # Mínimo de apariciones para formar un bigrama\n",
    "BIGRAM_THRESHOLD = 100.0           # Umbral de formación de bigramas\n",
    "\n",
    "# 1) Leer el JSON y extraer 'description' y 'category'\n",
    "df = cargar_descriptions_desde_json(RUTA_JSON)\n",
    "print(f\"Cargados {len(df)} registros desde '{RUTA_JSON}'.\\n\")\n",
    "\n",
    "# 2) Preprocesar los textos en 'description'\n",
    "textos_raw = df['description'].astype(str).tolist()\n",
    "documentos_tokenizados = []\n",
    "print(\"Preprocesando descripciones (tokenización + stemming)...\")\n",
    "for doc in tqdm(textos_raw, desc=\"Progreso preprocesado\"):\n",
    "    documentos_tokenizados.append(preprocess(doc))\n",
    "\n",
    "# 3) (Opcional) Generar e incluir bigramas\n",
    "if BIGRAM:\n",
    "    print(\"\\nDetectando bigramas en el corpus...\")\n",
    "    documentos_bi, modelo_bigram = aplicar_bigramas(\n",
    "        documentos_tokenizados,\n",
    "        min_count=BIGRAM_MIN_COUNT,\n",
    "        threshold=BIGRAM_THRESHOLD\n",
    "    )\n",
    "    textos_finales = documentos_bi\n",
    "else:\n",
    "    textos_finales = documentos_tokenizados\n",
    "\n",
    "# 4) Construir diccionario y corpus BoW\n",
    "print(\"\\nConstruyendo diccionario y corpus BoW...\")\n",
    "diccionario, corpus = construir_diccionario_corpus(\n",
    "    textos_tokenizados=textos_finales,\n",
    "    no_below=NO_BELOW,\n",
    "    no_above=NO_ABOVE\n",
    ")\n",
    "print(f\"Diccionario creado: {len(diccionario)} tokens únicos.\\n\")\n",
    "\n",
    "# 5) Entrenar modelo LDA\n",
    "print(f\"Entrenando LDA con {NUM_TOPICS} tópicos (passes={PASSES})...\")\n",
    "lda_model = entrenar_lda(\n",
    "    corpus=corpus,\n",
    "    diccionario=diccionario,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    passes=PASSES\n",
    ")\n",
    "print(\"Modelo LDA entrenado.\\n\")\n",
    "\n",
    "# 6) Evaluar modelo (perplejidad y coherencia)\n",
    "print(\"Evaluando modelo LDA...\")\n",
    "perplejidad, coherencia = evaluar_lda(\n",
    "    lda_model=lda_model,\n",
    "    corpus=corpus,\n",
    "    textos_tokenizados=textos_finales,\n",
    "    diccionario=diccionario\n",
    ")\n",
    "print(f\"► Perplejidad: {perplejidad:.4f}\")\n",
    "print(f\"► Coherencia (c_v): {coherencia:.4f}\\n\")\n",
    "\n",
    "# 7) Mostrar top-10 palabras de cada tópico\n",
    "print(\"Términos más representativos por tópico:\")\n",
    "for tid in range(NUM_TOPICS):\n",
    "    términos = lda_model.show_topic(tid, topn=10)\n",
    "    lista_palabras = \", \".join([palabra for palabra, _ in términos])\n",
    "    print(f\"  Tópico {tid}: {lista_palabras}\")\n",
    "print()\n",
    "\n",
    "# 8) Asignar tópico dominante a cada documento\n",
    "print(\"Asignando tópico dominante a cada documento...\")\n",
    "df['bow'] = corpus\n",
    "df['topic_principal'] = asignar_topic_principal(lda_model, corpus)"
   ],
   "id": "b5c57517904c6d64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargados 1292 registros desde 'gestionspider5.json'.\n",
      "\n",
      "Preprocesando descripciones (tokenización + stemming)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progreso preprocesado: 100%|██████████| 1292/1292 [00:04<00:00, 262.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detectando bigramas en el corpus...\n",
      "\n",
      "Construyendo diccionario y corpus BoW...\n",
      "Diccionario creado: 4690 tokens únicos.\n",
      "\n",
      "Entrenando LDA con 7 tópicos (passes=10)...\n",
      "Modelo LDA entrenado.\n",
      "\n",
      "Evaluando modelo LDA...\n",
      "► Perplejidad: -7.4595\n",
      "► Coherencia (c_v): 0.3879\n",
      "\n",
      "Términos más representativos por tópico:\n",
      "  Tópico 0: unid, chin, arancel, pais, trump, estadounidens, president, dij, donald_trump, acuerd\n",
      "  Tópico 1: millon, peru, pais, mayor, sector, inversion, export, crecimient, peruan, indic\n",
      "  Tópico 2: trabaj, pued, empres, accion, pag, laboral, hac, fond, sol, millon\n",
      "  Tópico 3: proyect, nacional, millon, congres, peru, inversion, pais, aprob, public, nuev\n",
      "  Tópico 4: inform, public, nuev, cas, present, nacional, servici, proces, investig, oper\n",
      "  Tópico 5: president, gobiern, ministr, public, peru, trabaj, miner, pais, congres, segur\n",
      "  Tópico 6: empres, oper, peru, nuev, marc, merc, proyect, millon, inversion, peruan\n",
      "\n",
      "Asignando tópico dominante a cada documento...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Identificación del tópico a 5 noticias",
   "id": "62bb22df2fa34017"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T16:29:18.210818Z",
     "start_time": "2025-06-01T16:29:18.201376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 9) Mostrar las primeras filas con su categoría original y tópico asignado\n",
    "print(\"\\nEjemplo de asignación de tópicos:\")\n",
    "print(df[['description', 'category', 'topic_principal']].head(5))"
   ],
   "id": "df755b6d59c8e0d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo de asignación de tópicos:\n",
      "                                         description             category  \\\n",
      "0  El expresidente de Estados Unidos, Joe Biden, ...                 EEUU   \n",
      "1  En un contexto donde cada vez más personas opt...             Economía   \n",
      "2  En abril, el sueldo requerido en el Perú se ub...  Management & Empleo   \n",
      "3  El Gobierno aprobó la Política Nacional de Pro...                 Perú   \n",
      "4  Este domingo 1 de junio comenzará a operar ofi...                 Perú   \n",
      "\n",
      "   topic_principal  \n",
      "0                0  \n",
      "1                2  \n",
      "2                1  \n",
      "3                4  \n",
      "4                4  \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:27:17.942496Z",
     "start_time": "2025-06-01T02:27:09.782366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS as GENSIM_STOP\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel, Phrases, TfidfModel\n",
    "from gensim.models.phrases import Phraser\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Descargas NLTK (solo la primera vez)\n",
    "# -------------------------------------------------\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Definir stopwords en español y stemmer\n",
    "# -------------------------------------------------\n",
    "nltk_stop = set(stopwords.words('spanish'))\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Función de preprocesado (tokenización + stemming)\n",
    "# -------------------------------------------------\n",
    "def preprocess(texto: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    1) Convierte a minúsculas y tokeniza quitando puntuación y acentos (deacc=True).\n",
    "    2) Filtra tokens de longitud <= 3.\n",
    "    3) Elimina stopwords de gensim y NLTK.\n",
    "    4) Aplica stemming en español.\n",
    "    Devuelve la lista de stems resultantes.\n",
    "    \"\"\"\n",
    "    tokens = simple_preprocess(texto, deacc=True)\n",
    "    tokens = [t for t in tokens if len(t) > 3 and t not in GENSIM_STOP and t not in nltk_stop]\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Generación de bigramas (opcional)\n",
    "# -------------------------------------------------\n",
    "def aplicar_bigramas(documentos: list[list[str]],\n",
    "                     min_count: int = 5,\n",
    "                     threshold: float = 100.0) -> tuple[list[list[str]], Phraser]:\n",
    "    \"\"\"\n",
    "    1) Entrena un modelo de bigramas sobre los documentos tokenizados.\n",
    "    2) Devuelve los documentos enriquecidos con bigramas (uni_bi) y el modelo Phraser.\n",
    "    \"\"\"\n",
    "    bigram = Phrases(documentos, min_count=min_count, threshold=threshold, progress_per=10000)\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    documentos_bi = [bigram_mod[doc] for doc in documentos]\n",
    "    return documentos_bi, bigram_mod\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. Construcción de diccionario y corpus BoW\n",
    "# -------------------------------------------------\n",
    "def construir_diccionario_corpus(textos_tokenizados: list[list[str]],\n",
    "                                 no_below: int = 5,\n",
    "                                 no_above: float = 0.5) -> tuple[corpora.Dictionary, list]:\n",
    "    \"\"\"\n",
    "    1) Crea un Dictionary a partir de la lista de documentos (lista de tokens).\n",
    "    2) Filtra palabras muy raras (no_below) o demasiado frecuentes (no_above).\n",
    "    3) Construye el corpus en formato BoW (lista de tuplas (id_token, frecuencia)).\n",
    "    Devuelve el diccionario filtrado y el corpus BoW.\n",
    "    \"\"\"\n",
    "    diccionario = corpora.Dictionary(textos_tokenizados)\n",
    "    diccionario.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    corpus_bow = [diccionario.doc2bow(doc) for doc in textos_tokenizados]\n",
    "    return diccionario, corpus_bow\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. Entrenamiento y evaluación del modelo LDA\n",
    "# -------------------------------------------------\n",
    "def entrenar_lda(corpus: list, diccionario: corpora.Dictionary,\n",
    "                 num_topics: int = 7,\n",
    "                 passes: int = 10,\n",
    "                 random_state: int = 42,\n",
    "                 alpha: str = 'auto') -> LdaModel:\n",
    "    \"\"\"\n",
    "    Entrena un modelo LDA con los parámetros indicados y devuelve el objeto LdaModel.\n",
    "    \"\"\"\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=diccionario,\n",
    "        num_topics=num_topics,\n",
    "        random_state=random_state,\n",
    "        passes=passes,\n",
    "        alpha=alpha,\n",
    "        per_word_topics=True\n",
    "    )\n",
    "    return lda_model\n",
    "\n",
    "\n",
    "def evaluar_lda(lda_model: LdaModel,\n",
    "                corpus: list,\n",
    "                textos_tokenizados: list[list[str]],\n",
    "                diccionario: corpora.Dictionary) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    1) Calcula la perplejidad del modelo sobre el corpus.\n",
    "    2) Calcula la coherencia (c_v) usando CoherenceModel.\n",
    "    Devuelve (perplejidad, coherencia).\n",
    "    \"\"\"\n",
    "    perplejidad = lda_model.log_perplexity(corpus)\n",
    "    coh_model = CoherenceModel(model=lda_model,\n",
    "                               texts=textos_tokenizados,\n",
    "                               dictionary=diccionario,\n",
    "                               coherence='c_v')\n",
    "    coherencia = coh_model.get_coherence()\n",
    "    return perplejidad, coherencia\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7. Asignación de tópico dominante a cada documento\n",
    "# -------------------------------------------------\n",
    "def asignar_topic_principal(lda_model: LdaModel, corpus: list) -> list[int]:\n",
    "    \"\"\"\n",
    "    Para cada documento en formato BoW o TF-IDF, obtiene la lista de tópicos\n",
    "    con sus probabilidades y retorna el ID del tópico con mayor probabilidad.\n",
    "    \"\"\"\n",
    "    topicos_principales = []\n",
    "    for doc_vector in corpus:\n",
    "        distribucion = lda_model.get_document_topics(doc_vector, minimum_probability=0.0)\n",
    "        topico_dom = max(distribucion, key=lambda x: x[1])[0]\n",
    "        topicos_principales.append(topico_dom)\n",
    "    return topicos_principales\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 8. Función principal: pipeline completo\n",
    "# -------------------------------------------------\n",
    "def main():\n",
    "    # Parámetros ajustables\n",
    "    RUTA_CSV = \"peru21news2.csv\"      # Nombre del CSV a leer\n",
    "    NUM_TOPICS = 7                    # Número de tópicos para LDA\n",
    "    PASSES = 10                       # Número de pasadas en LDA\n",
    "    NO_BELOW = 5                      # Mínimo de documentos para que un token sobreviva\n",
    "    NO_ABOVE = 0.5                    # Máximo porcentaje de documentos para que un token sobreviva\n",
    "    BIGRAM = True                     # Si generar bigramas (True/False)\n",
    "    BIGRAM_MIN_COUNT = 5              # Umbral mínimo de apariciones conjuntas para bigramas\n",
    "    BIGRAM_THRESHOLD = 100.0          # Umbral de formación de bigramas\n",
    "\n",
    "    # 1) Leer el CSV (solo columnas 'summit' y 'category')\n",
    "    df = pd.read_csv(RUTA_CSV, delimiter='|', encoding='utf-8', usecols=['summit', 'category'])\n",
    "    print(f\"Cargados {len(df)} registros desde '{RUTA_CSV}'.\\n\")\n",
    "\n",
    "    # 2) Preprocesar los textos (tokenización + stemming) con tqdm\n",
    "    textos_raw = df['summit'].astype(str).tolist()\n",
    "    documentos_tokenizados = []\n",
    "    print(\"1) Preprocesando textos (tokenización + stemming)...\")\n",
    "    for doc in tqdm(textos_raw, desc=\"Preprocesado\"):\n",
    "        documentos_tokenizados.append(preprocess(doc))\n",
    "\n",
    "    # 3) (Opcional) Generar e incluir bigramas\n",
    "    if BIGRAM:\n",
    "        print(\"\\n2) Detectando bigramas en el corpus...\")\n",
    "        documentos_bi, modelo_bigram = aplicar_bigramas(\n",
    "            documentos_tokenizados,\n",
    "            min_count=BIGRAM_MIN_COUNT,\n",
    "            threshold=BIGRAM_THRESHOLD\n",
    "        )\n",
    "        textos_finales = documentos_bi\n",
    "    else:\n",
    "        textos_finales = documentos_tokenizados\n",
    "\n",
    "    # 4) Construir diccionario y corpus BoW\n",
    "    print(\"\\n3) Construyendo diccionario y corpus BoW...\")\n",
    "    diccionario, corpus_bow = construir_diccionario_corpus(\n",
    "        textos_tokenizados=textos_finales,\n",
    "        no_below=NO_BELOW,\n",
    "        no_above=NO_ABOVE\n",
    "    )\n",
    "    print(f\"   - Diccionario creado con {len(diccionario)} tokens únicos.\\n\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 5) CÁLCULO DE TF-IDF\n",
    "    # -------------------------------------------------\n",
    "    print(\"4) Calculando modelo TF-IDF sobre el corpus BoW...\")\n",
    "    tfidf_model = TfidfModel(corpus_bow, dictionary=diccionario)\n",
    "    corpus_tfidf = tfidf_model[corpus_bow]\n",
    "\n",
    "    # 5.1) EXTRAER PALABRAS MÁS RELEVANTES (suma de TF-IDF por token)\n",
    "    print(\"   → Extrayendo las palabras más relevantes (sumando TF-IDF por token)...\")\n",
    "    # Creamos un vector global que acumule TF-IDF de cada token sobre todos los docuementos\n",
    "    tfidf_global = {}\n",
    "    for doc in corpus_tfidf:\n",
    "        for token_id, tfidf_val in doc:\n",
    "            if token_id not in tfidf_global:\n",
    "                tfidf_global[token_id] = tfidf_val\n",
    "            else:\n",
    "                tfidf_global[token_id] += tfidf_val\n",
    "\n",
    "    # Ordenamos todos los tokens por peso TF-IDF acumulado descendente\n",
    "    tokens_ordenados = sorted(tfidf_global.items(), key=lambda x: x[1], reverse=True)\n",
    "    TOP_TFIDF = 20\n",
    "    print(f\"\\n   Top {TOP_TFIDF} tokens más relevantes (TF-IDF acumulado):\")\n",
    "    for idx, (token_id, acumulado) in enumerate(tokens_ordenados[:TOP_TFIDF], start=1):\n",
    "        palabra = diccionario[token_id]\n",
    "        print(f\"     {idx:>2}. {palabra:<20}  —  TF-IDF acumulado: {acumulado:.4f}\")\n",
    "    print()\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 6) ENTRENAR LDA sobre CORPUS TF-IDF\n",
    "    # -------------------------------------------------\n",
    "    print(\"5) Entrenando LDA (7 tópicos) sobre el corpus TF-IDF...\")\n",
    "    lda_model = entrenar_lda(\n",
    "        corpus=list(corpus_tfidf),   # IMPORTANTE: usamos corpus_tfidf en lugar de corpus_bow\n",
    "        diccionario=diccionario,\n",
    "        num_topics=NUM_TOPICS,\n",
    "        passes=PASSES\n",
    "    )\n",
    "    print(\"   ► LDA entrenado.\\n\")\n",
    "\n",
    "    # 6.1) Evaluar modelo (perplejidad y coherencia)\n",
    "    print(\"6) Evaluando modelo LDA...\")\n",
    "    perplejidad, coherencia = evaluar_lda(\n",
    "        lda_model=lda_model,\n",
    "        corpus=list(corpus_tfidf),\n",
    "        textos_tokenizados=textos_finales,\n",
    "        diccionario=diccionario\n",
    "    )\n",
    "    print(f\"   ► Perplejidad: {perplejidad:.4f}\")\n",
    "    print(f\"   ► Coherencia (c_v): {coherencia:.4f}\\n\")\n",
    "\n",
    "    # 6.2) Mostrar top-10 palabras de cada tópico\n",
    "    print(\"7) Términos más representativos por tópico (Top 10):\")\n",
    "    for tid in range(NUM_TOPICS):\n",
    "        términos = lda_model.show_topic(tid, topn=10)\n",
    "        términos_str = \", \".join([pal for pal, _ in términos])\n",
    "        print(f\"   Tópico {tid}: {términos_str}\")\n",
    "    print()\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 7) ASIGNAR TÓPICO DOMINANTE A CADA DOCUMENTO\n",
    "    # -------------------------------------------------\n",
    "    print(\"8) Asignando tópico dominante a cada noticia...\")\n",
    "    df['bow'] = corpus_bow\n",
    "    # NOTA: La función `asignar_topic_principal` funciona con corpus en BoW o en TF-IDF\n",
    "    df['topic_principal'] = asignar_topic_principal(lda_model, list(corpus_tfidf))\n",
    "    print(\"   → Asignación completada.\\n\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 8) MOSTRAR RESULTADOS PARA 3 NOTICIAS EJEMPLO\n",
    "    # -------------------------------------------------\n",
    "    print(\"9) Mostrando resultados de LDA para 3 noticias de ejemplo:\\n\")\n",
    "    # Seleccionamos tres índices arbitrarios (por ejemplo 0, 10 y 20) o aleatorios\n",
    "    ejemplos_idx = [0, 10, 20]\n",
    "    for idx in ejemplos_idx:\n",
    "        texto_original = df.loc[idx, 'summit']\n",
    "        categoria = df.loc[idx, 'category']\n",
    "        bow         = df.loc[idx, 'bow']\n",
    "        tfidf_vec   = corpus_tfidf[idx]\n",
    "        topico_dom  = df.loc[idx, 'topic_principal']\n",
    "\n",
    "        # Obtenemos la distribución completa de tópicos para ese documento\n",
    "        distribucion = lda_model.get_document_topics(tfidf_vec, minimum_probability=0.0)\n",
    "\n",
    "        print(f\"→ Noticia #{idx} (categoría = '{categoria}'):\")\n",
    "        print(f\"   * Texto original (resumen): {texto_original[:100]}...\")  # mostramos solo los primeros 100 caracteres\n",
    "        print(f\"   * Tópico dominante: {topico_dom}\")\n",
    "        print(f\"   * Distribución completa de tópicos (id: probabilidad):\")\n",
    "        print(f\"     {[(t, round(p, 4)) for t, p in distribucion]}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "65634df1f10b2e1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargados 108 registros desde 'peru21news2.csv'.\n",
      "\n",
      "1) Preprocesando textos (tokenización + stemming)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocesado: 100%|██████████| 108/108 [00:00<00:00, 594.07it/s]\n",
      "2025-05-31 21:27:10,030 : INFO : collecting all words and their counts\n",
      "2025-05-31 21:27:10,032 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2025-05-31 21:27:10,038 : INFO : collected 2089 token types (unigram + bigrams) from a corpus of 1340 words and 108 sentences\n",
      "2025-05-31 21:27:10,038 : INFO : merged Phrases<2089 vocab, min_count=5, threshold=100.0, max_vocab_size=40000000>\n",
      "2025-05-31 21:27:10,038 : INFO : Phrases lifecycle event {'msg': 'built Phrases<2089 vocab, min_count=5, threshold=100.0, max_vocab_size=40000000> in 0.01s', 'datetime': '2025-05-31T21:27:10.038148', 'gensim': '4.3.3', 'python': '3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:42:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-05-31 21:27:10,038 : INFO : exporting phrases from Phrases<2089 vocab, min_count=5, threshold=100.0, max_vocab_size=40000000>\n",
      "2025-05-31 21:27:10,038 : INFO : FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<0 phrases, min_count=5, threshold=100.0> from Phrases<2089 vocab, min_count=5, threshold=100.0, max_vocab_size=40000000> in 0.00s', 'datetime': '2025-05-31T21:27:10.038148', 'gensim': '4.3.3', 'python': '3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:42:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-05-31 21:27:10,053 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2025-05-31 21:27:10,060 : INFO : built Dictionary<924 unique tokens: ['cancilleri', 'civil', 'coordin', 'digital', 'dni']...> from 108 documents (total 1340 corpus positions)\n",
      "2025-05-31 21:27:10,060 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<924 unique tokens: ['cancilleri', 'civil', 'coordin', 'digital', 'dni']...> from 108 documents (total 1340 corpus positions)\", 'datetime': '2025-05-31T21:27:10.060095', 'gensim': '4.3.3', 'python': '3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:42:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-05-31 21:27:10,060 : INFO : discarding 909 tokens: [('cancilleri', 1), ('civil', 1), ('coordin', 3), ('digital', 3), ('dni', 1), ('eleccion', 1), ('exterior', 1), ('identif', 1), ('impuls', 1), ('maner', 2)]...\n",
      "2025-05-31 21:27:10,060 : INFO : keeping 15 tokens which were in no less than 5 and no more than 54 (=50.0%) documents\n",
      "2025-05-31 21:27:10,060 : INFO : resulting dictionary: Dictionary<15 unique tokens: ['nacional', 'peruan', 'carg', 'segur', 'nuev']...>\n",
      "2025-05-31 21:27:10,060 : WARNING : constructor received both corpus and explicit inverse document frequencies; ignoring the corpus\n",
      "2025-05-31 21:27:10,103 : INFO : using autotuned alpha, starting with [0.14285715, 0.14285715, 0.14285715, 0.14285715, 0.14285715, 0.14285715, 0.14285715]\n",
      "2025-05-31 21:27:10,104 : INFO : using symmetric eta at 0.14285714285714285\n",
      "2025-05-31 21:27:10,106 : INFO : using serial LDA version on this node\n",
      "2025-05-31 21:27:10,108 : INFO : running online (multi-pass) LDA training, 7 topics, 10 passes over the supplied corpus of 108 documents, updating model once every 108 documents, evaluating perplexity every 108 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2025-05-31 21:27:10,132 : INFO : -6.144 per-word bound, 70.7 perplexity estimate based on a held-out corpus of 108 documents with 66 words\n",
      "2025-05-31 21:27:10,132 : INFO : PROGRESS: pass 0, at document #108/108\n",
      "2025-05-31 21:27:10,212 : INFO : optimized alpha [0.14767659, 0.13712594, 0.14465919, 0.14767767, 0.14251281, 0.14109881, 0.1358067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2) Detectando bigramas en el corpus...\n",
      "\n",
      "3) Construyendo diccionario y corpus BoW...\n",
      "   - Diccionario creado con 15 tokens únicos.\n",
      "\n",
      "4) Calculando modelo TF-IDF sobre el corpus BoW...\n",
      "   → Extrayendo las palabras más relevantes (sumando TF-IDF por token)...\n",
      "\n",
      "   Top 20 tokens más relevantes (TF-IDF acumulado):\n",
      "      1. peru                  —  TF-IDF acumulado: 6.1172\n",
      "      2. peruan                —  TF-IDF acumulado: 5.0400\n",
      "      3. aere                  —  TF-IDF acumulado: 4.9935\n",
      "      4. oper                  —  TF-IDF acumulado: 4.9335\n",
      "      5. nuev                  —  TF-IDF acumulado: 4.6406\n",
      "      6. millon                —  TF-IDF acumulado: 4.2167\n",
      "      7. carg                  —  TF-IDF acumulado: 4.1775\n",
      "      8. public                —  TF-IDF acumulado: 4.1745\n",
      "      9. anos                  —  TF-IDF acumulado: 4.1732\n",
      "     10. sol                   —  TF-IDF acumulado: 4.1732\n",
      "     11. pais                  —  TF-IDF acumulado: 4.0670\n",
      "     12. muert                 —  TF-IDF acumulado: 4.0243\n",
      "     13. segur                 —  TF-IDF acumulado: 3.9924\n",
      "     14. nacional              —  TF-IDF acumulado: 3.9231\n",
      "     15. seguidor              —  TF-IDF acumulado: 3.3627\n",
      "\n",
      "5) Entrenando LDA (7 tópicos) sobre el corpus TF-IDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 21:27:10,245 : INFO : topic #6 (0.136): 0.334*\"muert\" + 0.204*\"peruan\" + 0.148*\"aere\" + 0.026*\"oper\" + 0.026*\"peru\" + 0.026*\"segur\" + 0.026*\"carg\" + 0.026*\"nuev\" + 0.026*\"sol\" + 0.026*\"anos\"\n",
      "2025-05-31 21:27:10,280 : INFO : topic #1 (0.137): 0.310*\"anos\" + 0.166*\"peru\" + 0.108*\"segur\" + 0.103*\"oper\" + 0.103*\"public\" + 0.021*\"peruan\" + 0.021*\"carg\" + 0.021*\"aere\" + 0.021*\"sol\" + 0.021*\"millon\"\n",
      "2025-05-31 21:27:10,280 : INFO : topic #2 (0.145): 0.258*\"aere\" + 0.176*\"oper\" + 0.176*\"segur\" + 0.094*\"anos\" + 0.094*\"nuev\" + 0.094*\"peruan\" + 0.012*\"peru\" + 0.012*\"carg\" + 0.012*\"pais\" + 0.012*\"millon\"\n",
      "2025-05-31 21:27:10,280 : INFO : topic #0 (0.148): 0.190*\"pais\" + 0.172*\"nuev\" + 0.132*\"seguidor\" + 0.112*\"peruan\" + 0.092*\"public\" + 0.069*\"segur\" + 0.052*\"anos\" + 0.051*\"muert\" + 0.044*\"carg\" + 0.042*\"nacional\"\n",
      "2025-05-31 21:27:10,280 : INFO : topic #3 (0.148): 0.209*\"nacional\" + 0.162*\"carg\" + 0.149*\"oper\" + 0.109*\"public\" + 0.087*\"seguidor\" + 0.068*\"millon\" + 0.062*\"peruan\" + 0.048*\"peru\" + 0.032*\"segur\" + 0.031*\"nuev\"\n",
      "2025-05-31 21:27:10,280 : INFO : topic diff=3.719969, rho=1.000000\n",
      "2025-05-31 21:27:10,306 : INFO : -4.557 per-word bound, 23.5 perplexity estimate based on a held-out corpus of 108 documents with 66 words\n",
      "2025-05-31 21:27:10,308 : INFO : PROGRESS: pass 1, at document #108/108\n",
      "2025-05-31 21:27:10,320 : INFO : optimized alpha [0.14948384, 0.13399407, 0.14773753, 0.14922337, 0.14361043, 0.13864729, 0.13557197]\n",
      "2025-05-31 21:27:10,320 : INFO : topic #1 (0.134): 0.496*\"anos\" + 0.090*\"peru\" + 0.063*\"segur\" + 0.061*\"oper\" + 0.061*\"public\" + 0.023*\"peruan\" + 0.023*\"carg\" + 0.023*\"aere\" + 0.023*\"sol\" + 0.023*\"millon\"\n",
      "2025-05-31 21:27:10,320 : INFO : topic #6 (0.136): 0.351*\"muert\" + 0.335*\"peruan\" + 0.102*\"aere\" + 0.018*\"oper\" + 0.018*\"peru\" + 0.018*\"segur\" + 0.018*\"carg\" + 0.018*\"anos\" + 0.018*\"nuev\" + 0.018*\"nacional\"\n",
      "2025-05-31 21:27:10,320 : INFO : topic #2 (0.148): 0.267*\"oper\" + 0.256*\"aere\" + 0.241*\"segur\" + 0.057*\"nuev\" + 0.042*\"peruan\" + 0.042*\"anos\" + 0.011*\"peru\" + 0.011*\"carg\" + 0.011*\"public\" + 0.011*\"pais\"\n",
      "2025-05-31 21:27:10,331 : INFO : topic #3 (0.149): 0.256*\"nacional\" + 0.243*\"carg\" + 0.117*\"public\" + 0.092*\"seguidor\" + 0.091*\"oper\" + 0.041*\"peruan\" + 0.038*\"millon\" + 0.030*\"peru\" + 0.023*\"segur\" + 0.021*\"nuev\"\n",
      "2025-05-31 21:27:10,331 : INFO : topic #0 (0.149): 0.237*\"nuev\" + 0.224*\"pais\" + 0.146*\"seguidor\" + 0.123*\"public\" + 0.082*\"peruan\" + 0.036*\"segur\" + 0.028*\"anos\" + 0.028*\"muert\" + 0.025*\"carg\" + 0.024*\"nacional\"\n",
      "2025-05-31 21:27:10,333 : INFO : topic diff=0.421552, rho=0.577350\n",
      "2025-05-31 21:27:10,349 : INFO : -4.211 per-word bound, 18.5 perplexity estimate based on a held-out corpus of 108 documents with 66 words\n",
      "2025-05-31 21:27:10,349 : INFO : PROGRESS: pass 2, at document #108/108\n",
      "2025-05-31 21:27:10,368 : INFO : optimized alpha [0.1510114, 0.1314522, 0.15036908, 0.15035252, 0.14453173, 0.13663849, 0.13541882]\n",
      "2025-05-31 21:27:10,372 : INFO : topic #1 (0.131): 0.574*\"anos\" + 0.058*\"peru\" + 0.045*\"segur\" + 0.043*\"public\" + 0.043*\"oper\" + 0.024*\"peruan\" + 0.024*\"carg\" + 0.024*\"aere\" + 0.024*\"muert\" + 0.024*\"sol\"\n",
      "2025-05-31 21:27:10,372 : INFO : topic #6 (0.135): 0.369*\"peruan\" + 0.352*\"muert\" + 0.091*\"aere\" + 0.016*\"oper\" + 0.016*\"nacional\" + 0.016*\"peru\" + 0.016*\"anos\" + 0.016*\"segur\" + 0.016*\"carg\" + 0.016*\"nuev\"\n",
      "2025-05-31 21:27:10,372 : INFO : topic #3 (0.150): 0.284*\"carg\" + 0.281*\"nacional\" + 0.123*\"public\" + 0.096*\"seguidor\" + 0.057*\"oper\" + 0.027*\"peruan\" + 0.025*\"millon\" + 0.021*\"peru\" + 0.017*\"segur\" + 0.016*\"nuev\"\n",
      "2025-05-31 21:27:10,372 : INFO : topic #2 (0.150): 0.303*\"oper\" + 0.260*\"segur\" + 0.252*\"aere\" + 0.044*\"nuev\" + 0.025*\"peruan\" + 0.025*\"anos\" + 0.010*\"peru\" + 0.010*\"public\" + 0.010*\"nacional\" + 0.010*\"carg\"\n",
      "2025-05-31 21:27:10,372 : INFO : topic #0 (0.151): 0.265*\"nuev\" + 0.237*\"pais\" + 0.151*\"seguidor\" + 0.135*\"public\" + 0.070*\"peruan\" + 0.023*\"segur\" + 0.019*\"anos\" + 0.019*\"muert\" + 0.018*\"carg\" + 0.017*\"nacional\"\n",
      "2025-05-31 21:27:10,372 : INFO : topic diff=0.301531, rho=0.500000\n",
      "2025-05-31 21:27:10,400 : INFO : -4.094 per-word bound, 17.1 perplexity estimate based on a held-out corpus of 108 documents with 66 words\n",
      "2025-05-31 21:27:10,400 : INFO : PROGRESS: pass 3, at document #108/108\n",
      "2025-05-31 21:27:10,407 : INFO : optimized alpha [0.1534924, 0.12974447, 0.15325198, 0.1502436, 0.14533089, 0.13498703, 0.13583052]\n",
      "2025-05-31 21:27:10,421 : INFO : topic #1 (0.130): 0.620*\"anos\" + 0.043*\"peru\" + 0.035*\"segur\" + 0.034*\"public\" + 0.034*\"oper\" + 0.023*\"peruan\" + 0.023*\"muert\" + 0.023*\"carg\" + 0.023*\"aere\" + 0.023*\"pais\"\n",
      "2025-05-31 21:27:10,421 : INFO : topic #5 (0.135): 0.467*\"millon\" + 0.109*\"sol\" + 0.096*\"muert\" + 0.079*\"public\" + 0.073*\"peru\" + 0.029*\"carg\" + 0.016*\"peruan\" + 0.016*\"nuev\" + 0.016*\"oper\" + 0.016*\"pais\"\n",
      "2025-05-31 21:27:10,424 : INFO : topic #3 (0.150): 0.321*\"carg\" + 0.311*\"nacional\" + 0.104*\"seguidor\" + 0.079*\"public\" + 0.039*\"oper\" + 0.022*\"peruan\" + 0.020*\"millon\" + 0.018*\"peru\" + 0.015*\"segur\" + 0.015*\"nuev\"\n",
      "2025-05-31 21:27:10,426 : INFO : topic #2 (0.153): 0.314*\"oper\" + 0.263*\"segur\" + 0.262*\"aere\" + 0.038*\"nuev\" + 0.018*\"peruan\" + 0.018*\"anos\" + 0.010*\"public\" + 0.010*\"peru\" + 0.010*\"nacional\" + 0.010*\"pais\"\n",
      "2025-05-31 21:27:10,427 : INFO : topic #0 (0.153): 0.270*\"nuev\" + 0.236*\"pais\" + 0.181*\"public\" + 0.149*\"seguidor\" + 0.046*\"peruan\" + 0.017*\"segur\" + 0.015*\"anos\" + 0.014*\"muert\" + 0.014*\"carg\" + 0.013*\"nacional\"\n",
      "2025-05-31 21:27:10,429 : INFO : topic diff=0.269323, rho=0.447214\n",
      "2025-05-31 21:27:10,445 : INFO : -4.007 per-word bound, 16.1 perplexity estimate based on a held-out corpus of 108 documents with 66 words\n",
      "2025-05-31 21:27:10,445 : INFO : PROGRESS: pass 4, at document #108/108\n",
      "2025-05-31 21:27:10,466 : INFO : optimized alpha [0.15613517, 0.12833814, 0.1563415, 0.15019886, 0.14606827, 0.13359933, 0.1365746]\n",
      "2025-05-31 21:27:10,466 : INFO : topic #1 (0.128): 0.645*\"anos\" + 0.034*\"peru\" + 0.030*\"segur\" + 0.029*\"public\" + 0.029*\"oper\" + 0.023*\"peruan\" + 0.023*\"muert\" + 0.023*\"pais\" + 0.023*\"carg\" + 0.023*\"aere\"\n",
      "2025-05-31 21:27:10,466 : INFO : topic #5 (0.134): 0.483*\"millon\" + 0.105*\"sol\" + 0.092*\"muert\" + 0.080*\"public\" + 0.069*\"peru\" + 0.024*\"carg\" + 0.016*\"peruan\" + 0.016*\"nuev\" + 0.016*\"oper\" + 0.016*\"pais\"\n",
      "2025-05-31 21:27:10,466 : INFO : topic #3 (0.150): 0.343*\"carg\" + 0.328*\"nacional\" + 0.108*\"seguidor\" + 0.054*\"public\" + 0.029*\"oper\" + 0.018*\"peruan\" + 0.017*\"millon\" + 0.016*\"peru\" + 0.014*\"segur\" + 0.014*\"nuev\"\n",
      "2025-05-31 21:27:10,473 : INFO : topic #0 (0.156): 0.278*\"nuev\" + 0.234*\"pais\" + 0.203*\"public\" + 0.147*\"seguidor\" + 0.031*\"peruan\" + 0.014*\"segur\" + 0.012*\"anos\" + 0.012*\"muert\" + 0.012*\"carg\" + 0.011*\"nacional\"\n",
      "2025-05-31 21:27:10,476 : INFO : topic #2 (0.156): 0.317*\"oper\" + 0.279*\"aere\" + 0.262*\"segur\" + 0.028*\"nuev\" + 0.014*\"peruan\" + 0.014*\"anos\" + 0.009*\"public\" + 0.009*\"peru\" + 0.009*\"nacional\" + 0.009*\"pais\"\n",
      "2025-05-31 21:27:10,476 : INFO : topic diff=0.226654, rho=0.408248\n",
      "2025-05-31 21:27:10,491 : INFO : -3.949 per-word bound, 15.4 perplexity estimate based on a held-out corpus of 108 documents with 66 words\n",
      "2025-05-31 21:27:10,491 : INFO : PROGRESS: pass 5, at document #108/108\n",
      "2025-05-31 21:27:10,515 : INFO : optimized alpha [0.15861143, 0.12712333, 0.15924287, 0.15019348, 0.1470584, 0.13238007, 0.13730821]\n",
      "2025-05-31 21:27:10,515 : INFO : topic #1 (0.127): 0.659*\"anos\" + 0.030*\"peru\" + 0.027*\"segur\" + 0.027*\"public\" + 0.027*\"oper\" + 0.023*\"pais\" + 0.023*\"muert\" + 0.023*\"peruan\" + 0.023*\"sol\" + 0.023*\"carg\"\n",
      "2025-05-31 21:27:10,515 : INFO : topic #5 (0.132): 0.497*\"millon\" + 0.104*\"sol\" + 0.090*\"muert\" + 0.081*\"public\" + 0.057*\"peru\" + 0.022*\"carg\" + 0.017*\"peruan\" + 0.017*\"nuev\" + 0.017*\"oper\" + 0.017*\"pais\"\n",
      "2025-05-31 21:27:10,520 : INFO : topic #3 (0.150): 0.356*\"carg\" + 0.338*\"nacional\" + 0.111*\"seguidor\" + 0.039*\"public\" + 0.023*\"oper\" + 0.016*\"peruan\" + 0.016*\"millon\" + 0.015*\"peru\" + 0.014*\"segur\" + 0.014*\"nuev\"\n",
      "2025-05-31 21:27:10,523 : INFO : topic #0 (0.159): 0.284*\"nuev\" + 0.233*\"pais\" + 0.214*\"public\" + 0.146*\"seguidor\" + 0.023*\"peruan\" + 0.012*\"segur\" + 0.011*\"anos\" + 0.011*\"muert\" + 0.011*\"carg\" + 0.010*\"nacional\"\n",
      "2025-05-31 21:27:10,523 : INFO : topic #2 (0.159): 0.317*\"oper\" + 0.293*\"aere\" + 0.261*\"segur\" + 0.022*\"nuev\" + 0.012*\"peruan\" + 0.012*\"anos\" + 0.009*\"public\" + 0.009*\"peru\" + 0.009*\"nacional\" + 0.009*\"pais\"\n",
      "2025-05-31 21:27:10,527 : INFO : topic diff=0.185108, rho=0.377964\n",
      "2025-05-31 21:27:10,553 : INFO : -3.910 per-word bound, 15.0 perplexity estimate based on a held-out corpus of 108 documents with 66 words\n",
      "2025-05-31 21:27:10,553 : INFO : PROGRESS: pass 6, at document #108/108\n",
      "2025-05-31 21:27:10,571 : INFO : optimized alpha [0.16092892, 0.12604678, 0.16195391, 0.15021096, 0.14802511, 0.1312906, 0.13800526]\n",
      "2025-05-31 21:27:10,571 : INFO : topic #1 (0.126): 0.668*\"anos\" + 0.027*\"peru\" + 0.026*\"segur\" + 0.025*\"public\" + 0.025*\"oper\" + 0.023*\"pais\" + 0.023*\"muert\" + 0.023*\"peruan\" + 0.023*\"sol\" + 0.023*\"carg\"\n",
      "2025-05-31 21:27:10,571 : INFO : topic #5 (0.131): 0.507*\"millon\" + 0.103*\"sol\" + 0.089*\"muert\" + 0.082*\"public\" + 0.046*\"peru\" + 0.020*\"carg\" + 0.017*\"peruan\" + 0.017*\"nuev\" + 0.017*\"oper\" + 0.017*\"pais\"\n",
      "2025-05-31 21:27:10,571 : INFO : topic #3 (0.150): 0.363*\"carg\" + 0.344*\"nacional\" + 0.112*\"seguidor\" + 0.030*\"public\" + 0.020*\"oper\" + 0.015*\"peruan\" + 0.014*\"millon\" + 0.014*\"peru\" + 0.013*\"segur\" + 0.013*\"nuev\"\n",
      "2025-05-31 21:27:10,578 : INFO : topic #0 (0.161): 0.288*\"nuev\" + 0.233*\"pais\" + 0.221*\"public\" + 0.145*\"seguidor\" + 0.018*\"peruan\" + 0.011*\"segur\" + 0.010*\"anos\" + 0.010*\"muert\" + 0.010*\"carg\" + 0.010*\"nacional\"\n",
      "2025-05-31 21:27:10,578 : INFO : topic #2 (0.162): 0.317*\"oper\" + 0.301*\"aere\" + 0.260*\"segur\" + 0.018*\"nuev\" + 0.011*\"peruan\" + 0.011*\"anos\" + 0.009*\"public\" + 0.009*\"peru\" + 0.009*\"nacional\" + 0.009*\"pais\"\n",
      "2025-05-31 21:27:10,578 : INFO : topic diff=0.148204, rho=0.353553\n",
      "2025-05-31 21:27:10,603 : INFO : -3.884 per-word bound, 14.8 perplexity estimate based on a held-out corpus of 108 documents with 66 words\n",
      "2025-05-31 21:27:10,604 : INFO : PROGRESS: pass 7, at document #108/108\n",
      "2025-05-31 21:27:10,619 : INFO : optimized alpha [0.16341925, 0.12508896, 0.16451465, 0.15025969, 0.14893946, 0.13031879, 0.13867885]\n",
      "2025-05-31 21:27:10,619 : INFO : topic #1 (0.125): 0.673*\"anos\" + 0.026*\"peru\" + 0.025*\"segur\" + 0.024*\"public\" + 0.024*\"oper\" + 0.023*\"pais\" + 0.023*\"muert\" + 0.023*\"peruan\" + 0.023*\"sol\" + 0.023*\"carg\"\n",
      "2025-05-31 21:27:10,619 : INFO : topic #5 (0.130): 0.515*\"millon\" + 0.103*\"sol\" + 0.089*\"muert\" + 0.083*\"public\" + 0.038*\"peru\" + 0.019*\"carg\" + 0.017*\"peruan\" + 0.017*\"nuev\" + 0.017*\"oper\" + 0.017*\"pais\"\n",
      "2025-05-31 21:27:10,619 : INFO : topic #3 (0.150): 0.368*\"carg\" + 0.348*\"nacional\" + 0.113*\"seguidor\" + 0.024*\"public\" + 0.017*\"oper\" + 0.014*\"peruan\" + 0.014*\"millon\" + 0.013*\"peru\" + 0.013*\"segur\" + 0.013*\"nuev\"\n",
      "2025-05-31 21:27:10,619 : INFO : topic #0 (0.163): 0.289*\"nuev\" + 0.237*\"pais\" + 0.224*\"public\" + 0.144*\"seguidor\" + 0.015*\"peruan\" + 0.010*\"segur\" + 0.010*\"anos\" + 0.010*\"muert\" + 0.010*\"carg\" + 0.009*\"nacional\"\n",
      "2025-05-31 21:27:10,619 : INFO : topic #2 (0.165): 0.317*\"oper\" + 0.307*\"aere\" + 0.259*\"segur\" + 0.015*\"nuev\" + 0.010*\"peruan\" + 0.010*\"anos\" + 0.009*\"public\" + 0.009*\"peru\" + 0.009*\"nacional\" + 0.009*\"muert\"\n",
      "2025-05-31 21:27:10,619 : INFO : topic diff=0.122159, rho=0.333333\n",
      "2025-05-31 21:27:10,634 : INFO : -3.864 per-word bound, 14.6 perplexity estimate based on a held-out corpus of 108 documents with 66 words\n",
      "2025-05-31 21:27:10,634 : INFO : PROGRESS: pass 8, at document #108/108\n",
      "2025-05-31 21:27:10,649 : INFO : optimized alpha [0.16581373, 0.12422196, 0.16693556, 0.15032513, 0.149804, 0.12943831, 0.13932262]\n",
      "2025-05-31 21:27:10,649 : INFO : topic #1 (0.124): 0.676*\"anos\" + 0.025*\"peru\" + 0.024*\"segur\" + 0.024*\"public\" + 0.024*\"oper\" + 0.023*\"pais\" + 0.023*\"muert\" + 0.023*\"peruan\" + 0.023*\"sol\" + 0.023*\"carg\"\n",
      "2025-05-31 21:27:10,649 : INFO : topic #5 (0.129): 0.520*\"millon\" + 0.103*\"sol\" + 0.089*\"muert\" + 0.083*\"public\" + 0.032*\"peru\" + 0.019*\"carg\" + 0.017*\"peruan\" + 0.017*\"nuev\" + 0.017*\"oper\" + 0.017*\"pais\"\n",
      "2025-05-31 21:27:10,649 : INFO : topic #3 (0.150): 0.371*\"carg\" + 0.350*\"nacional\" + 0.114*\"seguidor\" + 0.021*\"public\" + 0.016*\"oper\" + 0.014*\"peruan\" + 0.013*\"millon\" + 0.013*\"peru\" + 0.013*\"segur\" + 0.013*\"nuev\"\n",
      "2025-05-31 21:27:10,665 : INFO : topic #0 (0.166): 0.289*\"nuev\" + 0.240*\"pais\" + 0.225*\"public\" + 0.143*\"seguidor\" + 0.013*\"peruan\" + 0.010*\"segur\" + 0.009*\"anos\" + 0.009*\"muert\" + 0.009*\"carg\" + 0.009*\"nacional\"\n",
      "2025-05-31 21:27:10,665 : INFO : topic #2 (0.167): 0.317*\"oper\" + 0.311*\"aere\" + 0.259*\"segur\" + 0.013*\"nuev\" + 0.010*\"peruan\" + 0.010*\"anos\" + 0.009*\"public\" + 0.009*\"nacional\" + 0.009*\"peru\" + 0.009*\"muert\"\n",
      "2025-05-31 21:27:10,665 : INFO : topic diff=0.098406, rho=0.316228\n",
      "2025-05-31 21:27:10,688 : INFO : -3.851 per-word bound, 14.4 perplexity estimate based on a held-out corpus of 108 documents with 66 words\n",
      "2025-05-31 21:27:10,688 : INFO : PROGRESS: pass 9, at document #108/108\n",
      "2025-05-31 21:27:10,704 : INFO : optimized alpha [0.16810137, 0.12343057, 0.16923407, 0.15040374, 0.15062778, 0.12863462, 0.13994049]\n",
      "2025-05-31 21:27:10,704 : INFO : topic #1 (0.123): 0.678*\"anos\" + 0.024*\"peru\" + 0.024*\"segur\" + 0.023*\"public\" + 0.023*\"oper\" + 0.023*\"pais\" + 0.023*\"muert\" + 0.023*\"peruan\" + 0.023*\"sol\" + 0.023*\"carg\"\n",
      "2025-05-31 21:27:10,704 : INFO : topic #5 (0.129): 0.523*\"millon\" + 0.103*\"sol\" + 0.089*\"muert\" + 0.084*\"public\" + 0.028*\"peru\" + 0.018*\"carg\" + 0.017*\"peruan\" + 0.017*\"nuev\" + 0.017*\"oper\" + 0.017*\"pais\"\n",
      "2025-05-31 21:27:10,710 : INFO : topic #4 (0.151): 0.513*\"peru\" + 0.300*\"sol\" + 0.030*\"pais\" + 0.017*\"peruan\" + 0.016*\"aere\" + 0.014*\"anos\" + 0.013*\"millon\" + 0.013*\"nuev\" + 0.012*\"nacional\" + 0.012*\"segur\"\n",
      "2025-05-31 21:27:10,710 : INFO : topic #0 (0.168): 0.288*\"nuev\" + 0.243*\"pais\" + 0.225*\"public\" + 0.142*\"seguidor\" + 0.012*\"peruan\" + 0.009*\"segur\" + 0.009*\"anos\" + 0.009*\"muert\" + 0.009*\"carg\" + 0.009*\"nacional\"\n",
      "2025-05-31 21:27:10,710 : INFO : topic #2 (0.169): 0.316*\"oper\" + 0.314*\"aere\" + 0.258*\"segur\" + 0.012*\"nuev\" + 0.010*\"peruan\" + 0.010*\"anos\" + 0.009*\"public\" + 0.009*\"nacional\" + 0.009*\"peru\" + 0.009*\"muert\"\n",
      "2025-05-31 21:27:10,716 : INFO : topic diff=0.078346, rho=0.301511\n",
      "2025-05-31 21:27:10,716 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=15, num_topics=7, decay=0.5, chunksize=2000> in 0.61s', 'datetime': '2025-05-31T21:27:10.716523', 'gensim': '4.3.3', 'python': '3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:42:21) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-05-31 21:27:10,741 : INFO : -3.841 per-word bound, 14.3 perplexity estimate based on a held-out corpus of 108 documents with 66 words\n",
      "2025-05-31 21:27:10,745 : INFO : using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ► LDA entrenado.\n",
      "\n",
      "6) Evaluando modelo LDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 21:27:17,838 : INFO : 11 accumulators retrieved from output queue\n",
      "2025-05-31 21:27:17,849 : INFO : accumulated word occurrence stats for 108 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ► Perplejidad: -3.8408\n",
      "   ► Coherencia (c_v): 0.4622\n",
      "\n",
      "7) Términos más representativos por tópico (Top 10):\n",
      "   Tópico 0: nuev, pais, public, seguidor, peruan, segur, anos, muert, carg, nacional\n",
      "   Tópico 1: anos, peru, segur, public, oper, pais, muert, peruan, sol, carg\n",
      "   Tópico 2: oper, aere, segur, nuev, peruan, anos, public, nacional, peru, muert\n",
      "   Tópico 3: carg, nacional, seguidor, public, oper, peruan, millon, peru, segur, nuev\n",
      "   Tópico 4: peru, sol, pais, peruan, aere, anos, millon, nuev, nacional, segur\n",
      "   Tópico 5: millon, sol, muert, public, peru, carg, peruan, nuev, oper, pais\n",
      "   Tópico 6: peruan, muert, aere, nacional, nuev, pais, anos, sol, peru, oper\n",
      "\n",
      "8) Asignando tópico dominante a cada noticia...\n",
      "   → Asignación completada.\n",
      "\n",
      "9) Mostrando resultados de LDA para 3 noticias de ejemplo:\n",
      "\n",
      "→ Noticia #0 (categoría = 'Política'):\n",
      "   * Texto original (resumen): El Registro Nacional de Identificación y Estado Civil y la Cancillería peruana coordinan e impulsan ...\n",
      "   * Tópico dominante: 6\n",
      "   * Distribución completa de tópicos (id: probabilidad):\n",
      "     [(0, 0.0709), (1, 0.052), (2, 0.0713), (3, 0.2519), (4, 0.0635), (5, 0.0542), (6, 0.4361)]\n",
      "\n",
      "→ Noticia #10 (categoría = 'Perú'):\n",
      "   * Texto original (resumen): La mujer de 56 años fue trasladada al hospital de Ica tras el accidente, pero su muerte fue confirma...\n",
      "   * Tópico dominante: 6\n",
      "   * Distribución completa de tópicos (id: probabilidad):\n",
      "     [(0, 0.0688), (1, 0.3398), (2, 0.0692), (3, 0.0615), (4, 0.0616), (5, 0.0527), (6, 0.3464)]\n",
      "\n",
      "→ Noticia #20 (categoría = 'Política'):\n",
      "   * Texto original (resumen): El humorista señaló que definirá su situación a fines de junio. \"Solo hay dos caminos: continúo o me...\n",
      "   * Tópico dominante: 4\n",
      "   * Distribución completa de tópicos (id: probabilidad):\n",
      "     [(0, 0.0828), (1, 0.0608), (2, 0.0834), (3, 0.0741), (4, 0.5667), (5, 0.0634), (6, 0.0689)]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
