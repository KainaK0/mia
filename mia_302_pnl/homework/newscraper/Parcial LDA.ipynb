{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214fdd3e41d9e1d7",
   "metadata": {},
   "source": [
    "## Modelado de Temas con LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4e4415acf177cf",
   "metadata": {},
   "source": [
    "### Instalación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T15:53:58.061869Z",
     "start_time": "2025-06-01T15:53:51.946010Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.12/site-packages (3.8.7)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.12/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.12/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.12/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/conda/lib/python3.12/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.12/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from spacy) (78.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas gensim spacy nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8198c2df8708582",
   "metadata": {},
   "source": [
    "### Implementación LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a8e6c8b4d585da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T16:26:33.741317Z",
     "start_time": "2025-06-01T16:26:33.727594Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS as GENSIM_STOP\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Descargas necesarias (solo la primera vez)\n",
    "# -------------------------------------------------\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Definir stopwords en español y stemmer\n",
    "nltk_stop = set(stopwords.words('spanish'))\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Función de preprocesado (tokenización + stemming)\n",
    "# -------------------------------------------------\n",
    "def preprocess(texto: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    1) Convierte a minúsculas y tokeniza quitando puntuación y acentos (deacc=True).\n",
    "    2) Filtra tokens de longitud <= 3.\n",
    "    3) Elimina stopwords de gensim y NLTK.\n",
    "    4) Aplica stemming en español.\n",
    "    Devuelve la lista de stems resultantes.\n",
    "    \"\"\"\n",
    "    tokens = simple_preprocess(texto, deacc=True)\n",
    "    tokens = [t for t in tokens if len(t) > 3 and t not in GENSIM_STOP and t not in nltk_stop]\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. (Opcional) Generación de bigramas\n",
    "# -------------------------------------------------\n",
    "def aplicar_bigramas(\n",
    "        documentos: list[list[str]],\n",
    "        min_count: int = 5,\n",
    "        threshold: float = 100.0\n",
    ") -> tuple[list[list[str]], Phraser]:\n",
    "    \"\"\"\n",
    "    1) Entrena un modelo de bigramas sobre los documentos tokenizados.\n",
    "    2) Devuelve los documentos enriquecidos con bigramas (uni_bi) y el modelo Phraser.\n",
    "    Parámetros:\n",
    "      - min_count: número mínimo de apariciones conjuntas para considerar bigrama.\n",
    "      - threshold: umbral de formación de bigramas (valor alto = más conservador).\n",
    "    \"\"\"\n",
    "    bigram = Phrases(documentos, min_count=min_count, threshold=threshold, progress_per=10000)\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    documentos_bi = [bigram_mod[doc] for doc in documentos]\n",
    "    return documentos_bi, bigram_mod\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Construcción de diccionario y corpus (BoW)\n",
    "# -------------------------------------------------\n",
    "def construir_diccionario_corpus(\n",
    "        textos_tokenizados: list[list[str]],\n",
    "        no_below: int = 5,\n",
    "        no_above: float = 0.5\n",
    ") -> tuple[corpora.Dictionary, list]:\n",
    "    \"\"\"\n",
    "    1) Crea un Dictionary a partir de la lista de documentos (lista de tokens).\n",
    "    2) Filtra palabras muy raras (no_below) o demasiado frecuentes (no_above).\n",
    "    3) Construye el corpus en formato BoW (lista de tuplas (id_token, frecuencia)).\n",
    "    Devuelve el diccionario filtrado y el corpus BoW.\n",
    "    \"\"\"\n",
    "    diccionario = corpora.Dictionary(textos_tokenizados)\n",
    "    diccionario.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    corpus = [diccionario.doc2bow(doc) for doc in textos_tokenizados]\n",
    "    return diccionario, corpus\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. Entrenamiento y evaluación del modelo LDA\n",
    "# -------------------------------------------------\n",
    "def entrenar_lda(\n",
    "        corpus: list,\n",
    "        diccionario: corpora.Dictionary,\n",
    "        num_topics: int = 7,\n",
    "        passes: int = 10,\n",
    "        random_state: int = 42,\n",
    "        alpha: str = 'auto'\n",
    ") -> LdaModel:\n",
    "    \"\"\"\n",
    "    Entrena un modelo LDA con los parámetros indicados y devuelve el objeto LdaModel.\n",
    "    \"\"\"\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=diccionario,\n",
    "        num_topics=num_topics,\n",
    "        random_state=random_state,\n",
    "        passes=passes,\n",
    "        alpha=alpha,\n",
    "        per_word_topics=True\n",
    "    )\n",
    "    return lda_model\n",
    "\n",
    "\n",
    "def evaluar_lda(\n",
    "        lda_model: LdaModel,\n",
    "        corpus: list,\n",
    "        textos_tokenizados: list[list[str]],\n",
    "        diccionario: corpora.Dictionary\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    1) Calcula la perplejidad del modelo sobre el corpus.\n",
    "    2) Calcula la coherencia (c_v) usando CoherenceModel.\n",
    "    Devuelve (perplejidad, coherencia).\n",
    "    \"\"\"\n",
    "    perplejidad = lda_model.log_perplexity(corpus)\n",
    "    coh_model = CoherenceModel(\n",
    "        model=lda_model,\n",
    "        texts=textos_tokenizados,\n",
    "        dictionary=diccionario,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    coherencia = coh_model.get_coherence()\n",
    "    return perplejidad, coherencia\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. Asignación de tópico dominante a cada documento\n",
    "# -------------------------------------------------\n",
    "def asignar_topic_principal(lda_model: LdaModel, corpus: list) -> list[int]:\n",
    "    \"\"\"\n",
    "    Para cada documento en formato BoW, obtiene la lista de tópicos con sus probabilidades\n",
    "    y retorna el ID del tópico con mayor probabilidad (dominante).\n",
    "    \"\"\"\n",
    "    topicos_principales = []\n",
    "    for bow in corpus:\n",
    "        distribucion = lda_model.get_document_topics(bow, minimum_probability=0.0)\n",
    "        topico_dom = max(distribucion, key=lambda x: x[1])[0]\n",
    "        topicos_principales.append(topico_dom)\n",
    "    return topicos_principales\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7. Leer JSON y preparar DataFrame con 'description' y 'category'\n",
    "# -------------------------------------------------\n",
    "def cargar_descriptions_desde_json(ruta_json: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lee un archivo JSON con estructura de lista de objetos:\n",
    "    [\n",
    "      {\n",
    "        \"title\": \"...\",\n",
    "        \"category\": \"...\",\n",
    "        \"summit\": \"...\",\n",
    "        \"description\": \"...\",\n",
    "        \"date\": \"...\",\n",
    "        \"autor\": \"...\",\n",
    "        \"tags\": \"['Seguro Social', 'Estados Unidos']\",\n",
    "        \"url\": \"...\"\n",
    "      },\n",
    "      ...\n",
    "    ]\n",
    "    Extrae las columnas 'description' y 'category', elimina filas con description vacía.\n",
    "    Devuelve un DataFrame con dichas columnas.\n",
    "    \"\"\"\n",
    "    # Cargar JSON en un DataFrame\n",
    "    df = pd.read_json(ruta_json, orient='records', encoding='utf-8')\n",
    "    # Filtrar filas donde 'description' exista y no esté vacío\n",
    "    df = df[df['description'].notna() & (df['description'].str.strip() != \"\")].reset_index(drop=True)\n",
    "    return df[['description', 'category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b423727d1f7990d",
   "metadata": {},
   "source": [
    "### Identificación de 7 tópicos mediante LDA y evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5c57517904c6d64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T16:29:14.135522Z",
     "start_time": "2025-06-01T16:28:48.691827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargados 3698 registros desde 'gestionspider4.json'.\n",
      "\n",
      "Preprocesando descripciones (tokenización + stemming)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progreso preprocesado: 100%|██████████| 3698/3698 [00:07<00:00, 475.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detectando bigramas en el corpus...\n",
      "\n",
      "Construyendo diccionario y corpus BoW...\n",
      "Diccionario creado: 8588 tokens únicos.\n",
      "\n",
      "Entrenando LDA con 7 tópicos (passes=10)...\n",
      "Modelo LDA entrenado.\n",
      "\n",
      "Evaluando modelo LDA...\n",
      "► Perplejidad: -7.5791\n",
      "► Coherencia (c_v): 0.4100\n",
      "\n",
      "Términos más representativos por tópico:\n",
      "  Tópico 0: empres, trabaj, peru, inversion, sector, millon, public, mayor, oper, indic\n",
      "  Tópico 1: president, unid, pais, trump, gobiern, estadounidens, dij, part, segur, segun\n",
      "  Tópico 2: cas, public, pued, inform, deb, present, congres, part, proces, nacional\n",
      "  Tópico 3: pued, empres, sol, trabaj, anos, hac, merc, peru, person, mejor\n",
      "  Tópico 4: product, merc, preci, export, arancel, unid, chin, pais, podri, import\n",
      "  Tópico 5: part, hor, nuev, pued, mexic, unid, pais, ser, canal, encuentr\n",
      "  Tópico 6: proyect, millon, oper, nuev, inversion, peru, lim, empres, marc, desarroll\n",
      "\n",
      "Asignando tópico dominante a cada documento...\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 8. Función principal: pipeline completo adaptado a JSON\n",
    "# Parámetros ajustables\n",
    "RUTA_JSON = \"gestionspider4.json\"        # Nombre del JSON a leer\n",
    "NUM_TOPICS = 7                     # Número de tópicos para LDA\n",
    "PASSES = 10                        # Número de pasadas (iteraciones) en el entrenamiento LDA\n",
    "NO_BELOW = 5                       # Filtrar tokens que aparezcan en menos de NO_BELOW documentos\n",
    "NO_ABOVE = 0.5                     # Filtrar tokens que aparezcan en más del 50% de documentos\n",
    "BIGRAM = True                      # Si quieres incluir n-gramas (True o False)\n",
    "BIGRAM_MIN_COUNT = 5               # Mínimo de apariciones para formar un bigrama\n",
    "BIGRAM_THRESHOLD = 100.0           # Umbral de formación de bigramas\n",
    "\n",
    "# 1) Leer el JSON y extraer 'description' y 'category'\n",
    "df = cargar_descriptions_desde_json(RUTA_JSON)\n",
    "print(f\"Cargados {len(df)} registros desde '{RUTA_JSON}'.\\n\")\n",
    "\n",
    "# 2) Preprocesar los textos en 'description'\n",
    "textos_raw = df['description'].astype(str).tolist()\n",
    "documentos_tokenizados = []\n",
    "print(\"Preprocesando descripciones (tokenización + stemming)...\")\n",
    "for doc in tqdm(textos_raw, desc=\"Progreso preprocesado\"):\n",
    "    documentos_tokenizados.append(preprocess(doc))\n",
    "\n",
    "# 3) (Opcional) Generar e incluir bigramas\n",
    "if BIGRAM:\n",
    "    print(\"\\nDetectando bigramas en el corpus...\")\n",
    "    documentos_bi, modelo_bigram = aplicar_bigramas(\n",
    "        documentos_tokenizados,\n",
    "        min_count=BIGRAM_MIN_COUNT,\n",
    "        threshold=BIGRAM_THRESHOLD\n",
    "    )\n",
    "    textos_finales = documentos_bi\n",
    "else:\n",
    "    textos_finales = documentos_tokenizados\n",
    "\n",
    "# 4) Construir diccionario y corpus BoW\n",
    "print(\"\\nConstruyendo diccionario y corpus BoW...\")\n",
    "diccionario, corpus = construir_diccionario_corpus(\n",
    "    textos_tokenizados=textos_finales,\n",
    "    no_below=NO_BELOW,\n",
    "    no_above=NO_ABOVE\n",
    ")\n",
    "print(f\"Diccionario creado: {len(diccionario)} tokens únicos.\\n\")\n",
    "\n",
    "# 5) Entrenar modelo LDA\n",
    "print(f\"Entrenando LDA con {NUM_TOPICS} tópicos (passes={PASSES})...\")\n",
    "lda_model = entrenar_lda(\n",
    "    corpus=corpus,\n",
    "    diccionario=diccionario,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    passes=PASSES\n",
    ")\n",
    "print(\"Modelo LDA entrenado.\\n\")\n",
    "\n",
    "# 6) Evaluar modelo (perplejidad y coherencia)\n",
    "print(\"Evaluando modelo LDA...\")\n",
    "perplejidad, coherencia = evaluar_lda(\n",
    "    lda_model=lda_model,\n",
    "    corpus=corpus,\n",
    "    textos_tokenizados=textos_finales,\n",
    "    diccionario=diccionario\n",
    ")\n",
    "print(f\"► Perplejidad: {perplejidad:.4f}\")\n",
    "print(f\"► Coherencia (c_v): {coherencia:.4f}\\n\")\n",
    "\n",
    "# 7) Mostrar top-10 palabras de cada tópico\n",
    "print(\"Términos más representativos por tópico:\")\n",
    "for tid in range(NUM_TOPICS):\n",
    "    términos = lda_model.show_topic(tid, topn=10)\n",
    "    lista_palabras = \", \".join([palabra for palabra, _ in términos])\n",
    "    print(f\"  Tópico {tid}: {lista_palabras}\")\n",
    "print()\n",
    "\n",
    "# 8) Asignar tópico dominante a cada documento\n",
    "print(\"Asignando tópico dominante a cada documento...\")\n",
    "df['bow'] = corpus\n",
    "df['topic_principal'] = asignar_topic_principal(lda_model, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb22df2fa34017",
   "metadata": {},
   "source": [
    "### Identificación del tópico a 5 noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df755b6d59c8e0d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T16:29:18.210818Z",
     "start_time": "2025-06-01T16:29:18.201376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo de asignación de tópicos:\n",
      "                                         description   category  \\\n",
      "0  Hace 26 años se iniciaron las concesiones de p...   Economía   \n",
      "1  El billete verde vuelve a cerrar un mes a la b...  Tu Dinero   \n",
      "2  La Victoria es uno de los distritos más resalt...   Empresas   \n",
      "3  Las alarmas permanecen encendidas en el sector...   Economía   \n",
      "4  El ahorro de los peruanos se sigue expandiendo...  Tu Dinero   \n",
      "\n",
      "   topic_principal  \n",
      "0                6  \n",
      "1                4  \n",
      "2                6  \n",
      "3                0  \n",
      "4                0  \n"
     ]
    }
   ],
   "source": [
    "# 9) Mostrar las primeras filas con su categoría original y tópico asignado\n",
    "print(\"\\nEjemplo de asignación de tópicos:\")\n",
    "print(df[['description', 'category', 'topic_principal']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65634df1f10b2e1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:27:17.942496Z",
     "start_time": "2025-06-01T02:27:09.782366Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['category', 'summit']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 269\u001b[39m\n\u001b[32m    265\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m     \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[(t,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mround\u001b[39m(p,\u001b[38;5;250m \u001b[39m\u001b[32m4\u001b[39m))\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mt,\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mdistribucion]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    144\u001b[39m BIGRAM_THRESHOLD = \u001b[32m100.0\u001b[39m          \u001b[38;5;66;03m# Umbral de formación de bigramas\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# 1) Leer el CSV (solo columnas 'summit' y 'category')\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRUTA_CSV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m|\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msummit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcategory\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCargados \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m registros desde \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRUTA_CSV\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# 2) Preprocesar los textos (tokenización + stemming) con tqdm\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:140\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.orig_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.usecols_dtype == \u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(usecols).issubset(\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.orig_names\n\u001b[32m    139\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_usecols_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43morig_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.names) > \u001b[38;5;28mlen\u001b[39m(usecols):  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/io/parsers/base_parser.py:979\u001b[39m, in \u001b[36mParserBase._validate_usecols_names\u001b[39m\u001b[34m(self, usecols, names)\u001b[39m\n\u001b[32m    977\u001b[39m missing = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m usecols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    980\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    981\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    982\u001b[39m     )\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m usecols\n",
      "\u001b[31mValueError\u001b[39m: Usecols do not match columns, columns expected but not found: ['category', 'summit']"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS as GENSIM_STOP\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel, Phrases, TfidfModel\n",
    "from gensim.models.phrases import Phraser\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Descargas NLTK (solo la primera vez)\n",
    "# -------------------------------------------------\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Definir stopwords en español y stemmer\n",
    "# -------------------------------------------------\n",
    "nltk_stop = set(stopwords.words('spanish'))\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Función de preprocesado (tokenización + stemming)\n",
    "# -------------------------------------------------\n",
    "def preprocess(texto: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    1) Convierte a minúsculas y tokeniza quitando puntuación y acentos (deacc=True).\n",
    "    2) Filtra tokens de longitud <= 3.\n",
    "    3) Elimina stopwords de gensim y NLTK.\n",
    "    4) Aplica stemming en español.\n",
    "    Devuelve la lista de stems resultantes.\n",
    "    \"\"\"\n",
    "    tokens = simple_preprocess(texto, deacc=True)\n",
    "    tokens = [t for t in tokens if len(t) > 3 and t not in GENSIM_STOP and t not in nltk_stop]\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Generación de bigramas (opcional)\n",
    "# -------------------------------------------------\n",
    "def aplicar_bigramas(documentos: list[list[str]],\n",
    "                     min_count: int = 5,\n",
    "                     threshold: float = 100.0) -> tuple[list[list[str]], Phraser]:\n",
    "    \"\"\"\n",
    "    1) Entrena un modelo de bigramas sobre los documentos tokenizados.\n",
    "    2) Devuelve los documentos enriquecidos con bigramas (uni_bi) y el modelo Phraser.\n",
    "    \"\"\"\n",
    "    bigram = Phrases(documentos, min_count=min_count, threshold=threshold, progress_per=10000)\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    documentos_bi = [bigram_mod[doc] for doc in documentos]\n",
    "    return documentos_bi, bigram_mod\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. Construcción de diccionario y corpus BoW\n",
    "# -------------------------------------------------\n",
    "def construir_diccionario_corpus(textos_tokenizados: list[list[str]],\n",
    "                                 no_below: int = 5,\n",
    "                                 no_above: float = 0.5) -> tuple[corpora.Dictionary, list]:\n",
    "    \"\"\"\n",
    "    1) Crea un Dictionary a partir de la lista de documentos (lista de tokens).\n",
    "    2) Filtra palabras muy raras (no_below) o demasiado frecuentes (no_above).\n",
    "    3) Construye el corpus en formato BoW (lista de tuplas (id_token, frecuencia)).\n",
    "    Devuelve el diccionario filtrado y el corpus BoW.\n",
    "    \"\"\"\n",
    "    diccionario = corpora.Dictionary(textos_tokenizados)\n",
    "    diccionario.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    corpus_bow = [diccionario.doc2bow(doc) for doc in textos_tokenizados]\n",
    "    return diccionario, corpus_bow\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. Entrenamiento y evaluación del modelo LDA\n",
    "# -------------------------------------------------\n",
    "def entrenar_lda(corpus: list, diccionario: corpora.Dictionary,\n",
    "                 num_topics: int = 7,\n",
    "                 passes: int = 10,\n",
    "                 random_state: int = 42,\n",
    "                 alpha: str = 'auto') -> LdaModel:\n",
    "    \"\"\"\n",
    "    Entrena un modelo LDA con los parámetros indicados y devuelve el objeto LdaModel.\n",
    "    \"\"\"\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=diccionario,\n",
    "        num_topics=num_topics,\n",
    "        random_state=random_state,\n",
    "        passes=passes,\n",
    "        alpha=alpha,\n",
    "        per_word_topics=True\n",
    "    )\n",
    "    return lda_model\n",
    "\n",
    "\n",
    "def evaluar_lda(lda_model: LdaModel,\n",
    "                corpus: list,\n",
    "                textos_tokenizados: list[list[str]],\n",
    "                diccionario: corpora.Dictionary) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    1) Calcula la perplejidad del modelo sobre el corpus.\n",
    "    2) Calcula la coherencia (c_v) usando CoherenceModel.\n",
    "    Devuelve (perplejidad, coherencia).\n",
    "    \"\"\"\n",
    "    perplejidad = lda_model.log_perplexity(corpus)\n",
    "    coh_model = CoherenceModel(model=lda_model,\n",
    "                               texts=textos_tokenizados,\n",
    "                               dictionary=diccionario,\n",
    "                               coherence='c_v')\n",
    "    coherencia = coh_model.get_coherence()\n",
    "    return perplejidad, coherencia\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7. Asignación de tópico dominante a cada documento\n",
    "# -------------------------------------------------\n",
    "def asignar_topic_principal(lda_model: LdaModel, corpus: list) -> list[int]:\n",
    "    \"\"\"\n",
    "    Para cada documento en formato BoW o TF-IDF, obtiene la lista de tópicos\n",
    "    con sus probabilidades y retorna el ID del tópico con mayor probabilidad.\n",
    "    \"\"\"\n",
    "    topicos_principales = []\n",
    "    for doc_vector in corpus:\n",
    "        distribucion = lda_model.get_document_topics(doc_vector, minimum_probability=0.0)\n",
    "        topico_dom = max(distribucion, key=lambda x: x[1])[0]\n",
    "        topicos_principales.append(topico_dom)\n",
    "    return topicos_principales\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 8. Función principal: pipeline completo\n",
    "# -------------------------------------------------\n",
    "def main():\n",
    "    # Parámetros ajustables\n",
    "    RUTA_CSV = \"gestionspider4.json\"      # Nombre del CSV a leer\n",
    "    NUM_TOPICS = 7                    # Número de tópicos para LDA\n",
    "    PASSES = 10                       # Número de pasadas en LDA\n",
    "    NO_BELOW = 5                      # Mínimo de documentos para que un token sobreviva\n",
    "    NO_ABOVE = 0.5                    # Máximo porcentaje de documentos para que un token sobreviva\n",
    "    BIGRAM = True                     # Si generar bigramas (True/False)\n",
    "    BIGRAM_MIN_COUNT = 5              # Umbral mínimo de apariciones conjuntas para bigramas\n",
    "    BIGRAM_THRESHOLD = 100.0          # Umbral de formación de bigramas\n",
    "\n",
    "    # 1) Leer el CSV (solo columnas 'summit' y 'category')\n",
    "    df = pd.read_csv(RUTA_CSV, delimiter='|', encoding='utf-8', usecols=['summit', 'category'])\n",
    "    print(f\"Cargados {len(df)} registros desde '{RUTA_CSV}'.\\n\")\n",
    "\n",
    "    # 2) Preprocesar los textos (tokenización + stemming) con tqdm\n",
    "    textos_raw = df['summit'].astype(str).tolist()\n",
    "    documentos_tokenizados = []\n",
    "    print(\"1) Preprocesando textos (tokenización + stemming)...\")\n",
    "    for doc in tqdm(textos_raw, desc=\"Preprocesado\"):\n",
    "        documentos_tokenizados.append(preprocess(doc))\n",
    "\n",
    "    # 3) (Opcional) Generar e incluir bigramas\n",
    "    if BIGRAM:\n",
    "        print(\"\\n2) Detectando bigramas en el corpus...\")\n",
    "        documentos_bi, modelo_bigram = aplicar_bigramas(\n",
    "            documentos_tokenizados,\n",
    "            min_count=BIGRAM_MIN_COUNT,\n",
    "            threshold=BIGRAM_THRESHOLD\n",
    "        )\n",
    "        textos_finales = documentos_bi\n",
    "    else:\n",
    "        textos_finales = documentos_tokenizados\n",
    "\n",
    "    # 4) Construir diccionario y corpus BoW\n",
    "    print(\"\\n3) Construyendo diccionario y corpus BoW...\")\n",
    "    diccionario, corpus_bow = construir_diccionario_corpus(\n",
    "        textos_tokenizados=textos_finales,\n",
    "        no_below=NO_BELOW,\n",
    "        no_above=NO_ABOVE\n",
    "    )\n",
    "    print(f\"   - Diccionario creado con {len(diccionario)} tokens únicos.\\n\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 5) CÁLCULO DE TF-IDF\n",
    "    # -------------------------------------------------\n",
    "    print(\"4) Calculando modelo TF-IDF sobre el corpus BoW...\")\n",
    "    tfidf_model = TfidfModel(corpus_bow, dictionary=diccionario)\n",
    "    corpus_tfidf = tfidf_model[corpus_bow]\n",
    "\n",
    "    # 5.1) EXTRAER PALABRAS MÁS RELEVANTES (suma de TF-IDF por token)\n",
    "    print(\"   → Extrayendo las palabras más relevantes (sumando TF-IDF por token)...\")\n",
    "    # Creamos un vector global que acumule TF-IDF de cada token sobre todos los docuementos\n",
    "    tfidf_global = {}\n",
    "    for doc in corpus_tfidf:\n",
    "        for token_id, tfidf_val in doc:\n",
    "            if token_id not in tfidf_global:\n",
    "                tfidf_global[token_id] = tfidf_val\n",
    "            else:\n",
    "                tfidf_global[token_id] += tfidf_val\n",
    "\n",
    "    # Ordenamos todos los tokens por peso TF-IDF acumulado descendente\n",
    "    tokens_ordenados = sorted(tfidf_global.items(), key=lambda x: x[1], reverse=True)\n",
    "    TOP_TFIDF = 20\n",
    "    print(f\"\\n   Top {TOP_TFIDF} tokens más relevantes (TF-IDF acumulado):\")\n",
    "    for idx, (token_id, acumulado) in enumerate(tokens_ordenados[:TOP_TFIDF], start=1):\n",
    "        palabra = diccionario[token_id]\n",
    "        print(f\"     {idx:>2}. {palabra:<20}  —  TF-IDF acumulado: {acumulado:.4f}\")\n",
    "    print()\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 6) ENTRENAR LDA sobre CORPUS TF-IDF\n",
    "    # -------------------------------------------------\n",
    "    print(\"5) Entrenando LDA (7 tópicos) sobre el corpus TF-IDF...\")\n",
    "    lda_model = entrenar_lda(\n",
    "        corpus=list(corpus_tfidf),   # IMPORTANTE: usamos corpus_tfidf en lugar de corpus_bow\n",
    "        diccionario=diccionario,\n",
    "        num_topics=NUM_TOPICS,\n",
    "        passes=PASSES\n",
    "    )\n",
    "    print(\"   ► LDA entrenado.\\n\")\n",
    "\n",
    "    # 6.1) Evaluar modelo (perplejidad y coherencia)\n",
    "    print(\"6) Evaluando modelo LDA...\")\n",
    "    perplejidad, coherencia = evaluar_lda(\n",
    "        lda_model=lda_model,\n",
    "        corpus=list(corpus_tfidf),\n",
    "        textos_tokenizados=textos_finales,\n",
    "        diccionario=diccionario\n",
    "    )\n",
    "    print(f\"   ► Perplejidad: {perplejidad:.4f}\")\n",
    "    print(f\"   ► Coherencia (c_v): {coherencia:.4f}\\n\")\n",
    "\n",
    "    # 6.2) Mostrar top-10 palabras de cada tópico\n",
    "    print(\"7) Términos más representativos por tópico (Top 10):\")\n",
    "    for tid in range(NUM_TOPICS):\n",
    "        términos = lda_model.show_topic(tid, topn=10)\n",
    "        términos_str = \", \".join([pal for pal, _ in términos])\n",
    "        print(f\"   Tópico {tid}: {términos_str}\")\n",
    "    print()\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 7) ASIGNAR TÓPICO DOMINANTE A CADA DOCUMENTO\n",
    "    # -------------------------------------------------\n",
    "    print(\"8) Asignando tópico dominante a cada noticia...\")\n",
    "    df['bow'] = corpus_bow\n",
    "    # NOTA: La función `asignar_topic_principal` funciona con corpus en BoW o en TF-IDF\n",
    "    df['topic_principal'] = asignar_topic_principal(lda_model, list(corpus_tfidf))\n",
    "    print(\"   → Asignación completada.\\n\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 8) MOSTRAR RESULTADOS PARA 3 NOTICIAS EJEMPLO\n",
    "    # -------------------------------------------------\n",
    "    print(\"9) Mostrando resultados de LDA para 3 noticias de ejemplo:\\n\")\n",
    "    # Seleccionamos tres índices arbitrarios (por ejemplo 0, 10 y 20) o aleatorios\n",
    "    ejemplos_idx = [0, 10, 20]\n",
    "    for idx in ejemplos_idx:\n",
    "        texto_original = df.loc[idx, 'summit']\n",
    "        categoria = df.loc[idx, 'category']\n",
    "        bow         = df.loc[idx, 'bow']\n",
    "        tfidf_vec   = corpus_tfidf[idx]\n",
    "        topico_dom  = df.loc[idx, 'topic_principal']\n",
    "\n",
    "        # Obtenemos la distribución completa de tópicos para ese documento\n",
    "        distribucion = lda_model.get_document_topics(tfidf_vec, minimum_probability=0.0)\n",
    "\n",
    "        print(f\"→ Noticia #{idx} (categoría = '{categoria}'):\")\n",
    "        print(f\"   * Texto original (resumen): {texto_original[:100]}...\")  # mostramos solo los primeros 100 caracteres\n",
    "        print(f\"   * Tópico dominante: {topico_dom}\")\n",
    "        print(f\"   * Distribución completa de tópicos (id: probabilidad):\")\n",
    "        print(f\"     {[(t, round(p, 4)) for t, p in distribucion]}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
