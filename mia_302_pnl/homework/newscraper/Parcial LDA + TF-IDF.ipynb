{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Modelado de Temas con LDA + TF-IDF",
   "id": "214fdd3e41d9e1d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Instalación de Librerías",
   "id": "7a4e4415acf177cf"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-01T15:53:58.061869Z",
     "start_time": "2025-06-01T15:53:51.946010Z"
    }
   },
   "source": "!pip install pandas gensim spacy nltk",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\programdata\\miniconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: gensim in c:\\programdata\\miniconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: spacy in c:\\programdata\\miniconda3\\lib\\site-packages (3.8.5)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\miniconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from gensim) (1.13.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (0.15.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (2.11.4)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (71.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\miniconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\miniconda3\\lib\\site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\miniconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\miniconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\miniconda3\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Implementación LDA + LDA + TF-IDF",
   "id": "d8198c2df8708582"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T16:36:56.593021Z",
     "start_time": "2025-06-01T16:36:56.574515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS as GENSIM_STOP\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel, Phrases, TfidfModel\n",
    "from gensim.models.phrases import Phraser\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Descargas necesarias (solo la primera vez)\n",
    "# -------------------------------------------------\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Definir stopwords en español y stemmer\n",
    "# -------------------------------------------------\n",
    "nltk_stop = set(stopwords.words('spanish'))\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Función de preprocesado (tokenización + stemming)\n",
    "# -------------------------------------------------\n",
    "def preprocess(texto: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    1) Convierte a minúsculas y tokeniza quitando puntuación y acentos (deacc=True).\n",
    "    2) Filtra tokens de longitud <= 3.\n",
    "    3) Elimina stopwords de gensim y NLTK.\n",
    "    4) Aplica stemming en español.\n",
    "    Devuelve la lista de stems resultantes.\n",
    "    \"\"\"\n",
    "    tokens = simple_preprocess(texto, deacc=True)\n",
    "    tokens = [t for t in tokens if len(t) > 3 and t not in GENSIM_STOP and t not in nltk_stop]\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Generación de bigramas (opcional)\n",
    "# -------------------------------------------------\n",
    "def aplicar_bigramas(\n",
    "        documentos: list[list[str]],\n",
    "        min_count: int = 5,\n",
    "        threshold: float = 100.0\n",
    ") -> tuple[list[list[str]], Phraser]:\n",
    "    \"\"\"\n",
    "    1) Entrena un modelo de bigramas sobre los documentos tokenizados.\n",
    "    2) Devuelve los documentos enriquecidos con bigramas (uni_bi) y el modelo Phraser.\n",
    "    \"\"\"\n",
    "    bigram = Phrases(documentos, min_count=min_count, threshold=threshold, progress_per=10000)\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    documentos_bi = [bigram_mod[doc] for doc in documentos]\n",
    "    return documentos_bi, bigram_mod\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. Construcción de diccionario y corpus BoW\n",
    "# -------------------------------------------------\n",
    "def construir_diccionario_corpus(\n",
    "        textos_tokenizados: list[list[str]],\n",
    "        no_below: int = 5,\n",
    "        no_above: float = 0.5\n",
    ") -> tuple[corpora.Dictionary, list]:\n",
    "    \"\"\"\n",
    "    1) Crea un Dictionary a partir de la lista de documentos (lista de tokens).\n",
    "    2) Filtra palabras muy raras (no_below) o demasiado frecuentes (no_above).\n",
    "    3) Construye el corpus en formato BoW (lista de tuplas (id_token, frecuencia)).\n",
    "    Devuelve el diccionario filtrado y el corpus BoW.\n",
    "    \"\"\"\n",
    "    diccionario = corpora.Dictionary(textos_tokenizados)\n",
    "    diccionario.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    corpus_bow = [diccionario.doc2bow(doc) for doc in textos_tokenizados]\n",
    "    return diccionario, corpus_bow\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. Entrenamiento y evaluación del modelo LDA\n",
    "# -------------------------------------------------\n",
    "def entrenar_lda(\n",
    "        corpus: list,\n",
    "        diccionario: corpora.Dictionary,\n",
    "        num_topics: int = 7,\n",
    "        passes: int = 10,\n",
    "        random_state: int = 42,\n",
    "        alpha: str = 'auto'\n",
    ") -> LdaModel:\n",
    "    \"\"\"\n",
    "    Entrena un modelo LDA con los parámetros indicados y devuelve el objeto LdaModel.\n",
    "    \"\"\"\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=diccionario,\n",
    "        num_topics=num_topics,\n",
    "        random_state=random_state,\n",
    "        passes=passes,\n",
    "        alpha=alpha,\n",
    "        per_word_topics=True\n",
    "    )\n",
    "    return lda_model\n",
    "\n",
    "\n",
    "def evaluar_lda(\n",
    "        lda_model: LdaModel,\n",
    "        corpus: list,\n",
    "        textos_tokenizados: list[list[str]],\n",
    "        diccionario: corpora.Dictionary\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    1) Calcula la perplejidad del modelo sobre el corpus.\n",
    "    2) Calcula la coherencia (c_v) usando CoherenceModel.\n",
    "    Devuelve (perplejidad, coherencia).\n",
    "    \"\"\"\n",
    "    perplejidad = lda_model.log_perplexity(corpus)\n",
    "    coh_model = CoherenceModel(\n",
    "        model=lda_model,\n",
    "        texts=textos_tokenizados,\n",
    "        dictionary=diccionario,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    coherencia = coh_model.get_coherence()\n",
    "    return perplejidad, coherencia\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7. Asignación de tópico dominante a cada documento\n",
    "# -------------------------------------------------\n",
    "def asignar_topic_principal(lda_model: LdaModel, corpus: list) -> list[int]:\n",
    "    \"\"\"\n",
    "    Para cada documento en formato BoW o TF-IDF, obtiene la lista de tópicos\n",
    "    con sus probabilidades y retorna el ID del tópico con mayor probabilidad.\n",
    "    \"\"\"\n",
    "    topicos_principales = []\n",
    "    for doc_vector in corpus:\n",
    "        distribucion = lda_model.get_document_topics(doc_vector, minimum_probability=0.0)\n",
    "        topico_dom = max(distribucion, key=lambda x: x[1])[0]\n",
    "        topicos_principales.append(topico_dom)\n",
    "    return topicos_principales\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 8. Leer JSON y preparar DataFrame con 'description' y 'category'\n",
    "# -------------------------------------------------\n",
    "def cargar_descriptions_desde_json(ruta_json: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lee un archivo JSON con estructura de lista de objetos:\n",
    "    [\n",
    "      {\n",
    "        \"title\": \"...\",\n",
    "        \"category\": \"...\",\n",
    "        \"summit\": \"...\",\n",
    "        \"description\": \"...\",\n",
    "        \"date\": \"...\",\n",
    "        \"autor\": \"...\",\n",
    "        \"tags\": \"['Seguro Social', 'Estados Unidos']\",\n",
    "        \"url\": \"...\"\n",
    "      },\n",
    "      ...\n",
    "    ]\n",
    "    Extrae las columnas 'description' y 'category', elimina filas con description vacía.\n",
    "    Devuelve un DataFrame con dichas columnas.\n",
    "    \"\"\"\n",
    "    df = pd.read_json(ruta_json, orient='records', encoding='utf-8')\n",
    "    # Filtrar filas donde 'description' exista y no esté vacío\n",
    "    df = df[df['description'].notna() & (df['description'].str.strip() != \"\")].reset_index(drop=True)\n",
    "    return df[['description', 'category']]"
   ],
   "id": "65634df1f10b2e1a",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Entrenamiento",
   "id": "2ef8cd85bd5ccb0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T16:37:57.478557Z",
     "start_time": "2025-06-01T16:37:29.228536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# -------------------------------------------------\n",
    "# 9. Función principal: pipeline completo adaptado a JSON\n",
    "# -------------------------------------------------\n",
    "# Parámetros ajustables\n",
    "RUTA_JSON = \"gestionspider5.json\"         # Nombre del JSON a leer\n",
    "NUM_TOPICS = 7                      # Número de tópicos para LDA\n",
    "PASSES = 10                         # Número de pasadas en LDA\n",
    "NO_BELOW = 5                        # Mínimo de documentos para que un token sobreviva\n",
    "NO_ABOVE = 0.5                      # Máximo porcentaje de documentos para que un token sobreviva\n",
    "BIGRAM = True                       # Si generar bigramas (True/False)\n",
    "BIGRAM_MIN_COUNT = 5                # Umbral mínimo de apariciones conjuntas para bigramas\n",
    "BIGRAM_THRESHOLD = 100.0            # Umbral de formación de bigramas\n",
    "\n",
    "# 1) Leer el JSON (solo columnas 'description' y 'category')\n",
    "df = cargar_descriptions_desde_json(RUTA_JSON)\n",
    "print(f\"Cargados {len(df)} registros desde '{RUTA_JSON}'.\\n\")\n",
    "\n",
    "# 2) Preprocesar los textos en 'description' (tokenización + stemming) con tqdm\n",
    "textos_raw = df['description'].astype(str).tolist()\n",
    "documentos_tokenizados = []\n",
    "print(\"1) Preprocesando descripciones (tokenización + stemming)...\")\n",
    "for doc in tqdm(textos_raw, desc=\"Preprocesado\"):\n",
    "    documentos_tokenizados.append(preprocess(doc))\n",
    "\n",
    "# 3) (Opcional) Generar e incluir bigramas\n",
    "if BIGRAM:\n",
    "    print(\"\\n2) Detectando bigramas en el corpus...\")\n",
    "    documentos_bi, modelo_bigram = aplicar_bigramas(\n",
    "        documentos_tokenizados,\n",
    "        min_count=BIGRAM_MIN_COUNT,\n",
    "        threshold=BIGRAM_THRESHOLD\n",
    "    )\n",
    "    textos_finales = documentos_bi\n",
    "else:\n",
    "    textos_finales = documentos_tokenizados\n",
    "\n",
    "# 4) Construir diccionario y corpus BoW\n",
    "print(\"\\n3) Construyendo diccionario y corpus BoW...\")\n",
    "diccionario, corpus_bow = construir_diccionario_corpus(\n",
    "    textos_tokenizados=textos_finales,\n",
    "    no_below=NO_BELOW,\n",
    "    no_above=NO_ABOVE\n",
    ")\n",
    "print(f\"   - Diccionario creado con {len(diccionario)} tokens únicos.\\n\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) CÁLCULO DE TF-IDF\n",
    "# -------------------------------------------------\n",
    "print(\"4) Calculando modelo TF-IDF sobre el corpus BoW...\")\n",
    "tfidf_model = TfidfModel(corpus_bow, dictionary=diccionario)\n",
    "corpus_tfidf = tfidf_model[corpus_bow]\n",
    "\n",
    "# 5.1) EXTRAER PALABRAS MÁS RELEVANTES (suma de TF-IDF por token)\n",
    "print(\"   → Extrayendo las palabras más relevantes (sumando TF-IDF por token)...\")\n",
    "tfidf_global = {}\n",
    "for doc in corpus_tfidf:\n",
    "    for token_id, tfidf_val in doc:\n",
    "        tfidf_global[token_id] = tfidf_global.get(token_id, 0.0) + tfidf_val\n",
    "\n",
    "tokens_ordenados = sorted(tfidf_global.items(), key=lambda x: x[1], reverse=True)\n",
    "TOP_TFIDF = 20\n",
    "print(f\"\\n   Top {TOP_TFIDF} tokens más relevantes (TF-IDF acumulado):\")\n",
    "for idx, (token_id, acumulado) in enumerate(tokens_ordenados[:TOP_TFIDF], start=1):\n",
    "    palabra = diccionario[token_id]\n",
    "    print(f\"     {idx:>2}. {palabra:<20}  —  TF-IDF acumulado: {acumulado:.4f}\")\n",
    "print()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) ENTRENAR LDA sobre CORPUS TF-IDF\n",
    "# -------------------------------------------------\n",
    "print(\"5) Entrenando LDA (7 tópicos) sobre el corpus TF-IDF...\")\n",
    "lda_model = entrenar_lda(\n",
    "    corpus=list(corpus_tfidf),   # Usamos corpus_tfidf en lugar de corpus_bow\n",
    "    diccionario=diccionario,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    passes=PASSES\n",
    ")\n",
    "print(\"   ► LDA entrenado.\\n\")\n",
    "\n",
    "# 6.1) Evaluar modelo (perplejidad y coherencia)\n",
    "print(\"6) Evaluando modelo LDA...\")\n",
    "perplejidad, coherencia = evaluar_lda(\n",
    "    lda_model=lda_model,\n",
    "    corpus=list(corpus_tfidf),\n",
    "    textos_tokenizados=textos_finales,\n",
    "    diccionario=diccionario\n",
    ")\n",
    "print(f\"   ► Perplejidad: {perplejidad:.4f}\")\n",
    "print(f\"   ► Coherencia (c_v): {coherencia:.4f}\\n\")\n",
    "\n",
    "# 6.2) Mostrar top-10 palabras de cada tópico\n",
    "print(\"7) Términos más representativos por tópico (Top 10):\")\n",
    "for tid in range(NUM_TOPICS):\n",
    "    términos = lda_model.show_topic(tid, topn=10)\n",
    "    términos_str = \", \".join([pal for pal, _ in términos])\n",
    "    print(f\"   Tópico {tid}: {términos_str}\")\n",
    "print()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7) ASIGNAR TÓPICO DOMINANTE A CADA DOCUMENTO\n",
    "# -------------------------------------------------\n",
    "print(\"8) Asignando tópico dominante a cada noticia...\")\n",
    "df['bow'] = corpus_bow\n",
    "df['topic_principal'] = asignar_topic_principal(lda_model, list(corpus_tfidf))\n",
    "print(\"   → Asignación completada.\\n\")"
   ],
   "id": "e1a731544f8b3b35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargados 1292 registros desde 'gestionspider5.json'.\n",
      "\n",
      "1) Preprocesando descripciones (tokenización + stemming)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocesado: 100%|██████████| 1292/1292 [00:04<00:00, 277.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2) Detectando bigramas en el corpus...\n",
      "\n",
      "3) Construyendo diccionario y corpus BoW...\n",
      "   - Diccionario creado con 4690 tokens únicos.\n",
      "\n",
      "4) Calculando modelo TF-IDF sobre el corpus BoW...\n",
      "   → Extrayendo las palabras más relevantes (sumando TF-IDF por token)...\n",
      "\n",
      "   Top 20 tokens más relevantes (TF-IDF acumulado):\n",
      "      1. proyect               —  TF-IDF acumulado: 25.5687\n",
      "      2. millon                —  TF-IDF acumulado: 24.4403\n",
      "      3. empres                —  TF-IDF acumulado: 22.7244\n",
      "      4. inversion             —  TF-IDF acumulado: 20.4075\n",
      "      5. trabaj                —  TF-IDF acumulado: 19.5196\n",
      "      6. oper                  —  TF-IDF acumulado: 19.0404\n",
      "      7. peru                  —  TF-IDF acumulado: 19.0082\n",
      "      8. unid                  —  TF-IDF acumulado: 18.1247\n",
      "      9. pais                  —  TF-IDF acumulado: 17.4621\n",
      "     10. public                —  TF-IDF acumulado: 17.4093\n",
      "     11. nuev                  —  TF-IDF acumulado: 17.1772\n",
      "     12. chin                  —  TF-IDF acumulado: 17.1278\n",
      "     13. merc                  —  TF-IDF acumulado: 17.1084\n",
      "     14. president             —  TF-IDF acumulado: 16.7162\n",
      "     15. pued                  —  TF-IDF acumulado: 16.4905\n",
      "     16. product               —  TF-IDF acumulado: 16.3900\n",
      "     17. peruan                —  TF-IDF acumulado: 16.0285\n",
      "     18. nacional              —  TF-IDF acumulado: 15.6524\n",
      "     19. congres               —  TF-IDF acumulado: 15.5042\n",
      "     20. polit                 —  TF-IDF acumulado: 15.4145\n",
      "\n",
      "5) Entrenando LDA (7 tópicos) sobre el corpus TF-IDF...\n",
      "   ► LDA entrenado.\n",
      "\n",
      "6) Evaluando modelo LDA...\n",
      "   ► Perplejidad: -8.9937\n",
      "   ► Coherencia (c_v): 0.4360\n",
      "\n",
      "7) Términos más representativos por tópico (Top 10):\n",
      "   Tópico 0: chin, arancel, trump, unid, estadounidens, ucrani, indi, rus, rusi, donald_trump\n",
      "   Tópico 1: caf, aeropuert, export, terminal, pasajer, hotel, ositr, caden, tiend, caca\n",
      "   Tópico 2: proyect, millon, empres, inversion, trabaj, oper, peru, public, pued, pais\n",
      "   Tópico 3: conden, nadin_heredi, humal, asil, exprimer_dam, odebrecht, sentenci, anos_prision, suer_fisiolog, heredi\n",
      "   Tópico 4: acert, commoditi, cambiari, fernandez_jeri, variacion, bursatil, audi, variabl, manteng, perfor\n",
      "   Tópico 5: nobo, daniel_nobo, luis_gonzalez, ecuatorian, ecuador, derrot, petr, sicariat, schial, adolescent\n",
      "   Tópico 6: pait, parqu, electr, transport_urban, muell, ecolog, reniec, transport, energi, hidroelectr\n",
      "\n",
      "8) Asignando tópico dominante a cada noticia...\n",
      "   → Asignación completada.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Identificación del tópico de 3 noticias",
   "id": "c6cf0713a14e25f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T16:38:17.493402Z",
     "start_time": "2025-06-01T16:38:17.462257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# -------------------------------------------------\n",
    "# 8) MOSTRAR RESULTADOS PARA 3 NOTICIAS EJEMPLO\n",
    "# -------------------------------------------------\n",
    "print(\"9) Mostrando resultados de LDA para 3 noticias de ejemplo:\\n\")\n",
    "ejemplos_idx = [0, 10, 20]  # índices de ejemplo\n",
    "for idx in ejemplos_idx:\n",
    "    texto_original = df.loc[idx, 'description']\n",
    "    categoria = df.loc[idx, 'category']\n",
    "    bow         = df.loc[idx, 'bow']\n",
    "    tfidf_vec   = corpus_tfidf[idx]\n",
    "    topico_dom  = df.loc[idx, 'topic_principal']\n",
    "\n",
    "    distribucion = lda_model.get_document_topics(tfidf_vec, minimum_probability=0.0)\n",
    "\n",
    "    print(f\"→ Noticia #{idx} (categoría = '{categoria}'):\")\n",
    "    print(f\"   * Descripción (texto original): {texto_original[:100]}...\")\n",
    "    print(f\"   * Tópico dominante: {topico_dom}\")\n",
    "    print(f\"   * Distribución completa de tópicos (id: probabilidad):\")\n",
    "    print(f\"     {[(t, round(p, 4)) for t, p in distribucion]}\\n\")"
   ],
   "id": "596bf6065b85ecb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9) Mostrando resultados de LDA para 3 noticias de ejemplo:\n",
      "\n",
      "→ Noticia #0 (categoría = 'EEUU'):\n",
      "   * Descripción (texto original): El expresidente de Estados Unidos, Joe Biden, confirmó este viernes que ya ha comenzado su tratamien...\n",
      "   * Tópico dominante: 2\n",
      "   * Distribución completa de tópicos (id: probabilidad):\n",
      "     [(0, 0.0094), (1, 0.0086), (2, 0.9576), (3, 0.0063), (4, 0.0056), (5, 0.0066), (6, 0.0059)]\n",
      "\n",
      "→ Noticia #10 (categoría = 'Economía'):\n",
      "   * Descripción (texto original): El Ministerio de Desarrollo Agrario y Riego (Midagri), a través del Servicio Nacional de Sanidad Agr...\n",
      "   * Tópico dominante: 2\n",
      "   * Distribución completa de tópicos (id: probabilidad):\n",
      "     [(0, 0.0089), (1, 0.2551), (2, 0.7131), (3, 0.0059), (4, 0.0053), (5, 0.0062), (6, 0.0056)]\n",
      "\n",
      "→ Noticia #20 (categoría = 'Finanzas Personales'):\n",
      "   * Descripción (texto original): A fin de mes, muchas personas enfrentan la misma disyuntiva: ¿debo priorizar el pago de mis deudas o...\n",
      "   * Tópico dominante: 2\n",
      "   * Distribución completa de tópicos (id: probabilidad):\n",
      "     [(0, 0.0073), (1, 0.0066), (2, 0.9673), (3, 0.0049), (4, 0.0043), (5, 0.0051), (6, 0.0046)]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
