{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214fdd3e41d9e1d7",
   "metadata": {},
   "source": [
    "## Modelado de Temas con LDA + TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4e4415acf177cf",
   "metadata": {},
   "source": [
    "### Instalación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T15:53:58.061869Z",
     "start_time": "2025-06-01T15:53:51.946010Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.12/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from spacy) (78.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy-3.8.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "Downloading preshed-3.0.10-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (869 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.3/869.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, shellingham, scipy, regex, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, nltk, language-data, gensim, typer, langcodes, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.2\n",
      "    Uninstalling scipy-1.15.2:\n",
      "      Successfully uninstalled scipy-1.15.2\n",
      "Successfully installed blis-1.2.1 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 gensim-4.3.3 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.13 nltk-3.9.1 preshed-3.0.10 regex-2024.11.6 scipy-1.13.1 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.16.0 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas gensim spacy nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8198c2df8708582",
   "metadata": {},
   "source": [
    "### Implementación LDA + LDA + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65634df1f10b2e1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T16:36:56.593021Z",
     "start_time": "2025-06-01T16:36:56.574515Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS as GENSIM_STOP\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel, Phrases, TfidfModel\n",
    "from gensim.models.phrases import Phraser\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Descargas necesarias (solo la primera vez)\n",
    "# -------------------------------------------------\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Definir stopwords en español y stemmer\n",
    "# -------------------------------------------------\n",
    "nltk_stop = set(stopwords.words('spanish'))\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Función de preprocesado (tokenización + stemming)\n",
    "# -------------------------------------------------\n",
    "def preprocess(texto: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    1) Convierte a minúsculas y tokeniza quitando puntuación y acentos (deacc=True).\n",
    "    2) Filtra tokens de longitud <= 3.\n",
    "    3) Elimina stopwords de gensim y NLTK.\n",
    "    4) Aplica stemming en español.\n",
    "    Devuelve la lista de stems resultantes.\n",
    "    \"\"\"\n",
    "    tokens = simple_preprocess(texto, deacc=True)\n",
    "    tokens = [t for t in tokens if len(t) > 3 and t not in GENSIM_STOP and t not in nltk_stop]\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Generación de bigramas (opcional)\n",
    "# -------------------------------------------------\n",
    "def aplicar_bigramas(\n",
    "        documentos: list[list[str]],\n",
    "        min_count: int = 5,\n",
    "        threshold: float = 100.0\n",
    ") -> tuple[list[list[str]], Phraser]:\n",
    "    \"\"\"\n",
    "    1) Entrena un modelo de bigramas sobre los documentos tokenizados.\n",
    "    2) Devuelve los documentos enriquecidos con bigramas (uni_bi) y el modelo Phraser.\n",
    "    \"\"\"\n",
    "    bigram = Phrases(documentos, min_count=min_count, threshold=threshold, progress_per=10000)\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    documentos_bi = [bigram_mod[doc] for doc in documentos]\n",
    "    return documentos_bi, bigram_mod\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. Construcción de diccionario y corpus BoW\n",
    "# -------------------------------------------------\n",
    "def construir_diccionario_corpus(\n",
    "        textos_tokenizados: list[list[str]],\n",
    "        no_below: int = 5,\n",
    "        no_above: float = 0.5\n",
    ") -> tuple[corpora.Dictionary, list]:\n",
    "    \"\"\"\n",
    "    1) Crea un Dictionary a partir de la lista de documentos (lista de tokens).\n",
    "    2) Filtra palabras muy raras (no_below) o demasiado frecuentes (no_above).\n",
    "    3) Construye el corpus en formato BoW (lista de tuplas (id_token, frecuencia)).\n",
    "    Devuelve el diccionario filtrado y el corpus BoW.\n",
    "    \"\"\"\n",
    "    diccionario = corpora.Dictionary(textos_tokenizados)\n",
    "    diccionario.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    corpus_bow = [diccionario.doc2bow(doc) for doc in textos_tokenizados]\n",
    "    return diccionario, corpus_bow\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. Entrenamiento y evaluación del modelo LDA\n",
    "# -------------------------------------------------\n",
    "def entrenar_lda(\n",
    "        corpus: list,\n",
    "        diccionario: corpora.Dictionary,\n",
    "        num_topics: int = 7,\n",
    "        passes: int = 10,\n",
    "        random_state: int = 42,\n",
    "        alpha: str = 'auto'\n",
    ") -> LdaModel:\n",
    "    \"\"\"\n",
    "    Entrena un modelo LDA con los parámetros indicados y devuelve el objeto LdaModel.\n",
    "    \"\"\"\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=diccionario,\n",
    "        num_topics=num_topics,\n",
    "        random_state=random_state,\n",
    "        passes=passes,\n",
    "        alpha=alpha,\n",
    "        per_word_topics=True\n",
    "    )\n",
    "    return lda_model\n",
    "\n",
    "\n",
    "def evaluar_lda(\n",
    "        lda_model: LdaModel,\n",
    "        corpus: list,\n",
    "        textos_tokenizados: list[list[str]],\n",
    "        diccionario: corpora.Dictionary\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    1) Calcula la perplejidad del modelo sobre el corpus.\n",
    "    2) Calcula la coherencia (c_v) usando CoherenceModel.\n",
    "    Devuelve (perplejidad, coherencia).\n",
    "    \"\"\"\n",
    "    perplejidad = lda_model.log_perplexity(corpus)\n",
    "    coh_model = CoherenceModel(\n",
    "        model=lda_model,\n",
    "        texts=textos_tokenizados,\n",
    "        dictionary=diccionario,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    coherencia = coh_model.get_coherence()\n",
    "    return perplejidad, coherencia\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7. Asignación de tópico dominante a cada documento\n",
    "# -------------------------------------------------\n",
    "def asignar_topic_principal(lda_model: LdaModel, corpus: list) -> list[int]:\n",
    "    \"\"\"\n",
    "    Para cada documento en formato BoW o TF-IDF, obtiene la lista de tópicos\n",
    "    con sus probabilidades y retorna el ID del tópico con mayor probabilidad.\n",
    "    \"\"\"\n",
    "    topicos_principales = []\n",
    "    for doc_vector in corpus:\n",
    "        distribucion = lda_model.get_document_topics(doc_vector, minimum_probability=0.0)\n",
    "        topico_dom = max(distribucion, key=lambda x: x[1])[0]\n",
    "        topicos_principales.append(topico_dom)\n",
    "    return topicos_principales\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 8. Leer JSON y preparar DataFrame con 'description' y 'category'\n",
    "# -------------------------------------------------\n",
    "def cargar_descriptions_desde_json(ruta_json: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lee un archivo JSON con estructura de lista de objetos:\n",
    "    [\n",
    "      {\n",
    "        \"title\": \"...\",\n",
    "        \"category\": \"...\",\n",
    "        \"summit\": \"...\",\n",
    "        \"description\": \"...\",\n",
    "        \"date\": \"...\",\n",
    "        \"autor\": \"...\",\n",
    "        \"tags\": \"['Seguro Social', 'Estados Unidos']\",\n",
    "        \"url\": \"...\"\n",
    "      },\n",
    "      ...\n",
    "    ]\n",
    "    Extrae las columnas 'description' y 'category', elimina filas con description vacía.\n",
    "    Devuelve un DataFrame con dichas columnas.\n",
    "    \"\"\"\n",
    "    df = pd.read_json(ruta_json, orient='records', encoding='utf-8')\n",
    "    # Filtrar filas donde 'description' exista y no esté vacío\n",
    "    df = df[df['description'].notna() & (df['description'].str.strip() != \"\")].reset_index(drop=True)\n",
    "    return df[['description', 'category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef8cd85bd5ccb0f",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a731544f8b3b35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T16:37:57.478557Z",
     "start_time": "2025-06-01T16:37:29.228536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargados 3698 registros desde 'gestionspider4.json'.\n",
      "\n",
      "1) Preprocesando descripciones (tokenización + stemming)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocesado: 100%|██████████| 3698/3698 [00:07<00:00, 469.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2) Detectando bigramas en el corpus...\n",
      "\n",
      "3) Construyendo diccionario y corpus BoW...\n",
      "   - Diccionario creado con 8588 tokens únicos.\n",
      "\n",
      "4) Calculando modelo TF-IDF sobre el corpus BoW...\n",
      "   → Extrayendo las palabras más relevantes (sumando TF-IDF por token)...\n",
      "\n",
      "   Top 20 tokens más relevantes (TF-IDF acumulado):\n",
      "      1. millon                —  TF-IDF acumulado: 65.7557\n",
      "      2. proyect               —  TF-IDF acumulado: 60.4189\n",
      "      3. empres                —  TF-IDF acumulado: 56.2502\n",
      "      4. inversion             —  TF-IDF acumulado: 52.1546\n",
      "      5. peru                  —  TF-IDF acumulado: 50.2855\n",
      "      6. trabaj                —  TF-IDF acumulado: 50.1291\n",
      "      7. unid                  —  TF-IDF acumulado: 47.5569\n",
      "      8. oper                  —  TF-IDF acumulado: 45.5046\n",
      "      9. pais                  —  TF-IDF acumulado: 45.0985\n",
      "     10. public                —  TF-IDF acumulado: 43.8802\n",
      "     11. president             —  TF-IDF acumulado: 42.8957\n",
      "     12. nuev                  —  TF-IDF acumulado: 42.8781\n",
      "     13. merc                  —  TF-IDF acumulado: 42.8305\n",
      "     14. pued                  —  TF-IDF acumulado: 42.7240\n",
      "     15. segur                 —  TF-IDF acumulado: 42.1552\n",
      "     16. product               —  TF-IDF acumulado: 42.0391\n",
      "     17. inform                —  TF-IDF acumulado: 40.3668\n",
      "     18. nacional              —  TF-IDF acumulado: 39.1257\n",
      "     19. person                —  TF-IDF acumulado: 39.0248\n",
      "     20. part                  —  TF-IDF acumulado: 39.0095\n",
      "\n",
      "5) Entrenando LDA (7 tópicos) sobre el corpus TF-IDF...\n",
      "   ► LDA entrenado.\n",
      "\n",
      "6) Evaluando modelo LDA...\n",
      "   ► Perplejidad: -9.0827\n",
      "   ► Coherencia (c_v): 0.4739\n",
      "\n",
      "7) Términos más representativos por tópico (Top 10):\n",
      "   Tópico 0: royal_rumbl, elimination_chamb, wrestlemani, peacock, netflix, alicorp, bursatil_variacion, preci_commoditi, john_cen, acert\n",
      "   Tópico 1: unid, trabaj, president, pued, pais, segur, trump, public, cas, part\n",
      "   Tópico 2: inter_miami, munoz, lionel_messi, obes, ventanill, subprefect, waykis_sombr, clasic_plat, estadi_centenari, oficin_normaliz\n",
      "   Tópico 3: real_madr, atlet_madr, jorn_lalig, barcelon, espanyol, amor, valentin, lalig, moned_centav, santiag_bernabeu\n",
      "   Tópico 4: senamhi, rafag_vient, graniz, niev, temperatur_maxim, radiacion_ultraviolet, sierr, precipit, temperatur_diurn, avis_meteorolog\n",
      "   Tópico 5: premi, powerball, dolar, sorte, gan, ganador, sup_bowl, jueg_loteri, meg_millions, bolet\n",
      "   Tópico 6: proyect, inversion, millon, miner, oper, crecimient, lim, produccion, sector, merc\n",
      "\n",
      "8) Asignando tópico dominante a cada noticia...\n",
      "   → Asignación completada.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------------\n",
    "# 9. Función principal: pipeline completo adaptado a JSON\n",
    "# -------------------------------------------------\n",
    "# Parámetros ajustables\n",
    "RUTA_JSON = \"gestionspider4.json\"         # Nombre del JSON a leer\n",
    "NUM_TOPICS = 7                      # Número de tópicos para LDA\n",
    "PASSES = 10                         # Número de pasadas en LDA\n",
    "NO_BELOW = 5                        # Mínimo de documentos para que un token sobreviva\n",
    "NO_ABOVE = 0.5                      # Máximo porcentaje de documentos para que un token sobreviva\n",
    "BIGRAM = True                       # Si generar bigramas (True/False)\n",
    "BIGRAM_MIN_COUNT = 5                # Umbral mínimo de apariciones conjuntas para bigramas\n",
    "BIGRAM_THRESHOLD = 100.0            # Umbral de formación de bigramas\n",
    "\n",
    "# 1) Leer el JSON (solo columnas 'description' y 'category')\n",
    "df = cargar_descriptions_desde_json(RUTA_JSON)\n",
    "print(f\"Cargados {len(df)} registros desde '{RUTA_JSON}'.\\n\")\n",
    "\n",
    "# 2) Preprocesar los textos en 'description' (tokenización + stemming) con tqdm\n",
    "textos_raw = df['description'].astype(str).tolist()\n",
    "documentos_tokenizados = []\n",
    "print(\"1) Preprocesando descripciones (tokenización + stemming)...\")\n",
    "for doc in tqdm(textos_raw, desc=\"Preprocesado\"):\n",
    "    documentos_tokenizados.append(preprocess(doc))\n",
    "\n",
    "# 3) (Opcional) Generar e incluir bigramas\n",
    "if BIGRAM:\n",
    "    print(\"\\n2) Detectando bigramas en el corpus...\")\n",
    "    documentos_bi, modelo_bigram = aplicar_bigramas(\n",
    "        documentos_tokenizados,\n",
    "        min_count=BIGRAM_MIN_COUNT,\n",
    "        threshold=BIGRAM_THRESHOLD\n",
    "    )\n",
    "    textos_finales = documentos_bi\n",
    "else:\n",
    "    textos_finales = documentos_tokenizados\n",
    "\n",
    "# 4) Construir diccionario y corpus BoW\n",
    "print(\"\\n3) Construyendo diccionario y corpus BoW...\")\n",
    "diccionario, corpus_bow = construir_diccionario_corpus(\n",
    "    textos_tokenizados=textos_finales,\n",
    "    no_below=NO_BELOW,\n",
    "    no_above=NO_ABOVE\n",
    ")\n",
    "print(f\"   - Diccionario creado con {len(diccionario)} tokens únicos.\\n\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) CÁLCULO DE TF-IDF\n",
    "# -------------------------------------------------\n",
    "print(\"4) Calculando modelo TF-IDF sobre el corpus BoW...\")\n",
    "tfidf_model = TfidfModel(corpus_bow, dictionary=diccionario)\n",
    "corpus_tfidf = tfidf_model[corpus_bow]\n",
    "\n",
    "# 5.1) EXTRAER PALABRAS MÁS RELEVANTES (suma de TF-IDF por token)\n",
    "print(\"   → Extrayendo las palabras más relevantes (sumando TF-IDF por token)...\")\n",
    "tfidf_global = {}\n",
    "for doc in corpus_tfidf:\n",
    "    for token_id, tfidf_val in doc:\n",
    "        tfidf_global[token_id] = tfidf_global.get(token_id, 0.0) + tfidf_val\n",
    "\n",
    "tokens_ordenados = sorted(tfidf_global.items(), key=lambda x: x[1], reverse=True)\n",
    "TOP_TFIDF = 20\n",
    "print(f\"\\n   Top {TOP_TFIDF} tokens más relevantes (TF-IDF acumulado):\")\n",
    "for idx, (token_id, acumulado) in enumerate(tokens_ordenados[:TOP_TFIDF], start=1):\n",
    "    palabra = diccionario[token_id]\n",
    "    print(f\"     {idx:>2}. {palabra:<20}  —  TF-IDF acumulado: {acumulado:.4f}\")\n",
    "print()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) ENTRENAR LDA sobre CORPUS TF-IDF\n",
    "# -------------------------------------------------\n",
    "print(\"5) Entrenando LDA (7 tópicos) sobre el corpus TF-IDF...\")\n",
    "lda_model = entrenar_lda(\n",
    "    corpus=list(corpus_tfidf),   # Usamos corpus_tfidf en lugar de corpus_bow\n",
    "    diccionario=diccionario,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    passes=PASSES\n",
    ")\n",
    "print(\"   ► LDA entrenado.\\n\")\n",
    "\n",
    "# 6.1) Evaluar modelo (perplejidad y coherencia)\n",
    "print(\"6) Evaluando modelo LDA...\")\n",
    "perplejidad, coherencia = evaluar_lda(\n",
    "    lda_model=lda_model,\n",
    "    corpus=list(corpus_tfidf),\n",
    "    textos_tokenizados=textos_finales,\n",
    "    diccionario=diccionario\n",
    ")\n",
    "print(f\"   ► Perplejidad: {perplejidad:.4f}\")\n",
    "print(f\"   ► Coherencia (c_v): {coherencia:.4f}\\n\")\n",
    "\n",
    "# 6.2) Mostrar top-10 palabras de cada tópico\n",
    "print(\"7) Términos más representativos por tópico (Top 10):\")\n",
    "for tid in range(NUM_TOPICS):\n",
    "    términos = lda_model.show_topic(tid, topn=10)\n",
    "    términos_str = \", \".join([pal for pal, _ in términos])\n",
    "    print(f\"   Tópico {tid}: {términos_str}\")\n",
    "print()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7) ASIGNAR TÓPICO DOMINANTE A CADA DOCUMENTO\n",
    "# -------------------------------------------------\n",
    "print(\"8) Asignando tópico dominante a cada noticia...\")\n",
    "df['bow'] = corpus_bow\n",
    "df['topic_principal'] = asignar_topic_principal(lda_model, list(corpus_tfidf))\n",
    "print(\"   → Asignación completada.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf0713a14e25f6",
   "metadata": {},
   "source": [
    "### Identificación del tópico de 3 noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "596bf6065b85ecb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T16:38:17.493402Z",
     "start_time": "2025-06-01T16:38:17.462257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9) Mostrando resultados de LDA para 3 noticias de ejemplo:\n",
      "\n",
      "→ Noticia #0 (categoría = 'Economía'):\n",
      "   * Descripción (texto original): Hace 26 años se iniciaron las concesiones de puertos en el Perú, ascendiendo a, actualmente, ocho. C...\n",
      "   * Tópico dominante: 6\n",
      "   * Distribución completa de tópicos (id: probabilidad):\n",
      "     [(0, 0.0034), (1, 0.4157), (2, 0.0025), (3, 0.0027), (4, 0.0026), (5, 0.0042), (6, 0.569)]\n",
      "\n",
      "→ Noticia #10 (categoría = 'Empresas'):\n",
      "   * Descripción (texto original): AstraZeneca, uno de los laboratorios más grandes del mundo, ve al mercado peruano como un hub releva...\n",
      "   * Tópico dominante: 1\n",
      "   * Distribución completa de tópicos (id: probabilidad):\n",
      "     [(0, 0.0037), (1, 0.757), (2, 0.0028), (3, 0.003), (4, 0.0029), (5, 0.0046), (6, 0.2261)]\n",
      "\n",
      "→ Noticia #20 (categoría = 'Política'):\n",
      "   * Descripción (texto original): A menos de un año de las elecciones generales 2026, la Oficina Nacional de Procesos Electorales (ONP...\n",
      "   * Tópico dominante: 1\n",
      "   * Distribución completa de tópicos (id: probabilidad):\n",
      "     [(0, 0.0042), (1, 0.7805), (2, 0.0032), (3, 0.0034), (4, 0.0033), (5, 0.0053), (6, 0.2001)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------------\n",
    "# 8) MOSTRAR RESULTADOS PARA 3 NOTICIAS EJEMPLO\n",
    "# -------------------------------------------------\n",
    "print(\"9) Mostrando resultados de LDA para 3 noticias de ejemplo:\\n\")\n",
    "ejemplos_idx = [0, 10, 20]  # índices de ejemplo\n",
    "for idx in ejemplos_idx:\n",
    "    texto_original = df.loc[idx, 'description']\n",
    "    categoria = df.loc[idx, 'category']\n",
    "    bow         = df.loc[idx, 'bow']\n",
    "    tfidf_vec   = corpus_tfidf[idx]\n",
    "    topico_dom  = df.loc[idx, 'topic_principal']\n",
    "\n",
    "    distribucion = lda_model.get_document_topics(tfidf_vec, minimum_probability=0.0)\n",
    "\n",
    "    print(f\"→ Noticia #{idx} (categoría = '{categoria}'):\")\n",
    "    print(f\"   * Descripción (texto original): {texto_original[:100]}...\")\n",
    "    print(f\"   * Tópico dominante: {topico_dom}\")\n",
    "    print(f\"   * Distribución completa de tópicos (id: probabilidad):\")\n",
    "    print(f\"     {[(t, round(p, 4)) for t, p in distribucion]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
